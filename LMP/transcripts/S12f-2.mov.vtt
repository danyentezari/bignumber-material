WEBVTT - This file was automatically generated by VIMEO

0
00:00:00.300 --> 00:00:01.800
Now, let me show you the remaining code.

1
00:00:03.300 --> 00:00:03.800
before we talk about

2
00:00:05.500 --> 00:00:07.500
the softmax or sigmoid

3
00:00:08.600 --> 00:00:09.500
activation function

4
00:00:10.600 --> 00:00:12.000
So if I scroll down here?

5
00:00:16.100 --> 00:00:17.200
I am going to go.

6
00:00:20.600 --> 00:00:21.600
review the

7
00:00:23.500 --> 00:00:25.300
model by typing model

8
00:00:26.100 --> 00:00:29.100
dot summary the summary method is going to

9
00:00:29.100 --> 00:00:30.900
 create a table that

10
00:00:33.800 --> 00:00:36.700
Indicates our architecture and the

11
00:00:36.700 --> 00:00:39.100
 number of parameters of this model has learned.

12
00:00:40.400 --> 00:00:41.100
So if I run this now?

13
00:00:42.900 --> 00:00:45.200
Give it a few seconds for the data set to be downloaded.

14
00:00:48.400 --> 00:00:50.900
And why is it complex?

15
00:00:52.900 --> 00:00:54.000
missing parenthesis

16
00:01:07.200 --> 00:01:10.200
So here you can see your the

17
00:01:10.200 --> 00:01:12.400
 architecture of your neuron at work.

18
00:01:13.400 --> 00:01:16.300
And assess here that you can learn this model can

19
00:01:16.300 --> 00:01:18.300
 learn a total of 122,

20
00:01:19.100 --> 00:01:19.600
000 parameters

21
00:01:21.300 --> 00:01:24.300
which relative to other pre-trained models this is very

22
00:01:28.400 --> 00:01:32.200
insignificant like the resident 50 model I

23
00:01:31.200 --> 00:01:32.700
 think has

24
00:01:34.200 --> 00:01:35.800
six digit or seven digit

25
00:01:37.200 --> 00:01:39.100
so it's it's billions of parameters.

26
00:01:39.900 --> 00:01:42.200
The more you parameters a model to learn.

27
00:01:42.800 --> 00:01:44.400
the well

28
00:01:46.400 --> 00:01:46.800
the more

29
00:01:50.100 --> 00:01:52.700
capabilities the more our variation can handle

30
00:01:53.700 --> 00:01:55.600
so we've built a model, but we haven't traded yet.

31
00:01:56.500 --> 00:01:58.200
so the train that we're going to use the

32
00:02:00.600 --> 00:02:00.800
compile method

33
00:02:02.200 --> 00:02:03.200
So that the model knows.

34
00:02:04.100 --> 00:02:08.200
Which algorithm to use to back propagate

35
00:02:07.200 --> 00:02:10.400
 the weights again, if you watch the video that I

36
00:02:10.400 --> 00:02:11.300
 put on Slack?

37
00:02:12.100 --> 00:02:14.400
This ID should be familiar to you.

38
00:02:16.100 --> 00:02:18.500
But the idea is to keep on.

39
00:02:20.800 --> 00:02:23.500
Feeding this neural network these pictures again and

40
00:02:23.500 --> 00:02:24.000
 again and again.

41
00:02:25.300 --> 00:02:26.000
and each time

42
00:02:26.800 --> 00:02:29.800
it is fed through the network the updated

43
00:02:29.800 --> 00:02:32.300
 wage. Remember we the point here is to adjust the weights.

44
00:02:33.300 --> 00:02:36.300
And we get the model to predict.

45
00:02:39.600 --> 00:02:40.000
images

46
00:02:42.300 --> 00:02:45.400
with labels that are closest to the original label so we

47
00:02:45.400 --> 00:02:45.600
 want

48
00:02:46.900 --> 00:02:49.300
The difference to be eliminated we want

49
00:02:49.300 --> 00:02:50.600
 to reduce error to zero.

50
00:02:51.300 --> 00:02:55.100
And empirically we'll do this with cross entropy,

51
00:02:54.100 --> 00:02:57.000
 which we'll talk about in a moment.

52
00:02:57.900 --> 00:03:00.300
I'm going to go model. Excuse me model that compile.

53
00:03:03.100 --> 00:03:04.200
the optimizer

54
00:03:05.700 --> 00:03:08.100
is Adam Adam stands for Adam is actually

55
00:03:08.100 --> 00:03:08.900
 an acronym.

56
00:03:09.800 --> 00:03:11.400
adaptive moment estimation

57
00:03:15.500 --> 00:03:16.300
Moment.

58
00:03:17.600 --> 00:03:20.000
in statistics is a

59
00:03:21.500 --> 00:03:24.100
is one of the details that describes the shape of the data.

60
00:03:25.100 --> 00:03:26.700
So one moment is for example.

61
00:03:27.700 --> 00:03:30.300
Variance, that's the spread of

62
00:03:30.300 --> 00:03:30.600
 the data.

63
00:03:32.500 --> 00:03:33.400
Another moment is the mean.

64
00:03:34.100 --> 00:03:36.300
The center the central tendency of the data.

65
00:03:37.200 --> 00:03:39.000
Another one is the skewness.

66
00:03:39.900 --> 00:03:42.100
So if it has a long right tail or

67
00:03:42.100 --> 00:03:44.800
 a left tail and kurtosis is the peakness.

68
00:03:45.700 --> 00:03:48.800
This word is borrowed from physics, but it has a slightly different

69
00:03:48.800 --> 00:03:51.800
 meaning in physics. But an estimation

70
00:03:51.800 --> 00:03:52.400
 is well.

71
00:03:53.400 --> 00:03:54.100
It's prediction.

72
00:03:56.600 --> 00:03:59.600
But the naming a neighbor side this is

73
00:03:59.600 --> 00:04:02.200
 going to be used to find the or adjust

74
00:04:02.200 --> 00:04:04.100
 the weights. Then we have the last function.

75
00:04:05.700 --> 00:04:06.200
which is

76
00:04:07.600 --> 00:04:09.700
TF dot Keras

77
00:04:10.600 --> 00:04:11.100
Dot

78
00:04:11.800 --> 00:04:12.500
Sports

79
00:04:13.900 --> 00:04:14.900
categorical

80
00:04:16.500 --> 00:04:17.000
cross

81
00:04:18.500 --> 00:04:20.900
entropy now cross entropy is a loss function.

82
00:04:22.100 --> 00:04:22.900
That can be used.

83
00:04:24.700 --> 00:04:26.200
for binary classification

84
00:04:27.700 --> 00:04:30.100
But in this case since we have every picture you can

85
00:04:30.100 --> 00:04:32.900
 fall into either one of 10 classes we want.

86
00:04:36.200 --> 00:04:39.500
Well, we have multi-class classification. So we

87
00:04:39.500 --> 00:04:42.700
 will use the sparse variant of it again. We'll look

88
00:04:42.700 --> 00:04:43.900
 at the moment of this in a second.

89
00:04:44.700 --> 00:04:47.100
And then we go from logics.

90
00:04:53.800 --> 00:04:56.500
You'll see what a logit is in a second from Logics equals

91
00:04:56.500 --> 00:04:56.700
 true.

92
00:05:02.900 --> 00:05:03.900
and metrics

93
00:05:06.900 --> 00:05:09.000
for a classification the

94
00:05:11.200 --> 00:05:12.200
we have a couple of metrics.

95
00:05:15.600 --> 00:05:18.100
There is accuracy precision and recall.

96
00:05:19.700 --> 00:05:22.600
If this time will I'll talk about Precision recall.

97
00:05:23.400 --> 00:05:26.600
But basically we want this number to convert to

98
00:05:26.600 --> 00:05:26.900
 one.

99
00:05:28.400 --> 00:05:30.600
So we want accuracy to be 100%

100
00:05:31.900 --> 00:05:34.100
and what we want the laws cross entropy.

101
00:05:35.300 --> 00:05:38.400
To convert to 0. So again, you're looking at two things the loss function. This

102
00:05:38.400 --> 00:05:39.200
 has to be 0

103
00:05:39.900 --> 00:05:41.100
the closet is either the better.

104
00:05:42.300 --> 00:05:44.200
And the met the accuracy.

105
00:05:45.200 --> 00:05:46.200
The closer to one the better.

106
00:05:46.800 --> 00:05:48.200
Let me put these on separate lines.
