WEBVTT - This file was automatically generated by VIMEO

0
00:00:00.400 --> 00:00:01.600
If you remember.

1
00:00:04.400 --> 00:00:05.800
from our last session we talked about

2
00:00:08.400 --> 00:00:11.200
the multi-- the perceptron

3
00:00:12.200 --> 00:00:14.300
it was this mathematical model.

4
00:00:15.100 --> 00:00:16.000
Where we had these features?

5
00:00:17.300 --> 00:00:19.100
and with each feature was associated with

6
00:00:20.700 --> 00:00:23.400
let me actually pull the pull that up. If you

7
00:00:23.400 --> 00:00:26.700
 remember we showed we talked about this very fundamental component of

8
00:00:26.700 --> 00:00:27.300
 a neural network.

9
00:00:29.300 --> 00:00:30.100
So it has

10
00:00:33.900 --> 00:00:34.300
you know.

11
00:00:36.700 --> 00:00:37.400
these inputs

12
00:00:38.200 --> 00:00:40.600
and each input corresponds to

13
00:00:43.100 --> 00:00:45.200
a feature of the data set

14
00:00:47.100 --> 00:00:50.200
So again to use a real estate example the features of a

15
00:00:50.200 --> 00:00:50.800
 property.

16
00:00:51.700 --> 00:00:54.100
It's the number of bedrooms the number of

17
00:00:54.100 --> 00:00:54.500
 bathrooms.

18
00:00:55.700 --> 00:00:56.700
the size of the apartment

19
00:00:58.800 --> 00:01:01.600
And these are the weights now in machine

20
00:01:01.600 --> 00:01:04.900
 learning terminology you we also

21
00:01:04.900 --> 00:01:05.800
 called these.

22
00:01:08.700 --> 00:01:09.500
parameters

23
00:01:11.500 --> 00:01:13.700
So weights parameters, these are the same thing.

24
00:01:16.400 --> 00:01:18.900
Parameters and weights refer to the same thing.

25
00:01:20.900 --> 00:01:21.300
So here

26
00:01:23.300 --> 00:01:24.900
the purpose of the training data set

27
00:01:25.400 --> 00:01:28.300
is to let the model learn the parameters to understand.

28
00:01:29.500 --> 00:01:31.000
To understand, you know, it's all.

29
00:01:32.300 --> 00:01:34.000
statistical calculation

30
00:01:35.200 --> 00:01:38.900
but we want the monitor calculate the weights. Now if

31
00:01:38.900 --> 00:01:41.500
 you have a regression problem the way you find the weights.

32
00:01:42.400 --> 00:01:45.000
Will you there's something called the Computing formula?

33
00:01:46.900 --> 00:01:49.900
Again, it's nothing more than basic arithmetic.

34
00:01:49.900 --> 00:01:51.500
 Maybe we can talk about the later on.

35
00:01:51.900 --> 00:01:54.100
But it has to do with the observations and the residuals.

36
00:01:54.800 --> 00:01:57.300
And the residual is a distance of the observation from

37
00:01:57.300 --> 00:02:01.000
 the regression line. If you've watched the video from

38
00:02:00.800 --> 00:02:03.300
 that, I referenced you would

39
00:02:03.300 --> 00:02:06.400
 be familiar with these terms anyway in the training stage.

40
00:02:06.400 --> 00:02:08.000
 We want the model to learn the parameters.

41
00:02:09.900 --> 00:02:12.100
Sometimes training in validation is done in one.

42
00:02:13.400 --> 00:02:14.100
in one go

43
00:02:15.500 --> 00:02:18.100
So depending on the library that you're using in the machine

44
00:02:18.100 --> 00:02:21.300
 learning while they're using you may have to create you may need to

45
00:02:21.300 --> 00:02:24.600
 have three divisions. So you will take your original data and

46
00:02:24.600 --> 00:02:25.300
 divide it three.

47
00:02:26.100 --> 00:02:29.600
Training validation tests other times you can just have trained and

48
00:02:29.600 --> 00:02:29.600
 test.

49
00:02:30.500 --> 00:02:31.600
the purpose of validation

50
00:02:32.400 --> 00:02:36.300
is to see the check the metrics to identify underfitting

51
00:02:35.300 --> 00:02:36.800
 overfitting.

52
00:02:37.800 --> 00:02:40.400
We said that underfing underfitting is when the

53
00:02:40.400 --> 00:02:43.400
 model has not been able to calculate the

54
00:02:44.500 --> 00:02:47.300
weights properly and you can easily detect this

55
00:02:47.300 --> 00:02:48.700
 because as soon as you look at the metrics

56
00:02:50.200 --> 00:02:54.000
Let's say we talked about MSC if the MSC is large.

57
00:02:53.600 --> 00:02:56.600
 It is diverging from zero for the

58
00:02:56.600 --> 00:02:58.000
 trading at the side. That's under 30.

59
00:02:58.600 --> 00:03:00.900
So we don't go any further. We don't even proceed to test.

60
00:03:02.200 --> 00:03:03.000
So if that happens.

61
00:03:04.000 --> 00:03:04.200
then

62
00:03:06.500 --> 00:03:09.900
we'd have to adjust our data or adjust our

63
00:03:09.900 --> 00:03:10.700
 machine learning model.

64
00:03:12.100 --> 00:03:15.400
And that is if we can even do something about it

65
00:03:15.400 --> 00:03:17.400
 because sometimes the data so sparse so.

66
00:03:19.400 --> 00:03:21.700
Unusable, there's not much you can do about it.

67
00:03:22.300 --> 00:03:23.900
You know the same garbage in garbage out.

68
00:03:24.700 --> 00:03:26.000
So we need to make sure there's no garbage.

69
00:03:27.500 --> 00:03:27.600
but

70
00:03:32.800 --> 00:03:35.200
another purpose of validation is for the model to

71
00:03:36.200 --> 00:03:38.300
Just a parameters afterwards has learned.

72
00:03:39.400 --> 00:03:41.800
So after it learns the parameters.

73
00:03:42.800 --> 00:03:45.200
Even if the metrics are good, let's

74
00:03:45.200 --> 00:03:46.300
 say MSC is good.

75
00:03:46.900 --> 00:03:48.100
We want it to be better still.

76
00:03:49.600 --> 00:03:52.300
So in this validation stage

77
00:03:52.300 --> 00:03:53.500
 with the validation data set.

78
00:03:54.500 --> 00:03:57.600
We can we look at the metrics and then if necessary

79
00:03:57.600 --> 00:04:00.200
 we up. We change the hyper parameters now do not.

80
00:04:04.400 --> 00:04:07.600
Conflate parameters in hyper parameters. Unfortunately, there's one

81
00:04:07.600 --> 00:04:10.500
 library at least that I S skater very popular Library.

82
00:04:11.300 --> 00:04:14.400
They use these two words interchangeably or

83
00:04:14.400 --> 00:04:17.800
 at least when I look at the documentation that of the case that's a

84
00:04:17.800 --> 00:04:18.000
 problem because

85
00:04:19.600 --> 00:04:22.700
hyper parameter parameters are different things very different

86
00:04:22.700 --> 00:04:25.100
 things. The parameters are the weights that the

87
00:04:25.100 --> 00:04:25.600
 model must learn.

88
00:04:26.200 --> 00:04:27.500
the hyperparameters

89
00:04:28.200 --> 00:04:32.100
Are like the arguments of the

90
00:04:31.100 --> 00:04:32.400
 function?

91
00:04:33.700 --> 00:04:34.400
so we will have

92
00:04:35.700 --> 00:04:39.200
let's see a machine learning class like sequential and

93
00:04:38.200 --> 00:04:42.300
 the sequential class can take different arguments. We

94
00:04:41.300 --> 00:04:43.100
 will see this in a minute.

95
00:04:43.800 --> 00:04:46.700
You as the data sites must choose the hyper parameters.

96
00:04:47.200 --> 00:04:50.800
They're effectively arguments of a function. So you must change the

97
00:04:50.800 --> 00:04:53.500
 architecture of the neural network. If in

98
00:04:53.500 --> 00:04:56.600
 the case of deep learning until you get a better combination, so

99
00:04:56.600 --> 00:04:59.100
 as the machine learning if you're doing machine under two things you

100
00:04:59.100 --> 00:04:59.400
 need to fix.

101
00:05:00.400 --> 00:05:03.500
Or two things need to take into account the date the

102
00:05:03.500 --> 00:05:04.100
 quality of the data.

103
00:05:04.900 --> 00:05:07.700
and the type of machine learning model, you've

104
00:05:07.700 --> 00:05:08.900
 choose you've chosen and the

105
00:05:10.800 --> 00:05:13.300
The configuration so actually that's

106
00:05:13.300 --> 00:05:15.300
 a better word for hyperprem is the configuration.

107
00:05:16.100 --> 00:05:18.900
hyperparameters equals configuration of the model

108
00:05:21.400 --> 00:05:21.500
now

109
00:05:23.500 --> 00:05:24.700
In order for you to find out.

110
00:05:26.300 --> 00:05:27.300
if this model

111
00:05:29.700 --> 00:05:30.100
is

112
00:05:31.800 --> 00:05:34.300
usable and can be put into production

113
00:05:34.300 --> 00:05:35.800
 after the best possible.

114
00:05:38.800 --> 00:05:40.500
configuration after the best possible data

115
00:05:42.400 --> 00:05:45.000
mudging that imagining is nobody clean the data.

116
00:05:46.200 --> 00:05:50.100
You do feature engineering. So assuming you've done you've picked

117
00:05:49.100 --> 00:05:52.400
 the best possible model and you have so you have

118
00:05:52.400 --> 00:05:55.400
 produced the best possible data if this

119
00:05:55.400 --> 00:05:58.200
 model is going to be useful or not, and that is used to test data set.

120
00:05:59.800 --> 00:06:00.500
the test data set

121
00:06:01.600 --> 00:06:04.500
after test essay will check the metrics after the adjusted model

122
00:06:04.500 --> 00:06:07.300
 after the adjusted parameter after the model has learned the new

123
00:06:07.300 --> 00:06:07.700
 parameters.

124
00:06:08.400 --> 00:06:12.000
So after the test data set your metrics are still poor then.

125
00:06:13.700 --> 00:06:16.500
The model is not the it's not the right option. That is

126
00:06:16.500 --> 00:06:17.500
 assuming your data is

127
00:06:18.200 --> 00:06:19.200
you know well

128
00:06:23.600 --> 00:06:26.400
You know, your data is in the best possible condition.

129
00:06:27.700 --> 00:06:30.300
So if that's the if that's the case and your model

130
00:06:30.300 --> 00:06:33.100
 is still pulled and you need to choose a different model. Either that or

131
00:06:33.100 --> 00:06:36.300
 again you need to check the hyper parameters. So if you find over

132
00:06:36.300 --> 00:06:36.800
 a fitting here.

133
00:06:38.300 --> 00:06:39.300
Then you must go back.

134
00:06:40.700 --> 00:06:41.500
to

135
00:06:43.600 --> 00:06:45.400
what comes before the training stage?

136
00:06:47.800 --> 00:06:48.100
Okay.

137
00:06:49.200 --> 00:06:51.300
So in our case, we just need training test.

138
00:06:53.300 --> 00:06:56.700
The training that is that would allow them all to learn the parameters. And

139
00:06:56.700 --> 00:07:00.300
 the test said I said we will see if we have underfitting overfitting.

140
00:07:00.900 --> 00:07:02.800
Or if we are on the right track.
