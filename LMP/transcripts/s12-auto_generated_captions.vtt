WEBVTT - This file was automatically generated by VIMEO

0
00:00:01.700 --> 00:00:03.500
in our last session

1
00:00:12.200 --> 00:00:13.000
we talked about.

2
00:00:16.900 --> 00:00:19.400
the convolution operation on matrices

3
00:00:21.400 --> 00:00:22.500
and we talked about the

4
00:00:25.400 --> 00:00:26.300
the pooling Matrix

5
00:00:27.600 --> 00:00:28.700
and what is purpose was?

6
00:00:30.700 --> 00:00:34.300
today we'll quickly review the first or

7
00:00:33.300 --> 00:00:36.200
 I won't say first but the portion

8
00:00:36.200 --> 00:00:37.000
 of the notebook that we

9
00:00:38.500 --> 00:00:39.700
Have so far worked on.

10
00:00:41.100 --> 00:00:42.200
but then once we reach

11
00:00:44.600 --> 00:00:46.600
the part where we have to choose the

12
00:00:48.400 --> 00:00:49.500
activation function

13
00:00:50.500 --> 00:00:53.300
and the loss function will have

14
00:00:53.300 --> 00:00:53.400
 to

15
00:00:54.700 --> 00:00:57.500
take a detour into the world of probability Theory.

16
00:01:01.300 --> 00:01:04.100
So yes, it's a linear mathematics course, but since

17
00:01:04.100 --> 00:01:07.600
 this is neural networks, I mean, we've understood

18
00:01:07.600 --> 00:01:11.100
 the Matrix part. We now need to talk about the probability probability

19
00:01:10.100 --> 00:01:13.200
 theory part so fully will appreciate.

20
00:01:15.100 --> 00:01:16.700
the probabilistic

21
00:01:17.500 --> 00:01:18.200
context

22
00:01:19.300 --> 00:01:19.900
so first

23
00:01:20.600 --> 00:01:21.300
quick review

24
00:01:23.100 --> 00:01:24.200
of The Notebook

25
00:01:25.700 --> 00:01:26.800
or image classification

26
00:01:28.100 --> 00:01:33.000
then we

27
00:01:32.200 --> 00:01:33.900
 want to talk about the

28
00:01:42.600 --> 00:01:44.800
the sigmoid activation function

29
00:01:48.100 --> 00:01:49.900
So this has some prerequisites.

30
00:01:51.500 --> 00:01:51.800
in

31
00:01:52.600 --> 00:01:54.700
probably Theory the purpose of the sigmoid function.

32
00:01:56.200 --> 00:01:56.700
is to

33
00:01:58.900 --> 00:01:59.500
associate a feature

34
00:02:01.200 --> 00:02:01.600
with

35
00:02:02.900 --> 00:02:03.400
a class

36
00:02:06.800 --> 00:02:10.600
so we will talk about this number

37
00:02:10.600 --> 00:02:13.400
 we call probability then we will talk about this thing called

38
00:02:13.400 --> 00:02:13.800
 odds.

39
00:02:15.300 --> 00:02:18.300
then the log with big function was in the context

40
00:02:18.300 --> 00:02:18.400
 of

41
00:02:19.200 --> 00:02:19.900
probability

42
00:02:22.600 --> 00:02:24.600
and then we will see how we derive this function.

43
00:02:27.900 --> 00:02:30.300
now since this is a classification problem, we will

44
00:02:30.300 --> 00:02:31.400
 use a loss function for

45
00:02:33.400 --> 00:02:35.400
evaluating this classification model

46
00:02:36.300 --> 00:02:37.200
so we will also talk about

47
00:02:41.900 --> 00:02:42.200
cross

48
00:02:43.300 --> 00:02:43.800
entropy

49
00:02:49.800 --> 00:02:52.300
we cover this. This will be easier. We will

50
00:02:52.300 --> 00:02:55.200
 have the we will have covered the preview records to talk

51
00:02:55.200 --> 00:02:55.400
 about this.

52
00:02:56.400 --> 00:02:59.100
And then if time allows it I want to talk about.

53
00:03:01.400 --> 00:03:05.100
the fundamental features of recurrent neural

54
00:03:04.100 --> 00:03:05.200
 network

55
00:03:09.200 --> 00:03:10.400
so let's call this overview of

56
00:03:12.300 --> 00:03:12.600
the current

57
00:03:23.300 --> 00:03:26.300
so let's begin and of course if there's any question you

58
00:03:26.300 --> 00:03:29.300
 have related to anything we've talked about today or

59
00:03:29.300 --> 00:03:30.700
 any of the sessions of the past.

60
00:03:32.100 --> 00:03:35.000
We can if we leave 10-15 minutes.

61
00:03:35.700 --> 00:03:37.200
Towards the end of the session.

62
00:03:37.900 --> 00:03:38.700
talk about those

63
00:03:40.900 --> 00:03:41.100
okay.

64
00:03:42.300 --> 00:03:45.100
Let me switch to the notebook.

65
00:03:47.300 --> 00:03:50.500
Let's start from the top. So what we started with the

66
00:03:50.500 --> 00:03:52.300
 importing the libraries.

67
00:03:53.500 --> 00:03:54.200
tensorflow

68
00:03:55.400 --> 00:03:56.200
and Keras

69
00:03:57.100 --> 00:03:58.000
and math problem

70
00:03:59.900 --> 00:04:02.800
here is just makes it easier to build a neural

71
00:04:02.800 --> 00:04:03.700
 network. We're using tensorflow.

72
00:04:05.100 --> 00:04:09.500
And I thought W. We're using to visualize a sample

73
00:04:09.500 --> 00:04:10.100
 of our data.

74
00:04:12.200 --> 00:04:15.700
And the metrics for model, I'm not

75
00:04:15.700 --> 00:04:18.400
 sure if we're going to plot the metrics, but I will show you a picture

76
00:04:18.400 --> 00:04:20.400
 of it nonetheless if we don't do it in this notebook.

77
00:04:21.300 --> 00:04:21.600
anyway

78
00:04:25.900 --> 00:04:28.700
Let me just make sure I have my reference code opener because

79
00:04:28.700 --> 00:04:30.500
 I have several items open here.

80
00:04:31.900 --> 00:04:34.000
It's just bring this over there.

81
00:04:34.900 --> 00:04:35.200
Okay.

82
00:04:36.400 --> 00:04:38.800
Yes, then we proceed to download the dataset.

83
00:04:40.900 --> 00:04:42.000
now Keras

84
00:04:45.200 --> 00:04:46.000
provides

85
00:04:50.800 --> 00:04:53.500
sample data for you to experiment

86
00:04:53.500 --> 00:04:55.500
 with with the rich numerical data or

87
00:04:56.500 --> 00:04:59.700
Pectoral data like we're using here the c510 database. It

88
00:04:59.700 --> 00:05:02.400
 also comes with pre-trained models. I will talk about pre-trains model

89
00:05:02.400 --> 00:05:03.300
 the later on.

90
00:05:04.300 --> 00:05:07.400
But what you're doing here is we're we're going to the data sets

91
00:05:07.400 --> 00:05:10.700
 object to download the c410 database.

92
00:05:13.200 --> 00:05:16.400
And we saw that we saw what the pictures were like your tiny images

93
00:05:16.400 --> 00:05:19.800
 Square tiny images with that would

94
00:05:19.800 --> 00:05:22.200
 fall into one of these 10 classes airplane, you know

95
00:05:22.200 --> 00:05:23.000
 the ones you see here.

96
00:05:24.300 --> 00:05:27.500
We talked about the train validation test stages.

97
00:05:28.700 --> 00:05:31.400
And the purpose for dividing a data

98
00:05:31.400 --> 00:05:34.500
 set into two or three such splits. You can

99
00:05:34.500 --> 00:05:34.900
 see here. We have a

100
00:05:38.900 --> 00:05:41.300
80% is going to the training

101
00:05:41.300 --> 00:05:44.200
 data set 20% is going to the test data set.

102
00:05:44.900 --> 00:05:45.100
so

103
00:05:47.200 --> 00:05:49.900
if you had a validation it would be typically a 60.

104
00:05:50.900 --> 00:05:52.100
20 split

105
00:05:52.600 --> 00:05:55.300
Or 50 25 25. The point

106
00:05:55.300 --> 00:05:57.300
 is the majority goes to the training data set.

107
00:05:59.500 --> 00:06:02.500
And then we use a math problem

108
00:06:02.500 --> 00:06:05.300
 and this for Loop to create a grid of

109
00:06:05.300 --> 00:06:08.100
 samples. So a sample from each class.

110
00:06:09.900 --> 00:06:11.200
or a few samples from each class

111
00:06:14.200 --> 00:06:16.800
so this model that we're going to create is very

112
00:06:20.600 --> 00:06:23.800
Basic so it can only predict images of this sort.

113
00:06:24.500 --> 00:06:27.700
If you want to predict something that is much larger

114
00:06:27.700 --> 00:06:30.300
 in resolution, maybe even busier than you

115
00:06:30.300 --> 00:06:30.800
 would need more.

116
00:06:31.500 --> 00:06:32.900
elaborate Network

117
00:06:34.100 --> 00:06:37.400
But this is a good starting point for us. Let's continue. And then well we

118
00:06:37.400 --> 00:06:40.200
 did is we went ahead and instantiated the sequential class which allows us

119
00:06:40.200 --> 00:06:41.300
 to add a sequence of layers.

120
00:06:42.700 --> 00:06:44.500
and here we talked about the layers we have the

121
00:06:47.200 --> 00:06:50.200
I have the convolutional 2D layer and always the first

122
00:06:52.500 --> 00:06:56.400
Element in the sequence of layers will

123
00:06:55.400 --> 00:06:57.600
 act also as the input layer.

124
00:06:58.800 --> 00:07:00.200
So that is why we have this argument.

125
00:07:03.100 --> 00:07:05.600
That's the activation function. I'll also tell you about value by the way.

126
00:07:06.200 --> 00:07:08.200
I don't listen to the agenda, but I'll tell you about value.

127
00:07:09.100 --> 00:07:11.700
And we'll also go to tensor.

128
00:07:13.900 --> 00:07:16.300
playground tensorflow.com so

129
00:07:16.300 --> 00:07:16.600
 I can show you

130
00:07:18.600 --> 00:07:19.300
an animated

131
00:07:22.900 --> 00:07:23.700
illustration of

132
00:07:24.900 --> 00:07:26.800
activation functions and neural networks

133
00:07:29.900 --> 00:07:32.000
We have our Max pooling layer and we do

134
00:07:32.200 --> 00:07:35.900
 this a couple of times because if you remember we said that we want to have convo 2D

135
00:07:35.900 --> 00:07:38.100
 Max pool converted Max pool so that we can

136
00:07:38.100 --> 00:07:38.500
 further.

137
00:07:39.800 --> 00:07:42.600
Extract the important features from the original image.

138
00:07:45.400 --> 00:07:48.300
Then we flatten it because we want to have a distribution. By the

139
00:07:48.300 --> 00:07:51.300
 way, when I turn to sigmoid, I'll talk about this. We will

140
00:07:51.300 --> 00:07:52.800
 revisit the concept of distribution again.

141
00:07:54.100 --> 00:07:56.400
Not sigmoid the cross entropy.

142
00:07:57.200 --> 00:08:00.400
But anyway, we will go from a matrix that represents

143
00:08:00.400 --> 00:08:04.000
 the pictures of the picture to affect your

144
00:08:03.700 --> 00:08:06.400
 corresponding to the distribution of

145
00:08:06.400 --> 00:08:06.800
 features.

146
00:08:11.200 --> 00:08:14.900
And then finally we end up we have this softmax activation.

147
00:08:19.300 --> 00:08:22.400
And this is something you have to keep remembering is

148
00:08:22.400 --> 00:08:25.700
 if softmax is a synonym for sigmoid, but

149
00:08:25.700 --> 00:08:28.400
 I'll return to this now. Let me show you the remaining

150
00:08:28.400 --> 00:08:28.600
 code.

151
00:08:29.900 --> 00:08:30.800
before we talk about

152
00:08:32.400 --> 00:08:34.400
the softmax or sigmoid

153
00:08:35.500 --> 00:08:36.400
activation function

154
00:08:37.500 --> 00:08:38.900
So if I scroll down here?

155
00:08:43.200 --> 00:08:44.000
I am going to go.

156
00:08:47.400 --> 00:08:48.600
review the

157
00:08:50.400 --> 00:08:52.100
model by typing model

158
00:08:53.300 --> 00:08:53.900
dot summary

159
00:08:54.800 --> 00:08:57.700
the summary method is going to create a table that

160
00:09:00.700 --> 00:09:02.400
indicates our architecture

161
00:09:03.400 --> 00:09:06.000
and the number of parameters of this model has learned.

162
00:09:06.900 --> 00:09:08.000
So if I run this now?

163
00:09:19.300 --> 00:09:21.500
Give it a few seconds for the data set to be downloaded.

164
00:09:43.400 --> 00:09:45.900
And why is it complex?

165
00:09:47.900 --> 00:09:49.100
missing parenthesis

166
00:10:02.200 --> 00:10:05.400
So here you can see your the

167
00:10:05.400 --> 00:10:07.500
 architecture of your neuron at work.

168
00:10:08.500 --> 00:10:11.300
And it says here that you can learn this model can

169
00:10:11.300 --> 00:10:13.400
 learn a total of 122,000.

170
00:10:14.100 --> 00:10:14.700
parameters

171
00:10:16.100 --> 00:10:19.300
which relative to other pre-trained models this is very

172
00:10:23.400 --> 00:10:26.900
insignificant like the resident 50 model

173
00:10:26.900 --> 00:10:27.700
 I think has

174
00:10:29.300 --> 00:10:31.000
six digit or seven digit

175
00:10:32.300 --> 00:10:34.100
so it's it's billions of parameters.

176
00:10:35.300 --> 00:10:37.300
The more you parameters a model can learn.

177
00:10:37.900 --> 00:10:39.500
the well

178
00:10:41.400 --> 00:10:41.800
the more

179
00:10:45.200 --> 00:10:47.700
capabilities the more our variation can handle

180
00:10:48.800 --> 00:10:51.900
anyone heard of gpt3 or gpt2

181
00:10:57.400 --> 00:10:58.300
Once is yes.

182
00:10:59.200 --> 00:11:02.300
So for context gpt3 which is this machine

183
00:11:02.300 --> 00:11:03.800
 learning model that can reproduce.

184
00:11:04.800 --> 00:11:05.500
articles

185
00:11:06.900 --> 00:11:09.500
right poems compose music you

186
00:11:09.500 --> 00:11:12.500
 can compose music if you go to YouTube and type in GB3

187
00:11:12.500 --> 00:11:12.900
 music.

188
00:11:14.200 --> 00:11:16.600
It is at least to Lament.

189
00:11:17.900 --> 00:11:18.600
such as myself

190
00:11:19.900 --> 00:11:22.900
it is it is so beautiful the

191
00:11:22.900 --> 00:11:25.900
 music it's indistinguishable from the composition

192
00:11:25.900 --> 00:11:27.700
 of a famous composer.

193
00:11:28.600 --> 00:11:31.300
But when you read a poetry you can't tell this is done by robots

194
00:11:31.300 --> 00:11:34.200
 or if you read the article in a newspaper.

195
00:11:34.800 --> 00:11:36.500
You can't tell it was written by a machine.

196
00:11:38.300 --> 00:11:41.700
And the gpt3 let's see how many parameters that

197
00:11:41.700 --> 00:11:44.900
 model has gpt3 parameters.

198
00:11:48.400 --> 00:11:50.400
Look at that 175 billion.

199
00:11:55.600 --> 00:11:56.500
So they've just taken.

200
00:11:59.500 --> 00:12:00.300
this concept

201
00:12:01.900 --> 00:12:04.400
of learning weights to the extreme

202
00:12:06.300 --> 00:12:10.800
and it's only a matter of time before the even exceed this.

203
00:12:12.600 --> 00:12:15.600
but I mean, this is what I will take for you to do one

204
00:12:15.600 --> 00:12:17.800
 or not one or a couple of

205
00:12:19.800 --> 00:12:22.200
things that a human is capable of doing if you remember we talked

206
00:12:22.200 --> 00:12:24.700
 about narrow AI this is still narrow AI.

207
00:12:25.500 --> 00:12:28.800
This is still not right. It's not even General AI but it's still

208
00:12:28.800 --> 00:12:29.500
 impressive nonetheless.

209
00:12:32.400 --> 00:12:35.200
Okay. So yeah, I mean compared to that this

210
00:12:35.200 --> 00:12:38.200
 is this is insignificant, but that's not the point of this is not

211
00:12:38.200 --> 00:12:41.400
 like I said to create a production level production ready machine learning

212
00:12:41.400 --> 00:12:42.100
 model is just understand.

213
00:12:43.600 --> 00:12:45.000
the linear algebra for

214
00:12:46.100 --> 00:12:48.400
and probability theory for deep learning.

215
00:12:49.400 --> 00:12:52.300
Okay, let's continue. What else do I have we have next?

216
00:12:53.300 --> 00:12:55.700
And then okay. So now that we've trained this model we want to

217
00:13:01.300 --> 00:13:04.400
Train it. So we've built a model we haven't traded yet.

218
00:13:05.200 --> 00:13:06.900
so the train that we're going to use the

219
00:13:08.700 --> 00:13:09.600
compile method

220
00:13:10.700 --> 00:13:12.100
So that the model knows.

221
00:13:12.900 --> 00:13:16.900
Which algorithm to use to back propagate

222
00:13:16.900 --> 00:13:19.100
 the weights again, if you watch the video that I

223
00:13:19.100 --> 00:13:19.900
 put on Slack?

224
00:13:20.900 --> 00:13:23.100
This idea should be familiar to you.

225
00:13:24.800 --> 00:13:27.300
But the idea is to keep on.

226
00:13:29.500 --> 00:13:32.300
Feeding this neural network these pictures again and again

227
00:13:32.300 --> 00:13:32.700
 and again.

228
00:13:34.500 --> 00:13:34.700
and each time

229
00:13:35.500 --> 00:13:39.000
it is fed through the network the updated wage.

230
00:13:38.200 --> 00:13:41.000
 Remember we the point here is to adjust the weights.

231
00:13:42.300 --> 00:13:45.000
And bring get the model to predict.

232
00:13:48.300 --> 00:13:48.700
images

233
00:13:51.200 --> 00:13:54.100
with labels that are closest to the original label so we

234
00:13:54.100 --> 00:13:54.300
 want

235
00:13:55.700 --> 00:13:58.100
The difference to be eliminated we want to

236
00:13:58.100 --> 00:13:59.300
 reduce error to zero.

237
00:14:00.200 --> 00:14:03.800
And empirically we'll do this with cross entropy,

238
00:14:03.800 --> 00:14:05.700
 which we'll talk about in a moment.

239
00:14:06.700 --> 00:14:09.100
I'm going to go model. Excuse me model that compile.

240
00:14:11.800 --> 00:14:12.900
the optimizer

241
00:14:14.400 --> 00:14:17.500
is Adam Adam stands for Adam is actually an acronym.

242
00:14:18.500 --> 00:14:19.200
adaptive

243
00:14:22.100 --> 00:14:25.800
I always keep forgetting what it would adaptive something adapt

244
00:14:25.800 --> 00:14:27.800
 and I'll go to them.

245
00:14:30.400 --> 00:14:32.400
It's the algorithm for.

246
00:14:46.200 --> 00:14:49.100
There we go, adaptive moment estimation. Look how far I have to come just

247
00:14:49.100 --> 00:14:49.300
 to get.

248
00:14:51.400 --> 00:14:53.100
the need for this

249
00:14:55.000 --> 00:14:56.000
Acronym

250
00:14:57.600 --> 00:14:59.200
adaptive moment estimation

251
00:15:03.200 --> 00:15:04.100
Moment.

252
00:15:05.300 --> 00:15:07.700
in statistics is a

253
00:15:13.900 --> 00:15:16.200
is one of the detail that describes the shape of

254
00:15:16.200 --> 00:15:16.500
 the data.

255
00:15:17.600 --> 00:15:19.200
So one moment is for example.

256
00:15:20.200 --> 00:15:23.100
Variance, that's the spread of the data.

257
00:15:24.400 --> 00:15:25.800
Another moment is the mean.

258
00:15:26.500 --> 00:15:28.700
The center the central tendency of the data.

259
00:15:29.500 --> 00:15:31.300
Another one is the skewness.

260
00:15:32.300 --> 00:15:35.300
So if it has a long right tail or a left tail and

261
00:15:35.300 --> 00:15:37.200
 kurtosis is the peak peakness.

262
00:15:38.100 --> 00:15:41.200
Now this word is borrowed from physics, but it has a slightly different

263
00:15:41.200 --> 00:15:44.300
 meaning in physics. But an estimation is

264
00:15:44.300 --> 00:15:44.700
 well.

265
00:15:45.800 --> 00:15:46.400
It's prediction.

266
00:15:49.100 --> 00:15:52.200
But the naming name aside this is

267
00:15:52.200 --> 00:15:55.600
 going to be used to find the or adjust the weights. Then

268
00:15:55.600 --> 00:15:56.600
 we have the loss function.

269
00:15:58.100 --> 00:15:58.500
which is

270
00:16:00.800 --> 00:16:02.000
TF dot Keras

271
00:16:03.000 --> 00:16:03.400
Dot

272
00:16:04.100 --> 00:16:04.800
Sports

273
00:16:06.300 --> 00:16:07.300
categorical

274
00:16:08.900 --> 00:16:09.400
cross

275
00:16:10.900 --> 00:16:13.400
entropy now cross entropy is a loss function.

276
00:16:14.500 --> 00:16:15.200
That can be used.

277
00:16:17.100 --> 00:16:18.500
for binary classification

278
00:16:20.100 --> 00:16:23.200
But in this case since we have every picture you can fall into either one

279
00:16:23.200 --> 00:16:25.300
 of 10 classes we want.

280
00:16:28.600 --> 00:16:31.800
Well, we have multi-class classification. So

281
00:16:31.800 --> 00:16:34.100
 we will use the sparse variant of

282
00:16:34.100 --> 00:16:36.300
 it again. We'll look at the moment of this in a second.

283
00:16:37.100 --> 00:16:39.500
And then we go from logics.

284
00:16:46.200 --> 00:16:49.100
You'll see what a larger it is in a second from large. It's equals true.

285
00:16:55.300 --> 00:16:56.300
and metrics

286
00:16:59.300 --> 00:17:01.400
for a classification the

287
00:17:03.600 --> 00:17:04.700
we have a couple of metrics.

288
00:17:08.200 --> 00:17:10.500
There is accuracy precision and recall.

289
00:17:12.100 --> 00:17:15.100
If this time I will talk about precision and recall.

290
00:17:15.800 --> 00:17:18.200
But basically we want this number to

291
00:17:18.200 --> 00:17:19.400
 convert to one.

292
00:17:20.700 --> 00:17:23.100
So we want accuracy to be 100%

293
00:17:24.300 --> 00:17:26.500
but we want the laws cross entropy.

294
00:17:27.400 --> 00:17:30.200
To convert to zero. So again, you're looking at two things the last

295
00:17:30.200 --> 00:17:31.700
 function this has to be zero.

296
00:17:32.300 --> 00:17:33.500
The closet is either the better.

297
00:17:34.700 --> 00:17:36.600
And the met the accuracy.

298
00:17:37.400 --> 00:17:38.600
The closer to one the better.

299
00:17:39.200 --> 00:17:40.700
Let me put these on separate lines.

300
00:17:50.800 --> 00:17:52.000
And then we train the model.

301
00:18:00.400 --> 00:18:01.100
compile

302
00:18:08.100 --> 00:18:11.700
and then we go model dot train.

303
00:18:14.800 --> 00:18:16.700
So this is going to learn from the first.

304
00:18:17.700 --> 00:18:18.400
safety

305
00:18:18.800 --> 00:18:19.600
thousand images

306
00:18:21.700 --> 00:18:22.700
but then to evaluate

307
00:18:23.800 --> 00:18:26.600
this model to see if it's overfitting or not. We'll use

308
00:18:26.600 --> 00:18:28.300
 the test data set the question the chart.

309
00:18:29.400 --> 00:18:32.200
With the model not be overfitting of the accuracy is 100%

310
00:18:34.100 --> 00:18:35.300
not if it gets to test.

311
00:18:36.100 --> 00:18:37.200
If it does no.

312
00:18:38.300 --> 00:18:40.400
if we look at the metric after training

313
00:18:45.200 --> 00:18:48.200
and if it's a very high score, it's really unlikely to

314
00:18:48.200 --> 00:18:49.700
 be 100% But if it's very high.

315
00:18:50.500 --> 00:18:53.200
It could be overfitting. Well, the only way

316
00:18:53.200 --> 00:18:56.400
 for us to know if it is overfitting is to get to the test data set.

317
00:18:58.400 --> 00:19:01.400
But if it's poor even from the start even from

318
00:19:01.400 --> 00:19:04.100
 the 20% then that's on their fitting so we

319
00:19:04.100 --> 00:19:07.400
 don't even need to proceed to the test test it is

320
00:19:07.400 --> 00:19:07.500
 that

321
00:19:10.800 --> 00:19:13.200
So yes, if it's perfect 100% something might be.

322
00:19:14.400 --> 00:19:16.500
something might be off because it's

323
00:19:17.200 --> 00:19:19.700
very unlikely for you to get a hundred percent accuracy.

324
00:19:22.700 --> 00:19:26.300
Okay, then what am I looking for? Yes training images.

325
00:19:27.600 --> 00:19:30.400
the train labels which she correspond to the class of

326
00:19:30.400 --> 00:19:30.700
 the

327
00:19:32.700 --> 00:19:33.100
picture

328
00:19:38.500 --> 00:19:39.200
and which

329
00:19:41.100 --> 00:19:44.400
Data set. We're going to use for validating.

330
00:19:45.800 --> 00:19:46.400
validation

331
00:19:49.100 --> 00:19:51.600
so for validation, we're going to use a test data set.

332
00:19:52.600 --> 00:19:53.900
So I'm going to go test images.

333
00:19:55.900 --> 00:19:57.100
to pull test images

334
00:20:00.100 --> 00:20:00.700
test

335
00:20:03.300 --> 00:20:04.600
that would be less.

336
00:20:07.200 --> 00:20:08.100
I'm going to run this.

337
00:20:19.200 --> 00:20:20.800
I think cross entropy is one word.

338
00:20:21.600 --> 00:20:21.900
Yeah.

339
00:20:28.700 --> 00:20:31.300
And it's losses dot losses.

340
00:20:37.300 --> 00:20:37.600
Okay.

341
00:20:44.900 --> 00:20:46.100
dot it

342
00:20:49.000 --> 00:20:49.400
careful

343
00:20:55.500 --> 00:20:58.400
And then the other mistake is the epochs number of

344
00:20:58.400 --> 00:20:59.200
 epochs.

345
00:21:00.800 --> 00:21:01.600
a box

346
00:21:02.500 --> 00:21:05.600
Epox refers to the number of times you're going to feed this data

347
00:21:05.600 --> 00:21:06.100
 through the network.

348
00:21:08.300 --> 00:21:11.200
For larger models you want to be careful with

349
00:21:11.200 --> 00:21:14.300
 this? Because if you set a high number of epochs while that

350
00:21:14.300 --> 00:21:17.100
 may lead to higher accuracy, it can take

351
00:21:17.100 --> 00:21:18.500
 a long time for it to

352
00:21:20.300 --> 00:21:20.800
terminate

353
00:21:21.700 --> 00:21:23.500
so that that particular cell terminate.

354
00:21:25.300 --> 00:21:27.800
so that is why sometimes maybe one upgrade your

355
00:21:30.500 --> 00:21:33.500
Environment in colab you can you have three tiers.

356
00:21:34.600 --> 00:21:37.200
As you keep that in

357
00:21:37.200 --> 00:21:39.100
 mind, and this should be validation data.

358
00:21:39.800 --> 00:21:40.600
one more time

359
00:21:44.100 --> 00:21:46.800
Hopefully this won't take more than a few minutes, but I'll just proceed with.

360
00:21:47.500 --> 00:21:50.500
The other lines of code, okay and what

361
00:21:50.500 --> 00:21:51.200
 we do after this.

362
00:21:52.300 --> 00:21:54.400
Is let me double check my code.

363
00:21:56.500 --> 00:21:57.500
We are going to

364
00:22:03.400 --> 00:22:06.500
yeah, we will we will evaluate the metrics and then

365
00:22:06.500 --> 00:22:08.100
 proceed to predict the sample image.

366
00:22:08.800 --> 00:22:10.300
So I'm at a cell here.

367
00:22:14.200 --> 00:22:15.200
evaluate

368
00:22:15.800 --> 00:22:16.700
model metrics

369
00:22:18.600 --> 00:22:21.300
so we want to look at the loss and the accuracy and then

370
00:22:21.300 --> 00:22:24.400
 after that we will go ahead and predict sample image.

371
00:22:33.400 --> 00:22:33.700
the

372
00:22:34.600 --> 00:22:36.600
two steps that we're not going to look at today are

373
00:22:37.500 --> 00:22:40.000
saving the weights of the model. So there are

374
00:22:40.800 --> 00:22:43.100
 a few file formats in which you could save the parameters of

375
00:22:43.100 --> 00:22:46.100
 the model like CSV. There's one called H5.

376
00:22:47.500 --> 00:22:50.500
And another thing we won't be looking at in this course is deploying the

377
00:22:50.500 --> 00:22:53.600
 machine learning model. So go to something like AWS or

378
00:22:53.600 --> 00:22:55.700
 Google cloud and deploying this.

379
00:22:57.400 --> 00:23:00.100
Depending on the model you or Library. I should

380
00:23:00.100 --> 00:23:01.800
 say you may have to

381
00:23:04.100 --> 00:23:07.100
go to something like compute engine or

382
00:23:08.800 --> 00:23:11.800
An ec2 instance on AWS and

383
00:23:11.800 --> 00:23:14.500
 then you would need to use Docker to download the

384
00:23:14.500 --> 00:23:17.400
 image and create a container.

385
00:23:17.400 --> 00:23:20.000
 So it will go into more of MLPs.

386
00:23:20.700 --> 00:23:23.200
That's what it takes to deploy a machine learning model.

387
00:23:23.200 --> 00:23:26.600
 In other cases if using Library like tensorflow on Google Cloud

388
00:23:26.600 --> 00:23:29.300
 specifically, you can just upload the script and

389
00:23:29.300 --> 00:23:30.600
 Google Cloud will take care of

390
00:23:31.800 --> 00:23:34.800
You know preparing the environment and by environment, I mean installing

391
00:23:34.800 --> 00:23:38.100
 operating system on the the hardware.

392
00:23:39.600 --> 00:23:43.200
Installing the right version of python installing the library dependencies.

393
00:23:44.100 --> 00:23:47.800
Process if you want to work on a large ml project in the cloud check out kubeflow.

394
00:23:47.800 --> 00:23:48.100
 Yes.

395
00:23:49.500 --> 00:23:52.500
Now I've used a Google Cloud mostly for my

396
00:23:52.500 --> 00:23:54.600
 machine learning projects. But yeah, I mean

397
00:23:55.300 --> 00:23:58.200
my point is that we will be going from data science to

398
00:23:58.200 --> 00:23:59.400
 data engineering if we

399
00:23:59.700 --> 00:24:00.500
get to that.

400
00:24:01.200 --> 00:24:02.000
And go with that topic.

401
00:24:04.400 --> 00:24:06.700
Maybe we can do a tutorial for that in another session.

402
00:24:07.700 --> 00:24:10.600
Okay, so I can't run these two cells until

403
00:24:10.600 --> 00:24:11.200
 that one is done.

404
00:24:12.300 --> 00:24:15.700
Let me at least see what I can do. Okay, let's just

405
00:24:15.700 --> 00:24:18.300
 switch then now until this, you know is cooking

406
00:24:18.300 --> 00:24:18.800
 at the background.

407
00:24:19.600 --> 00:24:21.000
and talk about

408
00:24:25.500 --> 00:24:27.100
activation functions first

409
00:24:30.300 --> 00:24:31.000
let's talk about

410
00:24:35.500 --> 00:24:37.500
value and then let's talk about softmax.

411
00:24:40.500 --> 00:24:43.400
Keep those who open source producer runs on GCS

412
00:24:43.400 --> 00:24:44.100
 or their AWS.

413
00:24:45.800 --> 00:24:46.000
cool

414
00:24:47.400 --> 00:24:47.600
now

415
00:24:48.800 --> 00:24:50.900
I need to visualize a value.

416
00:24:52.500 --> 00:24:52.800
and

417
00:24:56.600 --> 00:24:57.800
talk about the activation.

418
00:24:59.800 --> 00:25:02.200
So let me actually put up the slide.

419
00:25:03.800 --> 00:25:04.500
slides

420
00:25:06.300 --> 00:25:07.300
let's make sure we know what

421
00:25:08.800 --> 00:25:10.900
an activation is and then we'll talk about

422
00:25:13.200 --> 00:25:14.200
activation functions

423
00:25:15.500 --> 00:25:15.700
so

424
00:25:28.500 --> 00:25:30.600
Let me jump to the relevant slide.

425
00:25:33.900 --> 00:25:34.200
Here we go.

426
00:26:10.100 --> 00:26:13.400
Okay, so this is quite busy. So let

427
00:26:13.400 --> 00:26:14.000
 me try to

428
00:26:20.100 --> 00:26:21.100
simplify it

429
00:26:22.400 --> 00:26:23.800
I'm going to duplicate this.

430
00:26:27.500 --> 00:26:29.200
And make a copy of this by the way.

431
00:26:34.600 --> 00:26:38.700
I'm going to tap this for big number our course here.

432
00:27:16.100 --> 00:27:17.300
duplicate this

433
00:27:18.200 --> 00:27:22.100
Because I want to remove these labels

434
00:27:21.100 --> 00:27:22.700
 for now.

435
00:27:23.800 --> 00:27:26.100
So this is a very simple.

436
00:27:27.500 --> 00:27:28.400
neural network

437
00:27:30.900 --> 00:27:31.900
so this

438
00:27:37.300 --> 00:27:38.800
Is this line of code here?

439
00:27:49.500 --> 00:27:53.000
now, of course for the input, we don't have a tensor

440
00:27:52.200 --> 00:27:53.600
 the input for

441
00:27:54.900 --> 00:27:57.100
This network is only two features.

442
00:28:01.600 --> 00:28:04.600
And the dense is three so we have three outputs.

443
00:28:07.100 --> 00:28:11.100
Okay, let's see. Now. We said that neural network are neural

444
00:28:10.100 --> 00:28:11.200
 networks.

445
00:28:12.700 --> 00:28:14.600
Our matrices but this is a graphic.

446
00:28:16.400 --> 00:28:17.100
representation

447
00:28:19.200 --> 00:28:22.600
I also have a matrix representation up

448
00:28:22.600 --> 00:28:22.700
 here.

449
00:28:25.400 --> 00:28:28.300
Let me see if this one has the activation. This one does have

450
00:28:28.300 --> 00:28:29.000
 the activation. Yes.

451
00:28:33.300 --> 00:28:35.100
This is actually what a neural network is doing.

452
00:28:37.500 --> 00:28:40.500
Let me just see if I can remember what this is. Okay. Yeah, so let

453
00:28:40.500 --> 00:28:43.900
 me show you the graph representation. Then we'll look at the Matrix since

454
00:28:43.900 --> 00:28:45.600
 we've already covered, you know.

455
00:28:49.600 --> 00:28:51.200
matrix multiplication we can

456
00:28:52.200 --> 00:28:55.400
we can talk about this. Let me come up here. So let's say we

457
00:28:55.400 --> 00:28:56.900
 have I will use again the

458
00:28:59.500 --> 00:29:00.500
the real estate example

459
00:29:01.900 --> 00:29:04.600
And let's say this is a

460
00:29:04.600 --> 00:29:05.500
 very simple.

461
00:29:09.400 --> 00:29:10.500
classification model

462
00:29:11.500 --> 00:29:11.600
so

463
00:29:13.600 --> 00:29:15.600
we want to classify a property.

464
00:29:16.600 --> 00:29:19.500
By its category. So for example, is it a warehouse?

465
00:29:20.800 --> 00:29:21.800
or is it a

466
00:29:23.600 --> 00:29:25.500
is an apartment a residential apartment?

467
00:29:26.500 --> 00:29:29.200
Or let's say commercial property like an office

468
00:29:29.200 --> 00:29:32.400
 space in a building or apartment residential apartment.

469
00:29:34.100 --> 00:29:37.700
So how are they different you can have two properties that are similar in

470
00:29:37.700 --> 00:29:37.800
 size?

471
00:29:40.000 --> 00:29:40.300
but

472
00:29:41.500 --> 00:29:43.900
with offices you have less.

473
00:29:45.800 --> 00:29:46.500
partitioning

474
00:29:47.900 --> 00:29:50.500
you have fewer doors because you don't have any rooms.

475
00:29:50.500 --> 00:29:53.400
 No kitchen. Maybe you have a kitchen you don't

476
00:29:53.400 --> 00:29:53.700
 have a

477
00:29:54.600 --> 00:29:55.300
balcony

478
00:29:56.800 --> 00:29:58.400
and maybe it's just

479
00:29:59.600 --> 00:30:01.100
What's the word I'm looking for more open?

480
00:30:02.300 --> 00:30:05.200
Like there are no enclosing there are no partitioning essentially.

481
00:30:07.800 --> 00:30:10.200
so let's say I want is

482
00:30:10.200 --> 00:30:10.500
 the number of

483
00:30:13.200 --> 00:30:16.400
rooms in the property and number two is the

484
00:30:16.400 --> 00:30:19.500
 size of the property exactly wrong open

485
00:30:19.500 --> 00:30:20.300
 floor plan.

486
00:30:21.800 --> 00:30:22.600
Open floor plan.

487
00:30:25.500 --> 00:30:26.900
So obviously there's more to

488
00:30:29.600 --> 00:30:33.200
there's more things that could describe

489
00:30:32.200 --> 00:30:35.300
 to distinguish these two but again a point

490
00:30:35.300 --> 00:30:37.600
 here to understand what an activation is.

491
00:30:39.200 --> 00:30:42.700
And this is the context of really the rectified linear.

492
00:30:44.200 --> 00:30:47.100
What does it stand for I keep forgetting I will I'll take you

493
00:30:47.100 --> 00:30:48.200
 to the side of the moment. But here we go.

494
00:30:51.300 --> 00:30:52.100
I1 is the

495
00:30:53.200 --> 00:30:56.000
number of rooms I2 is the size of

496
00:30:56.300 --> 00:30:56.300
 the property.

497
00:30:58.900 --> 00:30:59.600
And we have three.

498
00:31:02.200 --> 00:31:05.100
Perceptrons in our one hidden layer. So

499
00:31:05.100 --> 00:31:06.100
 this is one hidden layer.

500
00:31:06.600 --> 00:31:08.200
one input one hidden one output

501
00:31:09.200 --> 00:31:11.500
What comes out of this output layer?

502
00:31:12.200 --> 00:31:12.500
is

503
00:31:15.500 --> 00:31:18.300
is the is the classification is a type

504
00:31:18.300 --> 00:31:21.500
 1 a property or the type 2 property? Okay. Let's

505
00:31:21.500 --> 00:31:22.100
 see how this is done.

506
00:31:23.100 --> 00:31:27.800
So what we do is if you look at these arrows that stem

507
00:31:26.800 --> 00:31:29.000
 from for example I2.

508
00:31:30.600 --> 00:31:34.000
I too will send to every perceptron.

509
00:31:36.300 --> 00:31:36.900
It's value.

510
00:31:38.100 --> 00:31:38.400
However

511
00:31:40.600 --> 00:31:43.300
It is not just setting the feature. It is also sending a

512
00:31:43.300 --> 00:31:46.300
 random weight and a different way to each

513
00:31:46.300 --> 00:31:46.900
 perceptron.

514
00:31:47.300 --> 00:31:50.700
So for example, they first perceptron

515
00:31:50.700 --> 00:31:52.200
 might get a weight of 5.

516
00:31:53.600 --> 00:31:55.900
The second perception may get a weight of one.

517
00:31:57.600 --> 00:32:00.200
And the third perceptor may get a weight of 10.

518
00:32:01.400 --> 00:32:04.200
So one perceptron will consider the feature to

519
00:32:04.200 --> 00:32:07.500
 be someone important on another one not so

520
00:32:07.500 --> 00:32:08.400
 important another one very important.

521
00:32:10.600 --> 00:32:12.200
And then we do the same thing with the first feature.

522
00:32:13.200 --> 00:32:14.500
We send.

523
00:32:15.300 --> 00:32:18.800
The value to all three perceptrons, but we associated with

524
00:32:18.800 --> 00:32:20.300
 the with different weights.

525
00:32:23.600 --> 00:32:24.700
That's what the slide is about.

526
00:32:26.100 --> 00:32:27.200
So this is nothing more than

527
00:32:28.800 --> 00:32:30.100
some arithmetic

528
00:32:30.800 --> 00:32:33.100
So here we can see we have the feature multiplied by

529
00:32:33.100 --> 00:32:34.100
 its random weight.

530
00:32:36.100 --> 00:32:37.400
We have the second feature.

531
00:32:38.600 --> 00:32:39.800
and of course all of this

532
00:32:40.600 --> 00:32:43.300
arithmetic is happening inside the crystal chapter. That's

533
00:32:43.300 --> 00:32:45.200
 what you that's why you have this summation.

534
00:32:46.100 --> 00:32:48.100
or this Sigma Sigma represents summation

535
00:32:49.000 --> 00:32:49.200
the

536
00:32:50.900 --> 00:32:53.100
integral sign here actually refers to the

537
00:32:58.700 --> 00:33:02.000
well, this is used for continuous values,

538
00:33:01.000 --> 00:33:02.100
 right?

539
00:33:03.100 --> 00:33:06.100
This is used for continuous value. So if you have discrete value you use the

540
00:33:06.100 --> 00:33:09.100
 sigma if you have continuous values you have.

541
00:33:09.800 --> 00:33:11.400
a definite integral

542
00:33:12.800 --> 00:33:13.100
anyway

543
00:33:13.900 --> 00:33:16.100
We multiply each feature by its rather wait.

544
00:33:17.300 --> 00:33:19.000
So you only have two features here. So.

545
00:33:19.800 --> 00:33:22.300
And since this is for the first picture perceptron, we'll

546
00:33:22.300 --> 00:33:25.200
 multiply it by its way and then we have the the sum of the products.

547
00:33:26.100 --> 00:33:26.800
so let's say

548
00:33:28.300 --> 00:33:28.600
that

549
00:33:30.800 --> 00:33:32.800
this sum is 32.

550
00:33:34.200 --> 00:33:37.100
We will then take this number 32 and pass it to the

551
00:33:37.100 --> 00:33:39.400
 activation function this Greek letter of Phi.

552
00:33:40.200 --> 00:33:41.500
represent the activation function

553
00:33:42.200 --> 00:33:44.600
which what does this activation function? Well, it depends.

554
00:33:45.500 --> 00:33:47.300
It depends on what you use.

555
00:33:49.800 --> 00:33:52.100
For your layer if I bring

556
00:33:52.100 --> 00:33:55.600
 you what let me keep a note. What's the slide number one five two? So

557
00:33:55.600 --> 00:33:58.200
 if I take you to the slide for the activation functions

558
00:34:05.400 --> 00:34:06.100
these are the

559
00:34:08.500 --> 00:34:10.000
activation functions you can choose

560
00:34:16.100 --> 00:34:17.800
so this is the rectified linear unit.

561
00:34:21.100 --> 00:34:24.600
For the others, I'll show you. I'll take you to the playground to talk

562
00:34:24.600 --> 00:34:25.000
 about this.

563
00:34:26.800 --> 00:34:28.200
now for the

564
00:34:29.800 --> 00:34:32.300
for any layer, except the output layer you never

565
00:34:32.300 --> 00:34:32.800
 want to use

566
00:34:33.700 --> 00:34:36.200
A linear activation function or here as

567
00:34:36.200 --> 00:34:37.600
 it's known as the identity function.

568
00:34:38.400 --> 00:34:38.700
because

569
00:34:42.900 --> 00:34:45.300
well, let me tell you about let me tell you how really works

570
00:34:45.300 --> 00:34:47.100
 and then you'll see why you don't want to use a linear function.

571
00:34:51.800 --> 00:34:53.500
If you remember we said that a

572
00:34:55.400 --> 00:34:58.200
Neural network has a basic component which we

573
00:34:58.200 --> 00:35:01.200
 call the perceptron and the perceptron is supposed

574
00:35:01.200 --> 00:35:02.200
 to be a mathematical model.

575
00:35:04.200 --> 00:35:06.500
of a human brain neuron

576
00:35:09.300 --> 00:35:11.800
and a neuron will pass a signal.

577
00:35:12.800 --> 00:35:15.500
Forward if the signal is strong enough.

578
00:35:17.200 --> 00:35:21.100
In the context of new networks if it's relevant enough the

579
00:35:20.100 --> 00:35:21.700
 value function.

580
00:35:23.500 --> 00:35:24.800
well, establish this

581
00:35:26.000 --> 00:35:26.600
threshold

582
00:35:27.200 --> 00:35:28.500
so the value is for

583
00:35:29.700 --> 00:35:30.700
setting a threshold.

584
00:35:31.700 --> 00:35:33.800
Let me show you something else. I did not talk about.

585
00:35:34.900 --> 00:35:37.600
This is slide 108 if I return to 152.

586
00:35:39.400 --> 00:35:41.100
You will notice we have these arrows.

587
00:35:42.600 --> 00:35:44.300
These arrows are biases.

588
00:35:49.400 --> 00:35:49.500
so

589
00:35:54.800 --> 00:35:57.600
let me take you to this mod

590
00:35:57.600 --> 00:35:58.500
 so I can graph it for you.

591
00:36:00.600 --> 00:36:02.900
there's Mass graphic calculator very nice application this

592
00:36:03.800 --> 00:36:04.500
This much.

593
00:36:05.500 --> 00:36:05.800
so

594
00:36:07.400 --> 00:36:09.000
I don't suppose we could do piecewise here.

595
00:36:13.200 --> 00:36:14.600
But I think I have a slide here.

596
00:36:21.700 --> 00:36:22.400
It was one of

597
00:36:24.500 --> 00:36:24.700
eight

598
00:36:48.900 --> 00:36:52.000
See if I think if I can do this in Desmos or

599
00:36:51.000 --> 00:36:52.100
 not.

600
00:37:13.900 --> 00:37:16.200
But I don't know how to set a range.

601
00:37:20.900 --> 00:37:21.200
Okay.

602
00:37:24.900 --> 00:37:27.900
There's more.com. Yeah, I

603
00:37:27.900 --> 00:37:30.300
 think I'm going to have to draw this out drawing it

604
00:37:30.300 --> 00:37:30.800
 out will be.

605
00:37:31.600 --> 00:37:32.600
a lot faster

606
00:37:33.200 --> 00:37:35.400
Let me quickly. Let me quickly join Zoom here.

607
00:37:39.300 --> 00:37:42.100
Because I have to move the graph around and not sure how to

608
00:37:42.100 --> 00:37:42.900
 do it in decimals.

609
00:37:44.800 --> 00:37:46.400
Or no, I can't do it online here.

610
00:37:48.300 --> 00:37:51.200
Sorry about going back and forth. I'm just trying to do this in the most.

611
00:37:52.800 --> 00:37:55.400
Efficient way possible I can draw

612
00:37:55.400 --> 00:37:57.500
 diagram here. So let me

613
00:38:05.200 --> 00:38:05.900
create a new one.

614
00:38:19.900 --> 00:38:22.000
as let's say this is

615
00:38:22.200 --> 00:38:23.500
 our code that

616
00:38:25.000 --> 00:38:25.300
hot

617
00:38:28.300 --> 00:38:31.200
thanks. Yeah, I I've also used you. I mean I played around with

618
00:38:32.700 --> 00:38:35.600
future but since I'm not, you know

619
00:38:35.600 --> 00:38:38.900
 fluent, I don't want to play around

620
00:38:38.900 --> 00:38:41.100
 like I don't want to waste time but this is

621
00:38:41.100 --> 00:38:44.800
 simple. Okay. So this is our this is our coordinates system.

622
00:38:44.800 --> 00:38:47.300
 Yes and the value

623
00:38:47.300 --> 00:38:49.100
 does this the value?

624
00:38:51.500 --> 00:38:52.000
will

625
00:38:55.100 --> 00:38:57.800
evaluate to zero for every value of x

626
00:38:59.200 --> 00:39:00.100
except for

627
00:39:03.300 --> 00:39:04.000
when you set

628
00:39:06.500 --> 00:39:09.900
a threshold now in the value that we see here. The threshold

629
00:39:09.900 --> 00:39:10.500
 is zero.

630
00:39:14.200 --> 00:39:15.100
This is our bias.

631
00:39:16.200 --> 00:39:17.200
the bias here

632
00:39:18.500 --> 00:39:19.400
is assumed to be

633
00:39:20.600 --> 00:39:21.300
is 0

634
00:39:24.300 --> 00:39:24.900
Okay, so

635
00:39:26.900 --> 00:39:30.300
let me do the following. I'm going to come over here and let

636
00:39:29.300 --> 00:39:30.300
 me

637
00:39:31.100 --> 00:39:33.100
change this to the color red.

638
00:39:36.500 --> 00:39:37.800
and we want

639
00:39:46.400 --> 00:39:49.400
it to be linear for every point of x.

640
00:39:50.400 --> 00:39:53.400
After zero is this linear? It is almost linear. Let's

641
00:39:53.400 --> 00:39:54.600
 bring it up here that

642
00:39:56.200 --> 00:39:57.500
So this is the one x axis.

643
00:40:00.300 --> 00:40:02.800
And let's say the x-axis is the size of the property.

644
00:40:08.100 --> 00:40:08.400
Okay.

645
00:40:14.900 --> 00:40:17.000
And the y axis is the activation.

646
00:40:18.700 --> 00:40:20.700
the strength of the signal so to speak

647
00:40:21.500 --> 00:40:22.700
I will use.

648
00:40:24.900 --> 00:40:25.400
There we go.

649
00:40:26.200 --> 00:40:27.200
So the y axis.

650
00:40:29.200 --> 00:40:30.100
Is the activation?

651
00:40:31.100 --> 00:40:31.400
the

652
00:40:39.800 --> 00:40:42.100
this is actually the size of the property times.

653
00:40:44.400 --> 00:40:45.100
the weight

654
00:40:46.800 --> 00:40:49.200
the random weight. So let's say if I

655
00:40:49.200 --> 00:40:52.700
 come back to our slides. I think it was W sub

656
00:40:52.700 --> 00:40:55.200
 2i, but it's a wait just a wait just

657
00:40:56.200 --> 00:40:57.000
some random weight

658
00:40:58.300 --> 00:40:58.700
Okay.

659
00:41:01.400 --> 00:41:04.300
and here the bias is set to zero,

660
00:41:04.300 --> 00:41:04.900
 which means

661
00:41:06.600 --> 00:41:07.400
it is 0

662
00:41:08.200 --> 00:41:09.000
so the origin

663
00:41:10.700 --> 00:41:10.900
okay.

664
00:41:12.200 --> 00:41:15.500
if I return to this unbiased activation

665
00:41:15.500 --> 00:41:15.800
 function

666
00:41:17.900 --> 00:41:20.000
I don't like to use the word unbias because it has a

667
00:41:20.500 --> 00:41:21.400
 specific meanings statistics.

668
00:41:22.100 --> 00:41:23.900
but if you look at this activation function here

669
00:41:25.900 --> 00:41:27.400
the bias is set to 0.

670
00:41:28.500 --> 00:41:31.100
so which means that I mean, it's like an if an

671
00:41:31.100 --> 00:41:33.500
 else, they've been right f of x will give you 0

672
00:41:35.200 --> 00:41:38.300
for any value of x that is less than 0 so basically

673
00:41:38.300 --> 00:41:41.000
 any negative numbers that is negative value is going to

674
00:41:42.100 --> 00:41:43.500
squish it to zero.

675
00:41:45.900 --> 00:41:48.100
but if any value is equal to 0 or above 0

676
00:41:48.800 --> 00:41:51.000
then you will get a you will get the linear you will get

677
00:41:51.500 --> 00:41:52.900
 an identity. So if you get five.

678
00:41:53.700 --> 00:41:56.300
You'll basically really will let

679
00:41:56.300 --> 00:41:58.300
 it through as long as it's above zero.

680
00:41:59.600 --> 00:42:02.200
But let's do this experiment or Let's

681
00:42:02.200 --> 00:42:04.600
 do let's talk about a hypothetical situation.

682
00:42:06.300 --> 00:42:06.900
suppose

683
00:42:09.200 --> 00:42:09.700
this is our

684
00:42:10.900 --> 00:42:11.900
customized value

685
00:42:12.800 --> 00:42:13.700
so I'm going to go text.

686
00:42:24.600 --> 00:42:25.900
Preview it.

687
00:42:36.500 --> 00:42:39.500
And then we will say it is the function

688
00:42:39.500 --> 00:42:40.800
 represented by f of x.

689
00:42:41.500 --> 00:42:44.500
And this will be a piecewise function. So begin.

690
00:42:46.600 --> 00:42:47.900
cases cases

691
00:42:52.300 --> 00:42:52.800
It will give you.

692
00:42:55.100 --> 00:42:55.600
zero

693
00:42:58.200 --> 00:42:58.400
if

694
00:43:03.900 --> 00:43:06.000
the value of x is

695
00:43:07.100 --> 00:43:07.700
greater than

696
00:43:10.600 --> 00:43:11.500
the bias

697
00:43:15.600 --> 00:43:16.200
Excuse me.

698
00:43:18.900 --> 00:43:20.000
Less than the bias.

699
00:43:24.400 --> 00:43:25.600
and it will give you

700
00:43:28.300 --> 00:43:28.600
X

701
00:43:29.900 --> 00:43:30.600
itself

702
00:43:32.900 --> 00:43:33.200
if

703
00:43:34.300 --> 00:43:35.200
it is equal to

704
00:43:37.500 --> 00:43:39.700
basically greater than or equal to the bias.

705
00:43:41.000 --> 00:43:41.500
Yes.

706
00:43:46.400 --> 00:43:48.000
Like a simple if enough statement.

707
00:43:48.700 --> 00:43:49.900
If it's less than bias.

708
00:43:50.800 --> 00:43:52.800
Zero equal to or above bias.

709
00:43:54.100 --> 00:43:55.400
X letter through

710
00:43:57.600 --> 00:44:00.300
So, let me tell you what device does let me

711
00:44:00.300 --> 00:44:00.700
 ask a question.

712
00:44:08.400 --> 00:44:08.600
how

713
00:44:13.400 --> 00:44:16.100
Big does a property have to be.

714
00:44:17.300 --> 00:44:21.400
What is the minimum minimal size the smallest of

715
00:44:20.400 --> 00:44:23.600
 property has to be for to

716
00:44:23.600 --> 00:44:23.900
 be considered?

717
00:44:25.800 --> 00:44:27.400
Either an office or an apartment.

718
00:44:30.200 --> 00:44:32.000
I think if if you're in, New York.

719
00:44:32.900 --> 00:44:35.400
That the better what's the

720
00:44:35.400 --> 00:44:38.300
 like you can go very small, but what's what's the smallest?

721
00:44:40.200 --> 00:44:43.400
I'm not a real estate expert on a property developer, but how let's

722
00:44:43.400 --> 00:44:46.600
 and you let's use a square square feet.

723
00:44:47.700 --> 00:44:48.700
How small?

724
00:44:49.300 --> 00:44:49.800
Can you go?

725
00:44:51.900 --> 00:44:54.100
Without like like it won't

726
00:44:54.100 --> 00:44:58.000
 be like we don't want to go as small as being a wardrobe but how small can

727
00:44:57.100 --> 00:45:00.200
 we go before we say this is no longer enough. It's no

728
00:45:00.200 --> 00:45:01.500
 longer a property. This is just

729
00:45:02.600 --> 00:45:05.500
A cabinet, you know, what's the smallest

730
00:45:05.500 --> 00:45:06.700
 size for an apartment?

731
00:45:07.400 --> 00:45:08.100
in square feet

732
00:45:11.500 --> 00:45:12.000
anybody

733
00:45:14.900 --> 00:45:16.600
200 square feet

734
00:45:17.800 --> 00:45:20.500
Fine then Richard. Thank

735
00:45:20.500 --> 00:45:24.200
 you for the answer. We have now set the bias to be equal to 200 square

736
00:45:23.200 --> 00:45:24.500
 feet.

737
00:45:28.700 --> 00:45:32.000
Now you'll see this hopefully fall into place. We will

738
00:45:31.200 --> 00:45:32.600
 set the bias.

739
00:45:34.900 --> 00:45:36.100
To be good to 200.

740
00:45:38.100 --> 00:45:39.400
Basically, we are doing this.

741
00:45:46.200 --> 00:45:49.100
If we set the device 200, we're going to shift this.

742
00:45:51.400 --> 00:45:52.700
200 points to the right

743
00:45:53.900 --> 00:45:55.300
imagine if I bring it over here.

744
00:45:58.500 --> 00:45:58.800
Okay.

745
00:46:01.200 --> 00:46:02.200
We have shifted.

746
00:46:03.400 --> 00:46:05.600
the bias 200 points

747
00:46:06.200 --> 00:46:07.600
Okay, what are we doing here?

748
00:46:08.500 --> 00:46:10.800
Suppose we take we have some property.

749
00:46:12.400 --> 00:46:15.200
And I will use this point to represent that problem. Let's make

750
00:46:15.200 --> 00:46:17.800
 this a very small observation.

751
00:46:21.400 --> 00:46:21.600
Okay.

752
00:46:23.900 --> 00:46:25.500
this represents

753
00:46:27.700 --> 00:46:28.800
the size times weight

754
00:46:29.400 --> 00:46:30.100
of this property

755
00:46:31.400 --> 00:46:32.500
if the property is

756
00:46:35.600 --> 00:46:36.500
let's say one.

757
00:46:38.500 --> 00:46:41.200
100 I can't even put it exactly on

758
00:46:41.200 --> 00:46:44.600
 the line. So let's just put it here. Let's say

759
00:46:44.600 --> 00:46:46.600
 this property is 150 square feet.

760
00:46:48.400 --> 00:46:51.200
Because it is below our threshold.

761
00:46:53.100 --> 00:46:54.700
We don't even considered a property.

762
00:46:56.200 --> 00:46:58.000
So it's activation will be zero.

763
00:47:01.000 --> 00:47:01.100
so

764
00:47:02.100 --> 00:47:05.600
f of x or if you or equivalently if you want 5 of

765
00:47:05.600 --> 00:47:05.700
 x

766
00:47:08.900 --> 00:47:10.400
I want to be consistent with the slides.

767
00:47:11.500 --> 00:47:14.300
The activation for this particular X will be zero.

768
00:47:15.200 --> 00:47:16.000
Because it's not a property.

769
00:47:18.600 --> 00:47:19.900
What if we make it one?

770
00:47:25.600 --> 00:47:28.200
You know 190 still not a property

771
00:47:28.200 --> 00:47:30.200
 not qualified to be a property.

772
00:47:31.400 --> 00:47:34.200
But once it gets 200 or above. Oh, well, then

773
00:47:34.200 --> 00:47:34.800
 we will let it through.

774
00:47:36.500 --> 00:47:38.400
That's what I mean by strength of the signal.

775
00:47:39.900 --> 00:47:42.300
If we had a stack of hidden layers, it

776
00:47:42.300 --> 00:47:45.600
 would not even get to the next layer because what's the point of figuring

777
00:47:45.600 --> 00:47:48.400
 because see the way we're going to determine

778
00:47:48.400 --> 00:47:51.400
 if it's a office or a real estate is to see how big it

779
00:47:51.400 --> 00:47:52.700
 is how many partitions?

780
00:47:53.800 --> 00:47:57.100
How many windows the height

781
00:47:56.100 --> 00:47:58.000
 of the ceiling Maybe?

782
00:47:58.900 --> 00:48:01.300
How many fire extinguishers?

783
00:48:02.900 --> 00:48:05.900
How many glass because of

784
00:48:05.900 --> 00:48:07.700
 Open Space is really they have these glass?

785
00:48:11.400 --> 00:48:14.500
Like you don't have well like you have you have

786
00:48:14.500 --> 00:48:17.100
 a b you have view like class Glass Walls. I

787
00:48:17.100 --> 00:48:17.700
 don't know how to call it.

788
00:48:18.600 --> 00:48:21.500
So what's the point of counting the number of rooms and

789
00:48:21.500 --> 00:48:24.400
 the height of the ceiling and the number of fire extinguishers if

790
00:48:24.400 --> 00:48:27.400
 it's even if it's too small to even be considered an

791
00:48:27.400 --> 00:48:30.000
 apartment, that's what this value is going to do.

792
00:48:32.700 --> 00:48:35.400
Products thank you rob by the way, all of this all of

793
00:48:35.400 --> 00:48:36.700
 this channel is going to be saved. Okay.

794
00:48:38.500 --> 00:48:40.700
All of the chats from all sashes have been saved. So

795
00:48:45.200 --> 00:48:47.200
some make you try to make accessible for all of you. Later.

796
00:48:49.200 --> 00:48:50.100
That's what they're really function does.

797
00:48:51.200 --> 00:48:52.800
So if I return if I return here to the

798
00:48:55.600 --> 00:48:56.000
the

799
00:48:57.800 --> 00:48:59.700
slides if I return to one five two

800
00:49:03.600 --> 00:49:06.400
That's what we try to indicate. So if you see here, let's say

801
00:49:06.400 --> 00:49:09.900
 the okay this one this activation function

802
00:49:09.900 --> 00:49:12.000
 for this example. The bias is set to zero.

803
00:49:12.900 --> 00:49:16.100
So when we take this sum of this product 32

804
00:49:15.100 --> 00:49:18.500
 because 32 is about the bias. It

805
00:49:18.500 --> 00:49:22.100
 will let we will let it through same thing for 55 with

806
00:49:21.100 --> 00:49:24.800
 letter through but for negative 4 we crush it

807
00:49:24.800 --> 00:49:25.300
 down to zero.

808
00:49:27.200 --> 00:49:29.600
then once we get to this point

809
00:49:31.200 --> 00:49:35.000
We will also do we will multiply this activation

810
00:49:34.300 --> 00:49:36.100
 by its weight.

811
00:49:37.400 --> 00:49:40.300
This activation by its weight this activation by

812
00:49:40.300 --> 00:49:40.700
 its weight.

813
00:49:42.100 --> 00:49:42.700
So this one?

814
00:49:44.300 --> 00:49:46.400
Since the signal was very weak.

815
00:49:49.300 --> 00:49:52.300
It will remain zero even when it reaches

816
00:49:52.300 --> 00:49:54.000
 this downstairs. So for example

817
00:49:55.600 --> 00:49:58.200
Let's say this particular property.

818
00:49:59.300 --> 00:49:59.700
has

819
00:50:07.300 --> 00:50:10.000
its size is below the threshold.

820
00:50:11.600 --> 00:50:14.300
That's already a red flag. It's not big enough for us to consider the

821
00:50:14.300 --> 00:50:14.500
 property.

822
00:50:15.500 --> 00:50:17.600
but it has a door and it has

823
00:50:18.800 --> 00:50:19.300
a window

824
00:50:21.900 --> 00:50:24.700
But because this signal from this perceptron

825
00:50:24.700 --> 00:50:25.400
 is very weak.

826
00:50:27.100 --> 00:50:30.600
And we have another activation function here. We will classify as

827
00:50:30.600 --> 00:50:31.300
 not property.

828
00:50:33.300 --> 00:50:35.600
but I talked about binary classification, so

829
00:50:37.700 --> 00:50:40.700
I guess it could be a very

830
00:50:40.700 --> 00:50:41.600
 small office.

831
00:50:42.700 --> 00:50:45.200
Like, you know when you go to currency exchange.

832
00:50:46.800 --> 00:50:48.400
They have these very small booths.

833
00:50:48.800 --> 00:50:51.000
So I guess you can call that an office.

834
00:50:53.300 --> 00:50:56.200
The example I'm giving is not perfect, but

835
00:50:56.200 --> 00:50:59.100
 hopefully understand the purpose of the rally function. That's really what

836
00:50:59.100 --> 00:51:00.200
 I'm trying to emphasize you.

837
00:51:01.700 --> 00:51:04.600
Now, let me quickly show you the Matrix representation of

838
00:51:04.600 --> 00:51:05.000
 the same.

839
00:51:06.400 --> 00:51:07.400
I be yeah.

840
00:51:10.200 --> 00:51:13.300
Here we have our input vector vector and we multiplied by The

841
00:51:13.300 --> 00:51:16.400
 Matrix of Weights. These cells are

842
00:51:16.400 --> 00:51:19.100
 in Black because these the stand for

843
00:51:19.100 --> 00:51:20.500
 their weight. Excuse me the bias.

844
00:51:21.600 --> 00:51:24.000
so the threshold for the activation function

845
00:51:25.100 --> 00:51:27.400
of course the order of

846
00:51:28.400 --> 00:51:31.300
Matrix multiplication is not Computing commutative.

847
00:51:32.200 --> 00:51:35.000
So the number of rows must match the

848
00:51:35.500 --> 00:51:38.200
 number of columns in the excuse me. The number of columns must match the

849
00:51:38.200 --> 00:51:38.800
 number of rows.

850
00:51:40.300 --> 00:51:43.500
for the vector or Matrix on the right, so we multiply this

851
00:51:43.500 --> 00:51:44.000
 and we will get

852
00:51:45.200 --> 00:51:46.000
a new vector

853
00:51:46.800 --> 00:51:49.500
and what is this Vector? What are the components of the vector? Well,

854
00:51:49.500 --> 00:51:50.600
 it's this part. It's the sum.

855
00:51:51.900 --> 00:51:54.600
It's still wait more. It's the random weight multiplied

856
00:51:54.600 --> 00:51:55.500
 by the bias.

857
00:51:56.600 --> 00:51:59.800
It's the input first input multiplied

858
00:51:59.800 --> 00:52:02.300
 by its weight the second to put multiply its weight and we

859
00:52:02.300 --> 00:52:03.900
 do this for all three perceptrons so really

860
00:52:10.700 --> 00:52:13.000
And that this is what it is. So if

861
00:52:13.100 --> 00:52:15.100
 we would if we would take the sum of these products.

862
00:52:16.500 --> 00:52:18.900
this would be the components of the

863
00:52:20.500 --> 00:52:21.300
hidden layer

864
00:52:22.300 --> 00:52:23.200
And then what we do?

865
00:52:24.500 --> 00:52:26.000
Is we pass this vector?

866
00:52:26.900 --> 00:52:29.300
And I'm using Z here because that's the convention

867
00:52:29.300 --> 00:52:31.900
 for Activation to this activation function.

868
00:52:34.500 --> 00:52:37.000
And what the activation function will do is it will go

869
00:52:37.100 --> 00:52:37.900
 through these components.

870
00:52:38.600 --> 00:52:39.400
Check the signal.

871
00:52:40.200 --> 00:52:43.600
This one is negative 4 so it will be brought down to zero and then

872
00:52:43.600 --> 00:52:46.200
 what we're going to do is we're going to again append the bias

873
00:52:46.200 --> 00:52:48.600
 term the bias numerical value.

874
00:52:49.800 --> 00:52:50.700
added to the

875
00:52:51.300 --> 00:52:52.000
to the vector

876
00:52:52.500 --> 00:52:55.800
this activation Factor will then be passed on to the

877
00:52:57.300 --> 00:52:58.000
the output layer

878
00:52:58.500 --> 00:52:59.000
if you notice

879
00:53:00.100 --> 00:53:01.800
this Vector corresponds to

880
00:53:07.100 --> 00:53:11.000
this so 32. This is

881
00:53:10.200 --> 00:53:13.100
 one that this whole part is the one

882
00:53:13.100 --> 00:53:13.300
 vector.

883
00:53:14.500 --> 00:53:15.600
We have three components.

884
00:53:16.600 --> 00:53:19.300
And these three components from the vector and this Vector will

885
00:53:19.300 --> 00:53:22.400
 be passed to the activation function of the last perceptron.

886
00:53:22.900 --> 00:53:23.800
And that's what we have here.

887
00:53:27.800 --> 00:53:28.400
So we do this.

888
00:53:32.500 --> 00:53:32.700
dot product

889
00:53:35.300 --> 00:53:36.900
and then we will have a single component.

890
00:53:38.200 --> 00:53:40.400
Once again, we'll output this to the activation function.

891
00:53:42.200 --> 00:53:45.100
the return value of this actually the evaluated value of

892
00:53:45.100 --> 00:53:46.300
 this activation function is

893
00:53:47.100 --> 00:53:47.600
zero

894
00:53:51.400 --> 00:53:54.400
So imagine we're trying to do we would try to do binary classification.

895
00:53:54.400 --> 00:53:56.200
 Is this a property or not?

896
00:53:56.900 --> 00:53:59.300
So let's say because the property was

897
00:53:59.300 --> 00:53:59.700
 too small.

898
00:54:00.700 --> 00:54:03.900
The signal was too weak. And so we classified it

899
00:54:03.900 --> 00:54:04.600
 as a non-property.

900
00:54:06.700 --> 00:54:09.300
And this is true if you want to for example classify something

901
00:54:09.300 --> 00:54:12.000
 as fraud or not fraud spam or not spam.

902
00:54:13.300 --> 00:54:14.200
And that's really the idea.

903
00:54:18.900 --> 00:54:19.100
Okay.

904
00:54:23.600 --> 00:54:23.800
now

905
00:54:28.300 --> 00:54:31.300
let me show you this other slide. This is from another very well at book.

906
00:54:32.300 --> 00:54:35.900
Which I have here on my on my desk. The book

907
00:54:35.900 --> 00:54:36.300
 is called.

908
00:54:38.300 --> 00:54:41.400
fundamentals of machine learning and predictive data analytics

909
00:54:42.300 --> 00:54:42.800
this is from

910
00:54:44.400 --> 00:54:47.700
MIT press again. I'll include the book.

911
00:54:48.600 --> 00:54:49.600
in the references file

912
00:54:50.500 --> 00:54:53.500
but this is hopefully going to help you understand the concept

913
00:54:53.500 --> 00:54:55.300
 intuitively suppose we have

914
00:54:56.500 --> 00:54:58.300
a picture with a triangle in it

915
00:54:59.700 --> 00:55:00.100
this one

916
00:55:03.400 --> 00:55:05.100
and we want to know what it is that we're looking at.

917
00:55:06.500 --> 00:55:06.900
each

918
00:55:08.200 --> 00:55:11.200
perceptron and now we only have one hidden layer. So this is a

919
00:55:11.200 --> 00:55:12.100
 very simplified example.

920
00:55:13.300 --> 00:55:14.800
each perceptron in this hidden layer

921
00:55:16.200 --> 00:55:18.300
is going to look at one feature of this picture.

922
00:55:20.200 --> 00:55:22.000
So for example, if you see here we have.

923
00:55:26.300 --> 00:55:29.500
One segment of the triangle here. We have another segment

924
00:55:29.500 --> 00:55:32.500
 of the triangle here. We have another yet another segment of

925
00:55:32.500 --> 00:55:32.800
 the triangle.

926
00:55:35.100 --> 00:55:36.000
the second here the layer

927
00:55:38.400 --> 00:55:41.500
Is going to combine the information from the previous end layer.

928
00:55:42.700 --> 00:55:45.200
And I will say oh we are forming an angle.

929
00:55:46.900 --> 00:55:48.500
So that's one angle form. That's one.

930
00:55:54.200 --> 00:55:57.100
And I want to use the word feature, but that is one.

931
00:55:59.300 --> 00:56:02.300
Part of the picture that this machine learning has discovered.

932
00:56:03.200 --> 00:56:05.500
And the same thing for the second perception of the Hidden layer.

933
00:56:06.200 --> 00:56:08.100
And you can see once we pass these two.

934
00:56:09.700 --> 00:56:12.500
And by the way, they're vectors. So it is a vectors of

935
00:56:12.500 --> 00:56:15.800
 the features. We pass this to the Apple layer. Then

936
00:56:15.800 --> 00:56:18.300
 the perceptron will put these two pieces of puzzles

937
00:56:18.300 --> 00:56:21.600
 together and it will say okay we're looking at a triangle. So this

938
00:56:21.600 --> 00:56:23.500
 is supposed to be an intuitive approach.

939
00:56:27.800 --> 00:56:30.500
Now before I take the sigmoil, let me actually take it the

940
00:56:30.500 --> 00:56:33.300
 playground playground that tensorflow so you can see why you would

941
00:56:33.300 --> 00:56:34.400
 use other activation functions.

942
00:56:42.100 --> 00:56:42.800
playground

943
00:56:43.700 --> 00:56:44.600
tensorflow

944
00:56:56.700 --> 00:56:59.300
let me tell you what we were looking at

945
00:56:59.300 --> 00:56:59.400
 here.

946
00:57:04.700 --> 00:57:05.900
First this is the neural network.

947
00:57:08.500 --> 00:57:09.400
this entire section

948
00:57:11.900 --> 00:57:14.200
you have your features your input Dimensions if

949
00:57:14.200 --> 00:57:15.600
 we use tensorflow terminology.

950
00:57:18.500 --> 00:57:19.700
your hidden layers

951
00:57:21.700 --> 00:57:21.900
now

952
00:57:22.900 --> 00:57:26.200
there is no hard rule or formula.

953
00:57:26.800 --> 00:57:28.500
for choosing the number of hidden layers

954
00:57:29.600 --> 00:57:31.000
but for most problems

955
00:57:32.200 --> 00:57:34.000
two hidden layers is sufficient.

956
00:57:35.100 --> 00:57:35.700
for most problems

957
00:57:37.400 --> 00:57:40.900
or at least numerical problems data, that

958
00:57:40.900 --> 00:57:41.600
 is numerical and

959
00:57:42.400 --> 00:57:43.600
Like if you're doing regression.

960
00:57:44.800 --> 00:57:47.100
For that so from majority problems. This is this is

961
00:57:47.100 --> 00:57:50.600
 this architectures sufficient for things like pictures and

962
00:57:53.200 --> 00:57:56.400
Sound sound bites or text. No you do.

963
00:57:56.400 --> 00:57:59.900
 You definitely need more elaborate neural network, and

964
00:57:59.900 --> 00:58:00.800
 then you have the output layer.

965
00:58:03.500 --> 00:58:06.000
So this entire thing is your neural network?

966
00:58:08.700 --> 00:58:09.500
Then up here we have.

967
00:58:10.600 --> 00:58:12.300
the hyper parameters

968
00:58:13.700 --> 00:58:16.700
Remember, we should hyper parameters are synonymous with

969
00:58:16.700 --> 00:58:18.000
 the configuration of the null Network.

970
00:58:19.800 --> 00:58:21.500
This will allow you to run this for.

971
00:58:22.400 --> 00:58:25.700
different number of epochs so you can keep feeding your

972
00:58:25.700 --> 00:58:26.900
 data to the network to see

973
00:58:27.900 --> 00:58:28.800
In how it's doing?

974
00:58:30.800 --> 00:58:33.200
And then you have these hyper parameters, which allow you

975
00:58:33.200 --> 00:58:33.600
 to control.

976
00:58:34.900 --> 00:58:37.500
how the model learned and

977
00:58:37.500 --> 00:58:40.300
 how to control its the overfitting

978
00:58:41.300 --> 00:58:43.300
so we have we can choose the learning rate.

979
00:58:44.900 --> 00:58:46.000
the regularization function

980
00:58:47.500 --> 00:58:49.600
the regularization rate

981
00:58:51.800 --> 00:58:52.300
and then here

982
00:58:55.800 --> 00:58:56.500
illustrate

983
00:58:57.300 --> 00:58:59.100
how effective and neural network is

984
00:59:00.900 --> 00:59:03.600
They have chosen some very unusual.

985
00:59:05.400 --> 00:59:08.500
Shapes of data. I mean, this is look at this look at what's

986
00:59:08.500 --> 00:59:09.000
 going on here.

987
00:59:10.100 --> 00:59:13.700
This is supposed to be like a scatter plot. We have two types

988
00:59:13.700 --> 00:59:16.800
 of data two data classes. The blue

989
00:59:16.800 --> 00:59:18.300
 one is centered.

990
00:59:19.400 --> 00:59:22.200
In the middle and the second class is around again.

991
00:59:22.200 --> 00:59:25.800
 You're unlikely to see something like this in in the real

992
00:59:25.800 --> 00:59:27.900
 world. This is supposed to be like a extreme example.

993
00:59:29.100 --> 00:59:32.400
The point of this demonstration to show you how a new network is still

994
00:59:32.400 --> 00:59:33.200
 able to classify.

995
00:59:35.900 --> 00:59:36.100
the

996
00:59:37.500 --> 00:59:39.100
two classes in the data set

997
00:59:39.700 --> 00:59:40.400
Here's another one.

998
00:59:42.300 --> 00:59:43.500
Here's another one. This is easier.

999
00:59:44.400 --> 00:59:46.300
And there's another one. Yeah, I mean, this is look at this. This is

1000
00:59:49.500 --> 00:59:51.000
the point that what now?

1001
00:59:52.600 --> 00:59:55.600
I have to play around with this, but we can use the

1002
00:59:55.600 --> 00:59:56.400
 same Nola work.

1003
00:59:57.100 --> 00:59:59.300
to completely separate

1004
01:00:00.300 --> 01:00:00.800
these

1005
01:00:01.800 --> 01:00:02.500
observations

1006
01:00:05.900 --> 01:00:06.500
now, how do we

1007
01:00:09.300 --> 01:00:09.800
separate

1008
01:00:14.500 --> 01:00:16.200
we need to talk about regression for a moment.

1009
01:00:18.400 --> 01:00:21.200
If I come back to this slide if I go all the way to

1010
01:00:21.200 --> 01:00:22.000
 up here.

1011
01:00:31.500 --> 01:00:34.400
This is supposed to be a picture that illustrates the

1012
01:00:34.400 --> 01:00:37.300
 application of the general regression model

1013
01:00:37.300 --> 01:00:38.800
 regression is used for.

1014
01:00:44.600 --> 01:00:47.000
modeling the direction of the data

1015
01:00:48.200 --> 01:00:49.500
and that's how you forecast. That's

1016
01:00:52.300 --> 01:00:55.700
an application of forecasting or others known as a regress.

1017
01:00:55.700 --> 01:00:56.800
 It's called regression because

1018
01:00:58.700 --> 01:00:59.000
in a

1019
01:01:00.200 --> 01:01:04.000
forecasting model the observations should

1020
01:01:03.800 --> 01:01:06.900
 regress towards this line.

1021
01:01:12.400 --> 01:01:15.500
A regression model this is obviously not linear because

1022
01:01:15.500 --> 01:01:18.500
 well it's curved can also be used to separate the

1023
01:01:18.500 --> 01:01:18.800
 data.

1024
01:01:19.900 --> 01:01:22.200
into two groups if in the case of

1025
01:01:22.200 --> 01:01:23.300
 a binary classification

1026
01:01:24.800 --> 01:01:27.100
So this would be a polynomial model.

1027
01:01:28.400 --> 01:01:31.600
And the idea is to draw the boundaries or

1028
01:01:31.600 --> 01:01:32.700
 to draw boundary.

1029
01:01:34.200 --> 01:01:36.600
Or rather model the boundary of the data set.

1030
01:01:39.100 --> 01:01:42.000
Of course, it won't be perfect. So yeah, I hope you can see on my screen. We have

1031
01:01:42.200 --> 01:01:44.500
 some light green observations some like green data points.

1032
01:01:45.200 --> 01:01:46.500
in the second cluster

1033
01:01:47.200 --> 01:01:50.400
Conversely we have some of these dark green or turquoise.

1034
01:01:50.400 --> 01:01:52.300
 If you like to call it in the first cluster.

1035
01:01:52.800 --> 01:01:55.600
But the majority have are well partitioned.

1036
01:01:57.400 --> 01:01:57.500
now

1037
01:02:02.500 --> 01:02:05.400
here's another reason why you can never want to use a linear model for

1038
01:02:05.400 --> 01:02:08.300
 classification because it's linear. It's

1039
01:02:08.300 --> 01:02:10.300
 a single line. You cannot use a single line.

1040
01:02:11.300 --> 01:02:14.200
To partition a sea of data points like the

1041
01:02:14.200 --> 01:02:17.200
 one you see here, like imagine if this were a straight line you could not

1042
01:02:17.200 --> 01:02:18.100
 clearly.

1043
01:02:20.400 --> 01:02:21.100
delineate

1044
01:02:23.100 --> 01:02:23.800
this section

1045
01:02:28.200 --> 01:02:31.400
You can't also do this with our value

1046
01:02:31.400 --> 01:02:31.900
 model either.

1047
01:02:34.600 --> 01:02:35.500
Unless of course.

1048
01:02:38.400 --> 01:02:41.000
You use multiple ready models. Let me let me

1049
01:02:41.000 --> 01:02:41.600
 demonstrate.

1050
01:02:44.100 --> 01:02:44.800
If we come back here.

1051
01:02:48.400 --> 01:02:50.200
I'm going to return to the

1052
01:02:52.400 --> 01:02:53.400
first example

1053
01:02:56.200 --> 01:02:58.100
I will not change the hyper parameters.

1054
01:03:00.500 --> 01:03:03.200
And I will not change the neural network. I will not

1055
01:03:03.200 --> 01:03:03.900
 change the configuration.

1056
01:03:05.200 --> 01:03:05.900
I'm gonna have to play.

1057
01:03:06.800 --> 01:03:07.900
and what I want you to look at

1058
01:03:09.200 --> 01:03:09.200
is

1059
01:03:10.800 --> 01:03:11.900
the animation in here

1060
01:03:14.800 --> 01:03:17.500
now what you're going to see is this epox are quickly going

1061
01:03:17.500 --> 01:03:18.400
 to increment.

1062
01:03:20.200 --> 01:03:21.100
like some sort of

1063
01:03:21.900 --> 01:03:22.800
time-lap

1064
01:03:26.300 --> 01:03:29.100
and what each Epoch another thing you will see here is that

1065
01:03:29.100 --> 01:03:30.300
 is the loss?

1066
01:03:31.600 --> 01:03:34.500
And we probably care for the loss. We want this

1067
01:03:34.500 --> 01:03:37.300
 to converge to zero and it will be animated here. So

1068
01:03:37.300 --> 01:03:39.800
 if I hit let's let's watch what happens. Okay, so you want to watch

1069
01:03:40.500 --> 01:03:43.500
you want to watch this and you want to watch this

1070
01:03:43.500 --> 01:03:44.200
 space over here?

1071
01:03:47.200 --> 01:03:49.400
And we want to see if you're able to use for.

1072
01:03:50.900 --> 01:03:52.100
Really you models.

1073
01:03:52.900 --> 01:03:53.800
not linear

1074
01:03:55.300 --> 01:03:58.400
not polynomial. We want to see if we can use that really

1075
01:03:58.400 --> 01:04:00.400
 that L. Looking model four of them.

1076
01:04:01.100 --> 01:04:01.900
to delineate

1077
01:04:03.300 --> 01:04:06.400
the boundaries of these two classes ready

1078
01:04:06.400 --> 01:04:07.300
 one, two three

1079
01:04:09.300 --> 01:04:12.300
All right. Look at the loss function is already converging to zero you

1080
01:04:12.300 --> 01:04:13.300
 see how it has climbed down.

1081
01:04:14.600 --> 01:04:17.000
It is getting closer and closer to zero. We are not

1082
01:04:17.300 --> 01:04:20.600
 at how many epochs 100 200 I'm

1083
01:04:20.600 --> 01:04:23.700
 gonna hit pause 236 epochs watch

1084
01:04:23.700 --> 01:04:24.200
 what has happened.

1085
01:04:26.600 --> 01:04:28.200
You see these boundaries.

1086
01:04:28.900 --> 01:04:30.600
These are formed by those four.

1087
01:04:31.600 --> 01:04:32.800
value functions

1088
01:04:37.500 --> 01:04:40.100
1 2 3 4 I can see here. We have formed the first.

1089
01:04:43.300 --> 01:04:45.000
Angle and this is the second angle.

1090
01:04:45.700 --> 01:04:46.500
Put these together.

1091
01:04:53.100 --> 01:04:53.700
and you're able to

1092
01:04:54.700 --> 01:04:57.300
you know separate these two

1093
01:04:57.300 --> 01:04:57.900
 clusters of data.

1094
01:05:02.900 --> 01:05:05.300
Is it clear like has this example have to

1095
01:05:05.300 --> 01:05:08.400
 understand what we're doing? What if I had said? Okay, by

1096
01:05:08.400 --> 01:05:10.700
 the way, this is 10 age. I should have changed us to value.

1097
01:05:11.700 --> 01:05:13.300
Let me say this to rally and try again.

1098
01:05:15.300 --> 01:05:18.200
One two three, I'm gonna hit play so you can see it still

1099
01:05:18.200 --> 01:05:18.600
 does a good job.

1100
01:05:19.500 --> 01:05:21.000
Obviously the corners are sharper.

1101
01:05:21.800 --> 01:05:22.700
But that's what really does.

1102
01:05:23.900 --> 01:05:25.700
What if I would set this to linear?

1103
01:05:27.200 --> 01:05:27.700
and hit play

1104
01:05:30.800 --> 01:05:31.300
it's not able to.

1105
01:05:32.900 --> 01:05:35.400
Because there's nothing linear about the pattern of

1106
01:05:35.400 --> 01:05:36.000
 this data.

1107
01:05:38.500 --> 01:05:40.500
Let's try this other one.

1108
01:05:41.400 --> 01:05:42.200
and I'm gonna use

1109
01:05:43.600 --> 01:05:45.900
Value again, and I'm gonna hit play.

1110
01:05:52.300 --> 01:05:55.200
And sometimes you actually have to restart this for to work properly. So I'm going

1111
01:05:55.200 --> 01:05:55.200
 to

1112
01:05:56.100 --> 01:05:57.400
hard refresh this

1113
01:05:59.100 --> 01:05:59.700
Try again.

1114
01:06:00.500 --> 01:06:01.000
There we go.

1115
01:06:02.100 --> 01:06:04.300
You can clearly see the value right now.

1116
01:06:12.900 --> 01:06:15.700
And for something like this, well, you need

1117
01:06:15.700 --> 01:06:18.600
 something that is more curved like

1118
01:06:18.600 --> 01:06:20.800
 ten age I hit play.

1119
01:06:21.700 --> 01:06:23.600
This might take a while longer.

1120
01:06:28.400 --> 01:06:29.300
so the loss is

1121
01:06:31.200 --> 01:06:32.100
a climbing

1122
01:06:34.100 --> 01:06:34.300
it's

1123
01:06:36.000 --> 01:06:36.800
consistent

1124
01:06:38.200 --> 01:06:40.300
I think I'm gonna have to a hard refresh.

1125
01:06:41.200 --> 01:06:42.200
To run this again.

1126
01:06:46.200 --> 01:06:48.400
So 10 age can it play once more?

1127
01:06:50.600 --> 01:06:52.700
I remember this example always takes a bit longer.

1128
01:06:55.100 --> 01:06:58.300
But it is ultimately able to you know.

1129
01:07:00.700 --> 01:07:01.400
separate

1130
01:07:03.300 --> 01:07:04.800
these two classes

1131
01:07:07.200 --> 01:07:10.400
Okay, look at the loss is is dropping. It's

1132
01:07:10.400 --> 01:07:12.800
 getting closer and closer to zero very slowly, but it is

1133
01:07:13.900 --> 01:07:14.800
still not good enough.

1134
01:07:15.500 --> 01:07:17.600
That this let's let it run a little longer.

1135
01:07:19.200 --> 01:07:22.000
It's 300. Is it gonna go down to 200?

1136
01:07:27.600 --> 01:07:29.300
Look at that. Look at that. Look at that.

1137
01:07:31.600 --> 01:07:32.900
We're almost 1,000 epochs.

1138
01:07:34.000 --> 01:07:34.300
and

1139
01:07:35.500 --> 01:07:38.400
the test loss is kind of stuck at 300.

1140
01:07:40.700 --> 01:07:43.300
So what we might have to do is play around

1141
01:07:43.300 --> 01:07:44.800
 with these hyper parameters like

1142
01:07:45.800 --> 01:07:47.300
change the regularization rate.

1143
01:07:49.600 --> 01:07:51.300
even adjust a bad size

1144
01:07:53.100 --> 01:07:56.200
But it's kind of oh look at that. Hold on. Hold on

1145
01:07:56.200 --> 01:07:56.900
 what something's happening?

1146
01:07:59.200 --> 01:08:01.400
It is it's dropping again. It's dropped some more.

1147
01:08:02.900 --> 01:08:04.600
That's 1500 epochs.

1148
01:08:06.200 --> 01:08:09.300
I'm gonna I don't know if I should keep running this.

1149
01:08:12.500 --> 01:08:15.500
But it is it's done somewhat. Well, I mean the Tesla is

1150
01:08:15.500 --> 01:08:16.000
 still not.

1151
01:08:17.200 --> 01:08:19.700
Good enough if we wanted to get closer to zero.

1152
01:08:20.700 --> 01:08:23.200
But clearly we want to use this activation function.

1153
01:08:23.900 --> 01:08:25.200
I think we had pause here.

1154
01:08:25.800 --> 01:08:28.200
But yes had I

1155
01:08:28.200 --> 01:08:31.700
 played around with this with the hyper parameters. I could

1156
01:08:31.700 --> 01:08:32.800
 have gotten it to perfectly.

1157
01:08:34.300 --> 01:08:37.300
You know separate the two classes.

1158
01:08:38.400 --> 01:08:38.600
Okay.

1159
01:08:45.500 --> 01:08:48.300
I'll put the link in the chat, but but it will

1160
01:08:48.300 --> 01:08:51.100
 be also put this in the slide so you don't know you can you can check this

1161
01:08:51.100 --> 01:08:52.900
 out later on play around with it.

1162
01:08:55.500 --> 01:08:57.900
now, let me return to the

1163
01:08:59.700 --> 01:09:01.000
notebook. I think it's done.

1164
01:09:03.600 --> 01:09:04.900
And if we look at our model.

1165
01:09:05.600 --> 01:09:09.000
After 10 epochs, it's accuracy has

1166
01:09:08.600 --> 01:09:12.500
 gone from 36% or almost 37%

1167
01:09:11.500 --> 01:09:15.000
 to 75% accuracy.

1168
01:09:16.300 --> 01:09:17.400
It's loss.

1169
01:09:18.500 --> 01:09:21.400
Its loss is well, not very good.

1170
01:09:21.400 --> 01:09:23.500
 It's not even zero.

1171
01:09:34.100 --> 01:09:37.100
But we could have in order to improve the loss function we could

1172
01:09:37.100 --> 01:09:39.600
 increase the number of epochs but even then.

1173
01:09:40.700 --> 01:09:43.200
Our data is a problematic because

1174
01:09:43.200 --> 01:09:46.400
 it's very small and our neural network

1175
01:09:46.400 --> 01:09:49.200
 is quite simple. Anyway, the accuracy is not

1176
01:09:49.200 --> 01:09:50.900
 that so 75% of the time.

1177
01:09:52.600 --> 01:09:53.500
we should

1178
01:09:55.200 --> 01:09:58.000
Not 75% about 75% accuracy. We should

1179
01:09:58.100 --> 01:10:01.100
 get the picture that the model is looking. Yeah. So,

1180
01:10:01.100 --> 01:10:03.100
 let's see how this happens then I'll return and talk about

1181
01:10:09.800 --> 01:10:11.600
Sig more than cross entropy.

1182
01:10:12.300 --> 01:10:14.800
But we only have 15 minutes left. Okay, so let me do this quickly.

1183
01:10:16.400 --> 01:10:18.000
I am going to upload a picture.

1184
01:10:19.300 --> 01:10:20.800
from the C4 10 database

1185
01:10:24.100 --> 01:10:24.300
Okay.

1186
01:10:26.100 --> 01:10:26.900
I will use.

1187
01:10:44.100 --> 01:10:45.700
cast to load the image

1188
01:10:47.600 --> 01:10:48.600
So I'm going to go.

1189
01:10:49.800 --> 01:10:50.600
Keras

1190
01:10:51.600 --> 01:10:54.200
I'll leave this. I'll leave this out for now just for

1191
01:10:54.200 --> 01:10:56.800
 the sake of time, but basically I was only going to plot.

1192
01:10:58.300 --> 01:11:01.600
The metrics actually, I think I have the code.

1193
01:11:01.600 --> 01:11:03.700
 I can just copy paste that so many copy paste it.

1194
01:11:04.500 --> 01:11:05.700
I'm gonna copy paste.

1195
01:11:17.600 --> 01:11:17.900
test

1196
01:11:19.500 --> 01:11:20.000
images

1197
01:11:21.600 --> 01:11:23.100
test out BLS

1198
01:11:27.800 --> 01:11:29.500
and the metrics we want this to be

1199
01:11:43.300 --> 01:11:46.500
Oh, I was supposed to assign to this

1200
01:11:46.500 --> 01:11:47.300
 a variable.

1201
01:11:48.800 --> 01:11:50.300
to this mistake

1202
01:11:51.300 --> 01:11:51.500
No problem.

1203
01:11:52.900 --> 01:11:56.400
I'll share this notebook with you you can run it but what

1204
01:11:55.400 --> 01:11:58.300
 we need to what we need this, you know the plot

1205
01:11:58.300 --> 01:12:01.100
 it but I won't plot this right now. I'm just going to comment it out and

1206
01:12:01.100 --> 01:12:03.900
 I'm just gonna show you a sample classification.

1207
01:12:06.300 --> 01:12:07.600
So I'm going to

1208
01:12:10.500 --> 01:12:11.300
load the image.

1209
01:12:16.500 --> 01:12:16.800
so

1210
01:12:18.900 --> 01:12:20.000
simple image

1211
01:12:23.900 --> 01:12:25.500
and I will use the

1212
01:12:27.500 --> 01:12:31.200
load image function from Keras

1213
01:12:31.900 --> 01:12:34.200
to take this cat picture

1214
01:12:36.200 --> 01:12:37.400
Which is from C4.

1215
01:12:38.400 --> 01:12:39.700
This is a PNG.

1216
01:12:40.700 --> 01:12:43.400
And I

1217
01:12:43.400 --> 01:12:46.700
 need to specify its Dimensions which of course is 32

1218
01:12:46.700 --> 01:12:47.500
 by 32.

1219
01:12:52.200 --> 01:12:52.500
and then

1220
01:12:54.300 --> 01:12:58.200
I need to convert it to an array and then normalize it.

1221
01:13:00.300 --> 01:13:02.400
So I'm going to go to sample image.

1222
01:13:10.500 --> 01:13:13.700
image dot img2 array

1223
01:13:14.800 --> 01:13:15.900
into a numpy array

1224
01:13:16.700 --> 01:13:17.000
test

1225
01:13:18.900 --> 01:13:19.900
or sample image

1226
01:13:27.200 --> 01:13:27.500
and then

1227
01:13:34.800 --> 01:13:35.700
sample image

1228
01:13:41.500 --> 01:13:42.000
equals

1229
01:13:43.500 --> 01:13:43.900
sample image

1230
01:13:48.100 --> 01:13:49.000
dot reshape

1231
01:13:50.500 --> 01:13:51.100
so we have

1232
01:13:52.300 --> 01:13:53.800
one sample

1233
01:13:54.700 --> 01:13:57.200
That is 32 by 32 or 3

1234
01:13:57.200 --> 01:13:57.900
 color channels.

1235
01:13:59.600 --> 01:14:01.600
And then we want to normalize. So this is normalize.

1236
01:14:02.200 --> 01:14:05.500
So what I mean by normal, so we have three color channels in

1237
01:14:05.500 --> 01:14:08.200
 each color channel the brightest pixel. So

1238
01:14:08.200 --> 01:14:11.500
 for example, if it's the red channel the most red pixel will

1239
01:14:11.500 --> 01:14:12.700
 have value 255.

1240
01:14:13.900 --> 01:14:16.900
And at least read pixel would

1241
01:14:16.900 --> 01:14:17.900
 be zero.

1242
01:14:18.800 --> 01:14:20.100
We want a normalize it because

1243
01:14:23.100 --> 01:14:24.600
the Machinery models don't understand.

1244
01:14:26.400 --> 01:14:27.400
What these values are?

1245
01:14:28.500 --> 01:14:31.400
And moreover if you remember the slides that

1246
01:14:31.400 --> 01:14:34.400
 we looked at when we I was talking about the Matrix operation of

1247
01:14:34.400 --> 01:14:37.800
 new light works. It will throw off our arithmetic.

1248
01:14:38.700 --> 01:14:41.700
So to normalize it well we do is we take this sample image.

1249
01:14:43.200 --> 01:14:45.000
And since it's a numpy object.

1250
01:14:45.900 --> 01:14:48.700
We can divide it by 255.

1251
01:14:50.300 --> 01:14:53.400
So if a number is 255 if it's like in the red Channel It's Perfectly

1252
01:14:53.400 --> 01:14:54.400
 red. It will be one.

1253
01:14:55.900 --> 01:14:58.200
Otherwise it will be 0 so I will now run this.

1254
01:15:01.800 --> 01:15:02.100
and

1255
01:15:05.900 --> 01:15:07.600
this is supposed to be Target size.

1256
01:15:12.200 --> 01:15:15.200
Okay, so if I show you what the sample images you will

1257
01:15:15.200 --> 01:15:15.500
 see it's

1258
01:15:18.200 --> 01:15:19.900
A multi-dimens. It's a tensor, basically.

1259
01:15:22.200 --> 01:15:26.900
And all about this is what we mean by normalized. So this is 77% of

1260
01:15:26.900 --> 01:15:27.500
 the pixel.

1261
01:15:28.700 --> 01:15:29.400
brightness

1262
01:15:30.800 --> 01:15:33.200
okay, then what I'm going to do is I'm going to use the

1263
01:15:35.400 --> 01:15:36.900
Predict method so I'm going to go.

1264
01:15:38.800 --> 01:15:39.800
predicted

1265
01:15:46.800 --> 01:15:47.200
class

1266
01:15:48.300 --> 01:15:48.800
index

1267
01:15:51.300 --> 01:15:52.400
will be equal to model.

1268
01:15:53.400 --> 01:15:55.200
dot predict

1269
01:15:58.400 --> 01:16:01.300
Sorry predict, the classes predict classes

1270
01:16:01.300 --> 01:16:05.200
 because it's a classification model and Sample.

1271
01:16:07.300 --> 01:16:08.200
sample image

1272
01:16:12.100 --> 01:16:14.600
and then I'm going to cast the index.

1273
01:16:15.600 --> 01:16:17.900
as an integer, so let me show you what happens if I

1274
01:16:19.800 --> 01:16:20.700
run this method

1275
01:16:26.900 --> 01:16:27.800
sequential object

1276
01:16:32.700 --> 01:16:36.100
and see I think the mistake somewhere if I

1277
01:16:35.100 --> 01:16:37.200
 scroll up here.

1278
01:16:59.300 --> 01:17:00.100
softmax

1279
01:17:08.900 --> 01:17:10.200
Yeah, that's correct.

1280
01:17:11.200 --> 01:17:14.500
All right. Let me see. I have to actually combine code from two separate

1281
01:17:14.500 --> 01:17:16.500
 notebooks predict.

1282
01:17:17.300 --> 01:17:19.800
Okay, let's try this. Let me try predict on its own.

1283
01:17:22.100 --> 01:17:22.400
it

1284
01:17:29.500 --> 01:17:31.000
cake you see we have a distribution.

1285
01:17:32.700 --> 01:17:33.400
Which means?

1286
01:17:40.200 --> 01:17:41.100
This is the shape.

1287
01:17:41.900 --> 01:17:43.000
That the model has predicted.

1288
01:17:44.300 --> 01:17:47.600
So remember we said that a machine learning every thing whether it's

1289
01:17:47.600 --> 01:17:50.200
 a property or a picture or sound bite.

1290
01:17:50.900 --> 01:17:53.300
Will be sort of associate with the shape. This is

1291
01:17:53.300 --> 01:17:54.000
 the shape of the data.

1292
01:17:54.900 --> 01:17:55.800
Now what I need to do?

1293
01:18:07.600 --> 01:18:09.100
Is use the right code?

1294
01:18:16.200 --> 01:18:17.500
Let me try this one more time.

1295
01:18:23.900 --> 01:18:24.900
paste

1296
01:18:41.600 --> 01:18:42.000
Okay.

1297
01:19:03.400 --> 01:19:04.300
Okay, and then

1298
01:19:06.400 --> 01:19:09.300
I have image classes. So what's this gonna

1299
01:19:09.300 --> 01:19:09.400
 give us?

1300
01:19:14.100 --> 01:19:15.300
And then if I scroll up here.

1301
01:19:22.200 --> 01:19:24.200
We want to pass this index.

1302
01:19:26.600 --> 01:19:28.200
into the into the list

1303
01:19:36.400 --> 01:19:38.700
so we go test labels.

1304
01:19:40.700 --> 01:19:42.100
pass in the result

1305
01:19:46.300 --> 01:19:46.400
and

1306
01:20:18.500 --> 01:20:20.300
Something is missing in my code.

1307
01:20:24.300 --> 01:20:25.500
result is

1308
01:20:42.400 --> 01:20:43.100
for yeah, I think this is it.

1309
01:20:45.400 --> 01:20:45.700
Okay.

1310
01:20:47.300 --> 01:20:50.100
Because I had to mix two two notebooks, but this is

1311
01:20:50.100 --> 01:20:50.400
 the idea.

1312
01:20:54.500 --> 01:20:57.300
Now to understand what this is we need to talk about.

1313
01:20:59.100 --> 01:20:59.500
the

1314
01:21:02.900 --> 01:21:05.600
the sigmoid or I keep seeing sigmoid here.

1315
01:21:05.600 --> 01:21:07.600
 We use a soft Max function.

1316
01:21:08.300 --> 01:21:09.600
We talk about softmax.

1317
01:21:13.800 --> 01:21:16.900
And then we will talk about the cross entropy so

1318
01:21:16.900 --> 01:21:20.600
 might go a few minutes a couple of minutes over our

1319
01:21:20.600 --> 01:21:23.300
 session. But I mean this is this is really

1320
01:21:23.300 --> 01:21:25.300
 important. This is good stuff relevant stuff.

1321
01:21:26.300 --> 01:21:28.900
Okay, so I actually have my notes here from before.

1322
01:21:31.500 --> 01:21:34.300
Okay, let me save this.

1323
01:21:37.900 --> 01:21:40.100
And call this activation functions.

1324
01:22:22.100 --> 01:22:24.500
Let me show you a picture of softmax function.

1325
01:22:29.300 --> 01:22:32.300
And they're very similar in that they are.

1326
01:22:33.300 --> 01:22:33.700
used for

1327
01:22:36.600 --> 01:22:37.900
assigning to every

1328
01:22:39.500 --> 01:22:40.400
feature distribution

1329
01:22:41.400 --> 01:22:41.700
a

1330
01:22:44.100 --> 01:22:48.100
class in the case of Sigma. It will be a binary classification.

1331
01:22:49.100 --> 01:22:51.000
And the case of softmax it will be.

1332
01:22:55.200 --> 01:22:56.600
multi-class classification

1333
01:22:58.100 --> 01:23:00.300
but we have this s looking curve.

1334
01:23:01.600 --> 01:23:03.400
and if a feature is

1335
01:23:04.400 --> 01:23:05.700
You know below the threshold.

1336
01:23:07.400 --> 01:23:10.400
it will be given 0 if it's binary and one

1337
01:23:10.400 --> 01:23:10.900
 otherwise

1338
01:23:12.400 --> 01:23:12.600
Okay.

1339
01:23:17.300 --> 01:23:21.400
No, we have to look at it mathematically pictorially it won't help very

1340
01:23:21.400 --> 01:23:21.600
 much.

1341
01:23:23.200 --> 01:23:25.400
Classification let's put softmax.

1342
01:23:30.400 --> 01:23:33.600
Yeah, let's let's talk about the math. The math will help.

1343
01:23:34.400 --> 01:23:34.600
Okay.

1344
01:23:36.400 --> 01:23:37.600
Let me first begin with.

1345
01:23:42.900 --> 01:23:44.900
a binary example

1346
01:23:48.200 --> 01:23:51.900
and then we can generalize it for multi-class classification. So

1347
01:23:51.900 --> 01:23:54.700
 before I talk about softmax, let me talk about sigmoid first.

1348
01:23:57.100 --> 01:23:58.100
Because it's easier to talk about.

1349
01:23:59.100 --> 01:24:01.600
It's easier to derive the sigmoid function because it's binary.

1350
01:24:04.500 --> 01:24:06.300
And the prerequisites are this.

1351
01:24:19.600 --> 01:24:20.400
and to do this

1352
01:24:30.700 --> 01:24:33.000
I'll just take you to this other application I've built.

1353
01:24:34.200 --> 01:24:37.500
I used to you sometimes users for teaching it's

1354
01:24:37.500 --> 01:24:39.700
 called a box map.

1355
01:24:40.100 --> 01:24:41.100
It bring us over here.

1356
01:24:41.900 --> 01:24:44.500
Okay. So these are the few in

1357
01:24:44.500 --> 01:24:48.100
 order for us to understand the sigmoid function otherwise known as expert. These

1358
01:24:47.100 --> 01:24:49.000
 are the prerequisites.

1359
01:24:49.700 --> 01:24:50.700
So I'm going to open each box.

1360
01:24:51.700 --> 01:24:53.600
and quickly take you through the

1361
01:24:54.500 --> 01:24:55.100
the

1362
01:24:56.400 --> 01:24:59.300
explanation the sigma function is giving by this formula.

1363
01:25:01.400 --> 01:25:04.600
So, let's see what this is. Why why this this?

1364
01:25:06.900 --> 01:25:08.700
mathematical expression will produce

1365
01:25:13.700 --> 01:25:16.100
that that's looking in line. We just looked at.

1366
01:25:22.200 --> 01:25:24.500
Okay, so, let's see. Let's derivation.

1367
01:25:25.800 --> 01:25:29.100
first of all, if the number of probability is a

1368
01:25:28.100 --> 01:25:31.200
 number between a real number between 0 and

1369
01:25:31.200 --> 01:25:31.500
 1

1370
01:25:36.000 --> 01:25:36.100
so

1371
01:25:39.300 --> 01:25:42.400
and it's given by this number is given by a ratio. It is

1372
01:25:42.400 --> 01:25:43.500
 the total outcomes.

1373
01:25:44.600 --> 01:25:47.400
In the denominator and the chosen outcomes on

1374
01:25:47.400 --> 01:25:49.700
 the numerator. So for example, let's consider flipping a coin.

1375
01:25:51.900 --> 01:25:52.500
or flipping

1376
01:25:54.200 --> 01:25:54.800
two coins

1377
01:25:57.500 --> 01:26:00.400
And you want to know what is the probability of getting getting either

1378
01:26:00.400 --> 01:26:03.200
 heads and tails or two tails or two heads?

1379
01:26:04.600 --> 01:26:07.600
The total number of outcomes are these three

1380
01:26:07.600 --> 01:26:10.800
 you can get two heads two tails or one

1381
01:26:10.800 --> 01:26:13.200
 heads and tails and this is a combination not a

1382
01:26:13.200 --> 01:26:15.100
 permutation so we don't care about the order of the coin.

1383
01:26:16.100 --> 01:26:19.100
So it doesn't matter if you get it tells her hairs or heads or tails with the

1384
01:26:19.100 --> 01:26:19.300
 same thing.

1385
01:26:21.100 --> 01:26:23.500
Anyone know what is a probability of getting two tails?

1386
01:26:25.700 --> 01:26:29.300
All you well, there's only one such.

1387
01:26:31.000 --> 01:26:31.600
outcome

1388
01:26:32.200 --> 01:26:35.300
from your total number of outcomes. So that's one over three or

1389
01:26:35.300 --> 01:26:36.200
 33%

1390
01:26:37.300 --> 01:26:39.300
if we can round it up to 34% if you want.

1391
01:26:41.300 --> 01:26:44.000
The chance of getting it tails on the Tails when you flip

1392
01:26:44.200 --> 01:26:44.600
 two coins.

1393
01:26:45.200 --> 01:26:46.900
Is 33%

1394
01:26:48.200 --> 01:26:49.100
That's probability.

1395
01:26:49.800 --> 01:26:50.100
Okay.

1396
01:26:51.400 --> 01:26:54.500
A several example is flipping one coin. How many outcomes you

1397
01:26:54.500 --> 01:26:57.100
 have? You only have two heads or tails. So what is it probably

1398
01:26:57.100 --> 01:26:57.800
 of getting your heads?

1399
01:26:59.100 --> 01:26:59.700
50%

1400
01:27:01.500 --> 01:27:04.300
Your total number of outcomes is two and then you're chosen outcome

1401
01:27:04.300 --> 01:27:05.800
 is one. So 1 over 2. That's 50%

1402
01:27:07.200 --> 01:27:09.300
Of course, assuming you have a perfectly balanced coin.

1403
01:27:11.200 --> 01:27:12.000
Then we have odds.

1404
01:27:13.700 --> 01:27:16.600
Odds is a is the ratio of two probabilities.

1405
01:27:19.700 --> 01:27:22.300
For example, you want to know what is the probability of getting a

1406
01:27:25.500 --> 01:27:28.500
let's say you have two people who are very making

1407
01:27:28.500 --> 01:27:28.900
 a bet.

1408
01:27:29.500 --> 01:27:32.300
And we want to know who has a higher probability of winning the bet.

1409
01:27:33.400 --> 01:27:36.500
One person bets on getting a tails and Tails. The

1410
01:27:36.500 --> 01:27:39.100
 other person gets bets on getting either heads or

1411
01:27:39.100 --> 01:27:39.300
 heads.

1412
01:27:39.800 --> 01:27:40.800
or heads and tails

1413
01:27:42.200 --> 01:27:42.500
so they have

1414
01:27:45.900 --> 01:27:48.100
a higher probability because they have more

1415
01:27:49.400 --> 01:27:51.800
the their number of chosen outcomes is greater.

1416
01:27:53.300 --> 01:27:56.800
And you might say this is unfair. Well so are many so

1417
01:27:56.800 --> 01:27:59.300
 is the real world, right? So for example in horse betting

1418
01:28:00.100 --> 01:28:03.400
people who can buy more bets have more number of

1419
01:28:03.400 --> 01:28:06.200
 outcomes if they better more horses naturally have

1420
01:28:06.200 --> 01:28:07.400
 a higher probability of winning.

1421
01:28:08.600 --> 01:28:11.200
Whereas if I buy if I put all my money on one

1422
01:28:11.200 --> 01:28:14.400
 horse. Well, I have a smaller problem and that's that's what

1423
01:28:14.400 --> 01:28:17.100
 odds is the odds of that. Let's say you have

1424
01:28:17.100 --> 01:28:20.100
 bet on three horses or in this case. Let's say it's two

1425
01:28:20.100 --> 01:28:23.400
 because we have your number of choice. Your number

1426
01:28:23.400 --> 01:28:25.800
 of outcomes is closed the total number of outcomes.

1427
01:28:26.700 --> 01:28:29.200
You'd have a higher probability. So if you compare your

1428
01:28:29.200 --> 01:28:32.200
 probability versus my probability, that's the odds ratio.

1429
01:28:32.900 --> 01:28:35.600
The odds would be in your favor you would

1430
01:28:35.600 --> 01:28:35.900
 have twice.

1431
01:28:37.200 --> 01:28:39.300
The odds of you winning is twice as high.

1432
01:28:40.400 --> 01:28:43.200
We'd also the probability as toys as how you say the odds is

1433
01:28:43.200 --> 01:28:43.800
 twice as high.

1434
01:28:44.800 --> 01:28:47.600
because if we divide your probability over my

1435
01:28:47.600 --> 01:28:48.900
 probability we will get to

1436
01:28:51.600 --> 01:28:51.900
okay.

1437
01:28:53.300 --> 01:28:53.600
now

1438
01:29:01.700 --> 01:29:04.200
another way to express the same ratio.

1439
01:29:07.200 --> 01:29:07.600
is to

1440
01:29:12.300 --> 01:29:15.700
Take my probability and subtract it from one. Why

1441
01:29:15.700 --> 01:29:17.200
 because well

1442
01:29:19.600 --> 01:29:22.600
There are three outcomes three three possible outcomes.

1443
01:29:22.600 --> 01:29:24.500
 I have taken one of them.

1444
01:29:26.200 --> 01:29:27.500
So there are two remaining.

1445
01:29:29.800 --> 01:29:32.700
So out of the 100% of the outcomes I

1446
01:29:32.700 --> 01:29:33.100
 have taken.

1447
01:29:33.800 --> 01:29:35.300
33% of it

1448
01:29:36.300 --> 01:29:39.200
That's what 1 minus 3 is. Oh, that's why we

1449
01:29:39.200 --> 01:29:42.400
 have one minus p t t so if I had

1450
01:29:42.400 --> 01:29:42.600
 no.

1451
01:29:45.400 --> 01:29:46.600
If you are taking all three.

1452
01:29:49.400 --> 01:29:52.300
Outcomes you would have 100% probability of winning

1453
01:29:52.300 --> 01:29:55.000
 because you have bet on all three outcomes. But if I take

1454
01:29:55.300 --> 01:29:58.600
 one of them you have one less than the entirety. That's

1455
01:29:58.600 --> 01:30:01.100
 what we have one minus PTT. Either way. You will get the same

1456
01:30:01.100 --> 01:30:01.600
 number.

1457
01:30:02.900 --> 01:30:05.800
Okay. Now why do we want to re-express the

1458
01:30:05.800 --> 01:30:08.200
 problem the express the odds ratio in this

1459
01:30:08.200 --> 01:30:10.400
 way because this is going to bring us to log on.

1460
01:30:12.500 --> 01:30:14.300
the log on it's about taking the

1461
01:30:15.300 --> 01:30:18.200
now if you see here what we have done is we

1462
01:30:18.200 --> 01:30:19.100
 have inverted.

1463
01:30:19.900 --> 01:30:20.700
the ratio

1464
01:30:22.300 --> 01:30:25.500
Okay. So this was P of X in the

1465
01:30:25.500 --> 01:30:28.300
 denominator and one minus P of X in the numerator here.

1466
01:30:28.300 --> 01:30:30.100
 We have just flipped it. I just

1467
01:30:31.800 --> 01:30:34.700
reversed in the previous example this you can understand the odds and the

1468
01:30:34.700 --> 01:30:37.500
 anal index and the analog of two people betting and one

1469
01:30:37.500 --> 01:30:37.900
 has

1470
01:30:40.200 --> 01:30:44.200
they have the luxury of more chosen outcomes than

1471
01:30:43.200 --> 01:30:44.600
 the other person.

1472
01:30:45.500 --> 01:30:46.500
but if we flip it around

1473
01:30:47.600 --> 01:30:48.200
we will get

1474
01:30:49.600 --> 01:30:51.900
We will still have the ratio but we have a problem.

1475
01:30:52.800 --> 01:30:54.500
the reason we use logarithm

1476
01:30:55.100 --> 01:30:58.300
In any context of probability, so there's something called likelihood and

1477
01:30:58.300 --> 01:31:01.100
 maximum likelihood you may have heard of this. The reason we

1478
01:31:01.100 --> 01:31:04.600
 use a lot a logarithmic function is to change the

1479
01:31:04.600 --> 01:31:07.100
 scale of the probability. You see the number can be

1480
01:31:07.100 --> 01:31:08.800
 very small if I have for example

1481
01:31:13.500 --> 01:31:16.500
0.05 probability over

1482
01:31:16.500 --> 01:31:17.000
 let's say

1483
01:31:19.600 --> 01:31:22.500
one minus P of X. It will be 1 minus.

1484
01:31:26.000 --> 01:31:26.900
0.5.

1485
01:31:28.400 --> 01:31:29.700
You see the probability is very small.

1486
01:31:30.400 --> 01:31:31.700
0.5%

1487
01:31:32.800 --> 01:31:33.100
and

1488
01:31:34.300 --> 01:31:37.100
in the case of multi-class classification, you will

1489
01:31:37.100 --> 01:31:41.000
 have all of these very tiny properties 5% 1% less

1490
01:31:40.200 --> 01:31:41.700
 than 1%

1491
01:31:42.600 --> 01:31:45.200
So for arithmetic purposes and in particular

1492
01:31:45.200 --> 01:31:46.200
 competition purposes

1493
01:31:47.500 --> 01:31:49.200
We want to kind of rescale.

1494
01:31:51.300 --> 01:31:54.700
The probability so for example, if I take 5% and I

1495
01:31:54.700 --> 01:31:57.000
 put it in the logarithmic function and the base of

1496
01:31:57.100 --> 01:31:59.700
 the logarithm is not really important. I have changed the scale.

1497
01:32:01.700 --> 01:32:03.400
So it's not one point negative 1.3.

1498
01:32:05.100 --> 01:32:08.200
Okay, so really what I want to get across is this

1499
01:32:08.200 --> 01:32:11.300
 the choice of the base is not that important.

1500
01:32:12.300 --> 01:32:15.000
But we want to apply the log rhythmic function just to change

1501
01:32:15.400 --> 01:32:18.200
 the scale. And of course log base of e is just a natural log with

1502
01:32:18.200 --> 01:32:19.700
 natural logarithm.

1503
01:32:21.300 --> 01:32:21.500
Okay.

1504
01:32:24.700 --> 01:32:27.200
Then we have the logic function. The largest function is

1505
01:32:27.200 --> 01:32:30.200
 really that when you apply the logarithmic function

1506
01:32:30.200 --> 01:32:33.800
 to your odds, that's where you call the logic function.

1507
01:32:34.600 --> 01:32:35.900
So let me zoom out.

1508
01:32:39.100 --> 01:32:42.100
So the expression we saw previously, that's the logic.

1509
01:32:44.300 --> 01:32:47.700
And it turns out when you evaluate this

1510
01:32:47.700 --> 01:32:50.900
 expression it is the same thing as evaluating beta

1511
01:32:50.900 --> 01:32:53.200
 Sub Zero and Beta Support. Now, what is better Sub Zero

1512
01:32:53.200 --> 01:32:53.700
 better sub one

1513
01:32:54.400 --> 01:32:57.400
These are the coefficients.

1514
01:32:59.500 --> 01:33:00.500
of the regression model

1515
01:33:02.400 --> 01:33:02.700
so

1516
01:33:06.500 --> 01:33:09.000
did I reference to the regression model anywhere else?

1517
01:33:10.200 --> 01:33:10.900
Not here.

1518
01:33:12.900 --> 01:33:15.300
The regression model is given by this. Let me

1519
01:33:15.300 --> 01:33:16.900
 type it very quickly.

1520
01:33:25.500 --> 01:33:28.300
And I think I actually have it here generalized.

1521
01:33:52.900 --> 01:33:53.800
Yes, this is it.

1522
01:34:02.900 --> 01:34:04.600
aggression

1523
01:34:14.100 --> 01:34:15.200
So let's move this up here.

1524
01:34:19.800 --> 01:34:22.100
And for Simplicity, let me remove the other.

1525
01:34:23.600 --> 01:34:24.400
Variables and let's just say we have two.

1526
01:34:28.200 --> 01:34:29.000
You have two variables.

1527
01:34:34.700 --> 01:34:36.500
The Epsilon is the residual.

1528
01:34:37.600 --> 01:34:40.300
That's the error rate. Let me not include that

1529
01:34:40.300 --> 01:34:42.200
 in here either just really keep it simple.

1530
01:34:43.200 --> 01:34:44.900
and let's just have beta sub 1

1531
01:34:46.100 --> 01:34:50.200
so beta Sub Zero is what we call the bias term.

1532
01:34:51.200 --> 01:34:54.300
Or the why it's the number on the wall by intercept.

1533
01:34:54.300 --> 01:34:57.500
 So in the value function, the bias was in the x-axis in

1534
01:34:57.500 --> 01:35:00.400
 the general regression model devices and the y axis

1535
01:35:01.100 --> 01:35:01.900
this is the y-intercept.

1536
01:35:02.700 --> 01:35:04.700
and this is the

1537
01:35:07.200 --> 01:35:10.200
x sub 1 is the feature and this is

1538
01:35:10.200 --> 01:35:11.200
 its weight.

1539
01:35:12.700 --> 01:35:15.200
Now we talk we'll talk about real estate size of the

1540
01:35:15.200 --> 01:35:16.400
 property number of bedrooms Etc.

1541
01:35:17.800 --> 01:35:20.500
X sub one is one

1542
01:35:20.500 --> 01:35:23.500
 feature and beta sub one. It's it's the weight associated with

1543
01:35:23.500 --> 01:35:25.300
 the correct weight associated with it.

1544
01:35:26.100 --> 01:35:26.400
Okay.

1545
01:35:28.300 --> 01:35:30.300
That's where beat us up 0 and better someone come from.

1546
01:35:31.200 --> 01:35:33.700
so if we return to this the logic function

1547
01:35:36.600 --> 01:35:36.900
the

1548
01:35:38.300 --> 01:35:41.200
Numerical value that we will get from a validity this expression

1549
01:35:41.200 --> 01:35:42.300
 is the same as that expression.

1550
01:35:43.600 --> 01:35:43.800
now

1551
01:35:45.700 --> 01:35:48.900
if we apply if we want to get rid of the logarithm if

1552
01:35:48.900 --> 01:35:50.100
 you want to change the scale again.

1553
01:35:50.900 --> 01:35:53.100
We will have to take this expression.

1554
01:35:55.400 --> 01:35:58.500
And raise it to the exponential function

1555
01:35:58.500 --> 01:36:00.300
 to cancel to eliminate log.

1556
01:36:01.100 --> 01:36:02.100
And that's what we're doing here.

1557
01:36:02.700 --> 01:36:03.900
We've taken the expression above.

1558
01:36:04.800 --> 01:36:07.300
And raise it to e but that

1559
01:36:07.300 --> 01:36:09.800
 mean but this is algebra. So we have to do with the other side as well.

1560
01:36:12.800 --> 01:36:15.100
Then of course e to the power log e will get

1561
01:36:15.100 --> 01:36:15.500
 eliminated.

1562
01:36:17.900 --> 01:36:19.700
and this expression

1563
01:36:21.100 --> 01:36:23.800
Oops, this expression comes down here.

1564
01:36:26.900 --> 01:36:29.300
Now in order to derive the sigmoid function, we have to do some

1565
01:36:29.300 --> 01:36:30.400
 algebraic manipulation.

1566
01:36:31.700 --> 01:36:34.100
Which is going to be a multiple steps to what I'm going to do is I'm going to

1567
01:36:34.100 --> 01:36:35.200
 avoid that instead what I've done.

1568
01:36:36.300 --> 01:36:36.700
is I have

1569
01:36:37.400 --> 01:36:38.400
created

1570
01:36:40.600 --> 01:36:43.700
if you can go to symbol lab what you looked at before to basically

1571
01:36:43.700 --> 01:36:47.200
 move one minus P of x to the other side and

1572
01:36:46.200 --> 01:36:49.700
 then after a little bit of algebraic manipulation.

1573
01:36:50.300 --> 01:36:50.900
We get this.

1574
01:36:53.700 --> 01:36:57.400
This is the formula. Basically this is odds. This

1575
01:36:56.400 --> 01:36:57.800
 is the odds.

1576
01:36:58.900 --> 01:37:01.300
In the context of binary classification we asking

1577
01:37:01.300 --> 01:37:04.800
 what is the what is the probability of it being

1578
01:37:04.800 --> 01:37:07.600
 this class versus being in that class? What

1579
01:37:07.600 --> 01:37:09.700
 is that? Probably of being of this particular?

1580
01:37:13.300 --> 01:37:13.800
feature

1581
01:37:14.900 --> 01:37:17.600
being fraud versus not front.

1582
01:37:17.600 --> 01:37:20.000
 What is a odds of this being?

1583
01:37:22.500 --> 01:37:25.500
Well, actually probability so we have taken the concept of odds and

1584
01:37:25.500 --> 01:37:28.900
 we have now representing as making it a probabilistic

1585
01:37:28.900 --> 01:37:31.100
 number. So what is it now the probability of

1586
01:37:31.100 --> 01:37:34.100
 being a Spam versus the probability of not being spam. What

1587
01:37:34.100 --> 01:37:35.500
 is it? What is the probability of it being?

1588
01:37:39.200 --> 01:37:42.100
An animal versus that probably of it not being in and that's the

1589
01:37:42.100 --> 01:37:42.400
 idea here.

1590
01:37:43.700 --> 01:37:44.000
Okay.

1591
01:37:45.400 --> 01:37:45.900
so this is very

1592
01:37:47.600 --> 01:37:48.200
sped up.

1593
01:37:49.700 --> 01:37:50.500
explanation of

1594
01:37:52.200 --> 01:37:53.500
the sigboid function

1595
01:37:54.600 --> 01:37:57.000
So what do we will do is then every time we get a feature?

1596
01:37:58.200 --> 01:37:59.800
it will be a

1597
01:38:01.900 --> 01:38:03.400
it's valuable be inside.

1598
01:38:04.900 --> 01:38:07.100
The probability of it being in a class

1599
01:38:07.100 --> 01:38:08.600
 versus the probability of not being in the class.

1600
01:38:12.300 --> 01:38:12.600
Okay.

1601
01:38:19.400 --> 01:38:20.200
and so that's what

1602
01:38:21.600 --> 01:38:22.800
the sigmoid is going to do.

1603
01:38:24.400 --> 01:38:27.300
if it could if it goes above the threshold it will classify

1604
01:38:27.300 --> 01:38:27.800
 as

1605
01:38:29.300 --> 01:38:32.500
let's say fraud if it falls below the threshold will classify as

1606
01:38:32.500 --> 01:38:34.700
 non-front binary classification.

1607
01:38:36.300 --> 01:38:38.800
So the observations will end up up here or down here.

1608
01:38:40.100 --> 01:38:43.100
Let me see if I can see one with the observations on it.

1609
01:38:49.200 --> 01:38:52.900
Yeah, and apparently sigmoid is a term used

1610
01:38:52.900 --> 01:38:53.700
 in medicine as well.

1611
01:39:01.900 --> 01:39:04.300
Come to think of it. I actually have a notebook where I

1612
01:39:04.300 --> 01:39:05.300
 have visualized all of this.

1613
01:39:12.400 --> 01:39:14.800
but it's already 15 minutes past the

1614
01:39:18.100 --> 01:39:21.500
The time and we don't have we still haven't got into

1615
01:39:21.500 --> 01:39:22.100
 cross entropy.

1616
01:39:25.200 --> 01:39:28.400
Alright, I'll do this. I have the notebook. Let me just show you let me

1617
01:39:28.400 --> 01:39:31.600
 show you the picture. I mean, let me show you and I'm

1618
01:39:31.600 --> 01:39:31.800
 GitHub.

1619
01:39:32.700 --> 01:39:35.200
But I'll put the I'll put the notebook on

1620
01:39:35.200 --> 01:39:38.700
 slack. The comments should be self-explanatory or is

1621
01:39:38.700 --> 01:39:39.800
 the let me show you where it is.

1622
01:39:43.800 --> 01:39:45.300
statistics with python

1623
01:39:48.700 --> 01:39:50.900
It's one of these notebooks. Let me open them all.

1624
01:39:56.800 --> 01:39:57.100
Yeah.

1625
01:39:58.300 --> 01:39:58.900
Here's one.

1626
01:40:02.100 --> 01:40:03.000
I think this is the one.

1627
01:40:07.600 --> 01:40:09.500
Yeah, so you can go I'll put this.

1628
01:40:11.300 --> 01:40:14.500
I'll put this link in this is of course some GitHub you

1629
01:40:14.500 --> 01:40:17.600
 can access it. You don't need anything, but I'll

1630
01:40:17.600 --> 01:40:20.500
 share this with you. Just go to the

1631
01:40:20.500 --> 01:40:21.100
 sales one by one.

1632
01:40:21.800 --> 01:40:24.100
So you can see this is the this is

1633
01:40:24.100 --> 01:40:27.300
 what the sigmoid function does, right it plots it on

1634
01:40:27.300 --> 01:40:29.600
 this on this s looking.

1635
01:40:31.700 --> 01:40:35.000
I mean, this is this this is this you're seeing

1636
01:40:34.600 --> 01:40:37.800
 them. This is the model plotting. It's

1637
01:40:37.800 --> 01:40:39.800
 not the actual observations is the model of the observations.

1638
01:40:41.400 --> 01:40:43.800
And if you if you draw a line.

1639
01:40:45.800 --> 01:40:48.800
you will get this red line, but what the

1640
01:40:52.800 --> 01:40:54.100
the expert function does

1641
01:40:55.300 --> 01:40:55.600
is

1642
01:40:58.300 --> 01:40:59.500
like you're seeing here it will.

1643
01:41:00.300 --> 01:41:02.100
push it up to one if it's

1644
01:41:04.800 --> 01:41:07.300
if it's a true if it's a truth classification and bring

1645
01:41:07.300 --> 01:41:09.500
 it all the way down to zero to false classification.

1646
01:41:10.600 --> 01:41:13.100
That's really what I try. I mean, I'm trying to speed up

1647
01:41:13.100 --> 01:41:13.800
 the explanation.

1648
01:41:15.300 --> 01:41:15.500
Okay.

1649
01:41:17.500 --> 01:41:19.700
and we didn't talk about I mean did

1650
01:41:23.500 --> 01:41:26.100
There is also recall and accuracy. I said

1651
01:41:26.100 --> 01:41:29.200
 we talk about this if we have time, but we don't have anymore much we gonna have much

1652
01:41:29.200 --> 01:41:31.900
 more time. Let me also remind you about this.

1653
01:41:34.600 --> 01:41:37.300
six days from now we have the second

1654
01:41:37.300 --> 01:41:38.000
 episode of

1655
01:41:39.200 --> 01:41:40.000
the ml papers

1656
01:41:45.400 --> 01:41:48.800
by the way, I strongly check this out because we're also now

1657
01:41:48.800 --> 01:41:50.400
 an ambassador of deep learning AI.

1658
01:41:51.600 --> 01:41:54.700
Which means if you participate in your

1659
01:41:54.700 --> 01:41:57.300
 complete a survey from Deep learning AI you

1660
01:41:57.300 --> 01:42:00.700
 can get a 50% discount on any one of the courses on

1661
01:42:00.700 --> 01:42:01.300
 Coursera.

1662
01:42:01.900 --> 01:42:04.300
So in in case you don't know deep learning

1663
01:42:04.300 --> 01:42:05.800
 they that AI is founded by.

1664
01:42:08.500 --> 01:42:09.600
Andrew Ong

1665
01:42:10.400 --> 01:42:13.700
I mean, I think everybody knows that he's the Stanford professor

1666
01:42:13.700 --> 01:42:13.800
 of

1667
01:42:14.500 --> 01:42:17.200
Machine learning, you know, he's the one who founded Coursera.

1668
01:42:18.100 --> 01:42:19.800
And that very famous machine learning course.

1669
01:42:21.100 --> 01:42:24.600
So we know I applied that we we got accepted

1670
01:42:24.600 --> 01:42:27.800
 as Ambassador. So, you know if you scroll down to the details.

1671
01:42:29.600 --> 01:42:31.400
Yeah, if you participate you can.

1672
01:42:33.100 --> 01:42:34.700
Get a 50% discount on the course.

1673
01:42:36.700 --> 01:42:39.300
Now I did promise to allocate 10

1674
01:42:39.300 --> 01:42:42.000
 minutes or so for any questions. And since this is the last

1675
01:42:42.300 --> 01:42:46.300
 course first of all, I appreciate that. You know, you stayed with me now 17

1676
01:42:45.300 --> 01:42:47.100
 minutes past the

1677
01:42:48.600 --> 01:42:49.900
session but

1678
01:42:51.200 --> 01:42:54.500
I'm gonna try to give you the remaining material anyway.

1679
01:42:55.700 --> 01:42:58.200
But let me worry. This is now beyond the algebra like we've

1680
01:42:58.200 --> 01:43:01.200
 got personally not up. This is now in the realm of probability Theory.

1681
01:43:02.500 --> 01:43:04.100
probability Theory and statistics

1682
01:43:05.200 --> 01:43:08.200
But let me just maybe for a few more minutes

1683
01:43:08.200 --> 01:43:11.700
 see if you have any questions suggestions questions about

1684
01:43:11.700 --> 01:43:15.000
 this session anything anything the past anything in

1685
01:43:14.100 --> 01:43:15.200
 the future?

1686
01:43:16.500 --> 01:43:19.200
now there are more courses coming up

1687
01:43:19.200 --> 01:43:21.200
 I'm going to for

1688
01:43:22.500 --> 01:43:23.000
August

1689
01:43:28.400 --> 01:43:30.300
our post about this on the slack Channel

1690
01:43:33.100 --> 01:43:36.600
I'm also going to send everybody an email to

1691
01:43:36.600 --> 01:43:38.100
 complete a survey.

1692
01:43:39.200 --> 01:43:42.100
And you will be rewarded for the the

1693
01:43:42.100 --> 01:43:45.500
 survey slash testimonial. So do check

1694
01:43:45.500 --> 01:43:48.400
 it out. I'll try to send it to you also not

1695
01:43:48.400 --> 01:43:48.900
 send it to you today.

1696
01:43:50.100 --> 01:43:51.800
Especially for those of you who are participating.

1697
01:43:52.900 --> 01:43:55.600
Who participate in all of these sessions but

1698
01:43:55.600 --> 01:43:57.800
 let me stop talking for second any questions or comments.

1699
01:44:00.300 --> 01:44:03.600
Anything you know, you you want to because by the way once wants

1700
01:44:03.600 --> 01:44:06.100
 to the session is over. I'm going to keep on

1701
01:44:06.100 --> 01:44:07.600
 adding improvements.

1702
01:44:08.700 --> 01:44:11.300
like the short video tutorials notebooks

1703
01:44:12.100 --> 01:44:13.600
here and there. Thank you.

1704
01:44:18.900 --> 01:44:21.100
Is that Rudy or is that Ramsey? I don't

1705
01:44:21.100 --> 01:44:24.600
 know. I can't see your first name in the chat. But thank you very much. No

1706
01:44:24.600 --> 01:44:25.600
 questions or comments.

1707
01:44:27.800 --> 01:44:28.600
no suggestions

1708
01:44:29.700 --> 01:44:32.700
Well, if you do have suggestions and you're not going to answer now, do

1709
01:44:32.700 --> 01:44:35.700
 do fill out the survey, please I'll really

1710
01:44:35.700 --> 01:44:35.900
 appreciate it.

1711
01:44:37.100 --> 01:44:39.000
And I will try to make it worth your while.

1712
01:44:41.100 --> 01:44:44.500
Well, today's notebooks Beyond. Yeah, the notebooks will definitely all

1713
01:44:44.500 --> 01:44:47.200
 the markdown files and what I usually do

1714
01:44:47.200 --> 01:44:50.400
 after every Workshop is I clean up the I clean up

1715
01:44:50.400 --> 01:44:52.500
 the files at comments.

1716
01:44:53.700 --> 01:44:57.100
And put the coding the markdown file

1717
01:44:56.100 --> 01:44:59.700
 Jupiter notebook all on the

1718
01:44:59.700 --> 01:44:59.900
 dashboard.

1719
01:45:00.700 --> 01:45:01.300
For sure.

1720
01:45:03.900 --> 01:45:05.600
The only questions no more comments.

1721
01:45:08.400 --> 01:45:10.000
Very good in that case.

1722
01:45:11.500 --> 01:45:13.500
Do check your email?

1723
01:45:15.900 --> 01:45:18.200
I will send it in a few hours that I do have a

1724
01:45:18.200 --> 01:45:18.700
 look at the email.

1725
01:45:19.700 --> 01:45:19.900
and

1726
01:45:21.900 --> 01:45:24.500
yeah, do keeps do stay

1727
01:45:24.500 --> 01:45:26.000
 tuned on the main slide Channel.

1728
01:45:27.300 --> 01:45:28.200
Because I'm going to post.

1729
01:45:29.500 --> 01:45:30.700
updates

1730
01:45:31.600 --> 01:45:33.700
For the upcoming course that starts in August.

1731
01:45:35.100 --> 01:45:38.200
And for now, let me thank you very much for joining me, especially for

1732
01:45:38.200 --> 01:45:39.500
 those of you who join me for all.

1733
01:45:40.900 --> 01:45:41.700
these four weeks

1734
01:45:43.600 --> 01:45:47.100
and I look forward to your response

1735
01:45:46.100 --> 01:45:48.600
 in the email about to send you.

1736
01:45:49.100 --> 01:45:51.400
And I look forward to seeing you in the next session.

1737
01:45:52.600 --> 01:45:55.400
So until then for now, take care. Enjoy the

1738
01:45:55.400 --> 01:45:56.000
 rest of your day.

1739
01:45:56.800 --> 01:45:58.200
I'll see you soon. Bye.
