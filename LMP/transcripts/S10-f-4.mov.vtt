WEBVTT - This file was automatically generated by VIMEO

0
00:00:02.700 --> 00:00:03.700
Okay, let me use this notebook.

1
00:00:05.200 --> 00:00:06.900
So as opposed to typing it from scratch.

2
00:00:07.600 --> 00:00:10.300
I'm going to show you the whole thing and then we will break it

3
00:00:10.300 --> 00:00:10.600
 down.

4
00:00:11.900 --> 00:00:13.100
I'll break down the code but

5
00:00:20.400 --> 00:00:23.300
There is the we need to cover some more

6
00:00:23.300 --> 00:00:24.200
 we need to cover.

7
00:00:26.400 --> 00:00:26.600
some

8
00:00:31.400 --> 00:00:33.900
prerequisites in statistics and

9
00:00:34.700 --> 00:00:36.100
multivariable calculus

10
00:00:36.600 --> 00:00:39.200
I already have a video for that. So I will tell you

11
00:00:39.200 --> 00:00:40.200
 where to find. It's on YouTube.

12
00:00:41.500 --> 00:00:43.400
I'll post a video in the slack Channel.

13
00:00:45.200 --> 00:00:46.000
All right, so let's begin.

14
00:00:49.300 --> 00:00:50.500
We have pandas.

15
00:00:52.300 --> 00:00:56.200
Pandas is a library that extends the capabilities

16
00:00:55.200 --> 00:00:57.000
 of numpy.

17
00:00:58.600 --> 00:01:01.800
It extends it by turning a

18
00:01:01.800 --> 00:01:04.300
 numpy array into a thing called

19
00:01:04.300 --> 00:01:05.100
 a data frame.

20
00:01:06.200 --> 00:01:10.000
And it's data frames are really good

21
00:01:09.200 --> 00:01:12.800
 because you can convert an Excel sheet into

22
00:01:12.800 --> 00:01:13.600
 a python.

23
00:01:15.200 --> 00:01:15.800
object

24
00:01:16.600 --> 00:01:19.300
so you can see the column names of the CSV file

25
00:01:19.300 --> 00:01:21.200
 or Excel file or tsv file?

26
00:01:22.400 --> 00:01:24.700
You can perform summary statistics.

27
00:01:25.500 --> 00:01:26.600
the summary statistics

28
00:01:27.400 --> 00:01:30.800
Or what actually what I should say is descriptive statistics.

29
00:01:30.800 --> 00:01:33.400
 So we have to descriptive statistics, which is very basic stuff.

30
00:01:34.100 --> 00:01:37.100
And then there's inferential statistics. This is the most sophisticated.

31
00:01:38.800 --> 00:01:41.100
In descriptive statistics, we want to see for example, what is

32
00:01:41.100 --> 00:01:43.200
 what are the averages the mean median and mode?

33
00:01:44.200 --> 00:01:45.600
That's what we call measures of.

34
00:01:47.500 --> 00:01:50.000
Central tendency like what is it centered?

35
00:01:50.800 --> 00:01:51.900
and the second part of

36
00:01:53.500 --> 00:01:56.200
descriptive statistics is measure of variance.

37
00:01:57.900 --> 00:02:00.100
Or other measures of

38
00:02:00.100 --> 00:02:00.900
 this Persian.

39
00:02:01.400 --> 00:02:04.500
How much variation do we have in data? If you know how much variation we

40
00:02:04.500 --> 00:02:07.500
 have? And what is the center of the data? We can approximate the

41
00:02:07.500 --> 00:02:10.000
 shape of data and knowing the shape of

42
00:02:10.600 --> 00:02:11.800
 data is a huge huge deal.

43
00:02:14.600 --> 00:02:17.100
And if you want to know why I'll in that video.

44
00:02:17.100 --> 00:02:20.400
 It's a one video in that one video.

45
00:02:20.400 --> 00:02:23.500
 I will talk about distributions and gradient descent

46
00:02:23.500 --> 00:02:24.900
 and that should give you a good enough.

47
00:02:27.700 --> 00:02:30.900
A review of probably talking about in

48
00:02:30.900 --> 00:02:32.500
 the subsequent sessions or even in here.

49
00:02:33.200 --> 00:02:35.700
So we will have what pan doesn't have carers Keras.

50
00:02:36.300 --> 00:02:40.200
Is a library birth on top of tensorflow basically the

51
00:02:39.200 --> 00:02:42.200
 abstracts away some of the complexities of

52
00:02:42.200 --> 00:02:46.100
 tensorflow. So if you want to build a quick simple new network,

53
00:02:45.100 --> 00:02:48.600
 you may you want to go with Keras as

54
00:02:48.600 --> 00:02:49.500
 opposed to tensorflow.

55
00:02:52.200 --> 00:02:55.600
In here, we are going to create a sequential model why sequential because

56
00:02:55.600 --> 00:02:58.100
 we can have a sequence of layers.

57
00:02:59.700 --> 00:03:00.500
input layer

58
00:03:01.200 --> 00:03:04.200
a few or several hidden layers and then

59
00:03:04.200 --> 00:03:05.000
 an upper layer.

60
00:03:06.900 --> 00:03:09.600
Of course, but we want to we need to put layers inside

61
00:03:09.600 --> 00:03:12.500
 this sequence. So we're going to import everything again.

62
00:03:12.500 --> 00:03:15.800
 The storm is everything so from keras.layers, we

63
00:03:15.800 --> 00:03:16.900
 will import everything all the layers.

64
00:03:19.500 --> 00:03:22.800
Another important part of machine learning is evaluating the

65
00:03:22.800 --> 00:03:22.900
 model.

66
00:03:24.600 --> 00:03:25.800
But I believe that for later.

67
00:03:26.800 --> 00:03:28.100
This is for more evaluation.

68
00:03:29.700 --> 00:03:31.100
We have the CSV data.

69
00:03:32.300 --> 00:03:34.000
Which you I'll show you right right down below.

70
00:03:34.800 --> 00:03:36.100
And I will give this file to you later.

71
00:03:37.900 --> 00:03:39.700
We will import instead of using pandas.

72
00:03:40.600 --> 00:03:43.400
Sorry about that. We will input the data using pandas.

73
00:03:44.200 --> 00:03:47.500
And you will visualize the first five rows.

74
00:03:47.500 --> 00:03:48.500
 That's what the head method does.

75
00:03:50.700 --> 00:03:51.800
You can see here we have.

76
00:03:52.800 --> 00:03:53.700
the year built

77
00:03:54.600 --> 00:03:55.600
the number of stories

78
00:03:56.400 --> 00:03:57.100
the number of

79
00:03:57.900 --> 00:03:59.400
one story house two story house

80
00:04:00.200 --> 00:04:01.000
the number of bedrooms

81
00:04:02.400 --> 00:04:05.500
the number of full bathrooms. I have others four bathrooms our

82
00:04:05.500 --> 00:04:08.300
 bathrooms that contain, you know showers or baths.

83
00:04:09.400 --> 00:04:11.300
Now one thing you'll notice is that these numbers.

84
00:04:12.900 --> 00:04:14.000
are normalized

85
00:04:17.100 --> 00:04:20.400
so no value goes between no everybody every

86
00:04:20.400 --> 00:04:21.500
 numbers between 0 and 1.

87
00:04:23.200 --> 00:04:25.600
of course the way we can normalize this data using

88
00:04:28.200 --> 00:04:29.400
scikit learn

89
00:04:30.600 --> 00:04:31.700
Cyclone is a library for machine learning.

90
00:04:32.700 --> 00:04:35.200
But this data this this state has been normalized.

91
00:04:37.300 --> 00:04:37.900
So for example

92
00:04:41.200 --> 00:04:45.600
If let's say one of the let's say we have one

93
00:04:45.600 --> 00:04:47.000
 property that has six.

94
00:04:48.700 --> 00:04:51.000
Bathrooms, it's a very spacious house.

95
00:04:52.500 --> 00:04:53.600
Cup Penthouse or something?

96
00:04:54.600 --> 00:04:54.900
then

97
00:04:56.200 --> 00:04:58.900
if when we normalize it or if you

98
00:05:03.800 --> 00:05:06.200
if we if we normalize that this will be this will have

99
00:05:06.200 --> 00:05:07.200
 the number one.

100
00:05:09.200 --> 00:05:11.300
the highest number of full bathrooms

101
00:05:12.100 --> 00:05:15.400
So if you want to know how many or yeah if

102
00:05:15.400 --> 00:05:17.900
 you want to know for example, how many bathrooms?

103
00:05:18.800 --> 00:05:22.000
This apartment has you will multiply six by

104
00:05:21.800 --> 00:05:22.900
 zero point.

105
00:05:25.900 --> 00:05:26.700
one to five

106
00:05:29.300 --> 00:05:33.000
Okay, this is not even one. So I think the part I

107
00:05:32.100 --> 00:05:34.200
 think the property with the most.

108
00:05:36.500 --> 00:05:39.100
Number of bathrooms was something like 8 or 10.

109
00:05:39.100 --> 00:05:40.100
 I have to look at the original data.

110
00:05:40.900 --> 00:05:41.700
But you get the idea.

111
00:05:42.800 --> 00:05:43.000
Yeah.

112
00:05:46.900 --> 00:05:49.500
And suppose we have one that same property

113
00:05:49.500 --> 00:05:53.300
 had four stories. That was a very large house. So 0.35. Well,

114
00:05:52.300 --> 00:05:54.400
 that's just the one story house.

115
00:05:57.500 --> 00:06:00.300
Now we did some data cleaning before

116
00:06:00.300 --> 00:06:04.400
 we got this. So if you see here this data set is called how data

117
00:06:03.400 --> 00:06:04.400
 set.

118
00:06:05.800 --> 00:06:07.000
house data set

119
00:06:08.200 --> 00:06:09.000
scaled

120
00:06:11.500 --> 00:06:14.500
so in this notebook in this Jupiter notebook, I'm skipping

121
00:06:14.500 --> 00:06:14.700
 the

122
00:06:16.100 --> 00:06:17.300
data cleaning part

123
00:06:18.300 --> 00:06:21.200
Anyway, this is the this is an end dimensional data

124
00:06:21.200 --> 00:06:24.100
 as you can see. It's so big that it cannot even

125
00:06:24.100 --> 00:06:25.700
 fit and we have these three dots.

126
00:06:26.500 --> 00:06:26.900
I think

127
00:06:29.600 --> 00:06:32.500
there were I don't know they were

128
00:06:32.500 --> 00:06:33.500
 definitely more than 20.

129
00:06:34.400 --> 00:06:37.600
Columns this Matrix. So it's a very large Matrix.

130
00:06:38.600 --> 00:06:41.000
For this we would PC would be very helpful.

131
00:06:43.700 --> 00:06:46.300
Anyway, that's the data set. We can review with the

132
00:06:46.300 --> 00:06:46.900
 word pandas.

133
00:06:50.400 --> 00:06:50.400
and

134
00:06:53.800 --> 00:06:56.100
let me skip to discussion about labels and features because

135
00:06:56.100 --> 00:06:57.200
 we haven't talked about yet.

136
00:06:57.800 --> 00:07:00.100
And let me skip the train test split part. Let me show you how

137
00:07:00.100 --> 00:07:02.100
 to create a sequence of neural networks.

138
00:07:03.500 --> 00:07:06.200
So here what we do is we have the sequential class and we

139
00:07:06.200 --> 00:07:09.000
 know classes because classes begin with

140
00:07:09.300 --> 00:07:09.800
 an uppercase letter.

141
00:07:10.400 --> 00:07:13.900
Anytime you see some function or a

142
00:07:13.900 --> 00:07:16.500
 variable spelling uppercase. That's a

143
00:07:16.500 --> 00:07:16.600
 class.

144
00:07:18.100 --> 00:07:21.100
Which means we can instantiate it and this will become an object

145
00:07:21.100 --> 00:07:24.200
 and what we do what we know about what we know of an object

146
00:07:24.200 --> 00:07:25.700
 is that they have attributes and properties.

147
00:07:28.200 --> 00:07:31.400
So one of the properties of the sequential object is

148
00:07:31.400 --> 00:07:31.900
 at

149
00:07:33.200 --> 00:07:35.200
So here we're adding a densely connected.

150
00:07:36.100 --> 00:07:37.000
Excuse me.

151
00:07:38.100 --> 00:07:38.800
a dense

152
00:07:40.400 --> 00:07:41.100
input layer

153
00:07:43.500 --> 00:07:43.800
we have

154
00:07:46.800 --> 00:07:48.100
63 inputs

155
00:07:51.300 --> 00:07:52.000
and 50

156
00:07:54.200 --> 00:07:57.700
outputs so if I go to that slide

157
00:07:58.500 --> 00:07:59.300
If you see here.

158
00:08:01.600 --> 00:08:02.700
We are talking about.

159
00:08:07.500 --> 00:08:08.100
this part

160
00:08:10.600 --> 00:08:13.300
so we have 63 Dimensions here. We

161
00:08:13.300 --> 00:08:17.100
 have Harmony to me here the input dimensions are what one two,

162
00:08:16.100 --> 00:08:19.600
 three, four, five six, seven eight.

163
00:08:20.400 --> 00:08:23.800
And the output dimensions are one two, three, four, five

164
00:08:23.800 --> 00:08:25.500
 six, seven eight nine.

165
00:08:26.400 --> 00:08:29.200
So input dimensions are eight output. Dimensions are 9.

166
00:08:30.600 --> 00:08:34.000
Now there is by the way, there is no precise formula

167
00:08:33.200 --> 00:08:36.200
 for deciding the number of output dimensions.

168
00:08:38.800 --> 00:08:42.000
And there are some experimentation involved.

169
00:08:42.700 --> 00:08:45.500
Tensorflow has something called the tensor playground,

170
00:08:45.500 --> 00:08:48.500
 which is a very nice visual way for

171
00:08:48.500 --> 00:08:51.400
 you to see how neural networks work. But again

172
00:08:51.400 --> 00:08:54.700
 because we don't have much time. I'm going to leave that for later session anyway.

173
00:08:55.700 --> 00:08:56.500
input dimensions

174
00:08:57.200 --> 00:09:00.700
I'll put Dimension. So we we actually create the first hidden

175
00:09:00.700 --> 00:09:02.300
 layer and the first input layer in tensorflow.

176
00:09:03.400 --> 00:09:05.300
In one goal. Okay, so here

177
00:09:10.300 --> 00:09:13.100
This is the first input layer and the head layer in one go.

178
00:09:13.100 --> 00:09:15.800
 So the input Dimensions the number of incoming.

179
00:09:16.700 --> 00:09:19.500
features and the number of perceptions

180
00:09:23.700 --> 00:09:26.300
Activation function like cardio. I'm really sorry about this. I don't mind.

181
00:09:26.300 --> 00:09:29.300
 It's very sensitive activation function. Like I told you is how you

182
00:09:29.300 --> 00:09:31.400
 process the signal.

183
00:09:34.500 --> 00:09:36.100
the value the rectified

184
00:09:40.100 --> 00:09:41.600
linear something. I forgot the name.

185
00:09:42.600 --> 00:09:43.800
is used

186
00:09:44.700 --> 00:09:45.600
is actually a very

187
00:09:48.300 --> 00:09:50.000
effective activation function

188
00:09:51.600 --> 00:09:54.000
But in order for me to show you why it's affected by we have to

189
00:09:54.100 --> 00:09:55.200
 leave that for a separate session.

190
00:09:56.300 --> 00:09:59.300
Well, let me just run this as quickly as I can. So the

191
00:09:59.300 --> 00:09:59.600
 first

192
00:10:02.600 --> 00:10:03.400
input slash hidden layer

193
00:10:04.300 --> 00:10:05.500
And then the other hidden layers.

194
00:10:06.600 --> 00:10:08.600
So this one has this one will receive.

195
00:10:10.500 --> 00:10:13.300
A 50 inputs from the previous layer and

196
00:10:13.300 --> 00:10:14.600
 output 100 layers.

197
00:10:15.500 --> 00:10:18.000
Why 100 because we want to learn as many ways as we can.

198
00:10:18.900 --> 00:10:21.200
And then another hidden layer and then file the

199
00:10:21.200 --> 00:10:23.500
 upper layer and here we can see we've set the output to one.

200
00:10:24.400 --> 00:10:27.300
Why one well because we want a single value we

201
00:10:27.300 --> 00:10:28.200
 want a single.

202
00:10:28.800 --> 00:10:29.600
numeric

203
00:10:30.200 --> 00:10:30.900
estimate

204
00:10:31.500 --> 00:10:32.900
for any given property

205
00:10:35.100 --> 00:10:39.400
you can see here. We once we have created a

206
00:10:38.400 --> 00:10:41.400
 sequence of layers. We will then compile it.

207
00:10:43.700 --> 00:10:44.300
then we have

208
00:10:45.300 --> 00:10:48.300
A loss function again, the loss function is how

209
00:10:48.300 --> 00:10:51.300
 we evaluate the prediction quality of this model.

210
00:10:51.300 --> 00:10:54.200
 We said that we want the MSU the converge to zero.

211
00:10:55.300 --> 00:10:56.200
So if you see here.

212
00:10:57.300 --> 00:10:58.200
When we train it.

213
00:10:59.400 --> 00:11:03.000
It's a scientific notation. It says 0.00.

214
00:11:04.400 --> 00:11:07.100
0 1 7 7 160

215
00:11:08.200 --> 00:11:09.600
and as

216
00:11:10.900 --> 00:11:11.900
it keeps on learning.

217
00:11:13.500 --> 00:11:16.100
you can see the MSC starts to

218
00:11:16.100 --> 00:11:18.200
 grow smaller so you can see this and went from

219
00:11:18.800 --> 00:11:21.500
E minus zero fourth e minus zero

220
00:11:21.500 --> 00:11:23.400
 five, so that's one more zero next to it.

221
00:11:24.100 --> 00:11:27.000
so it's even closer and closer to zero every

222
00:11:29.200 --> 00:11:32.700
Progression of time sometimes it will go up a little bit. But ultimately.

223
00:11:34.100 --> 00:11:35.600
It should converge to zero.

224
00:11:36.800 --> 00:11:39.400
If it does not conversion zero and it's instead it

225
00:11:39.400 --> 00:11:43.000
 diverges. Well, then there was something wrong with the code

226
00:11:42.100 --> 00:11:43.200
 you've done.

227
00:11:46.100 --> 00:11:46.400
Okay.

228
00:11:47.800 --> 00:11:48.100
anyway

229
00:11:50.100 --> 00:11:53.000
This is how we this is one metric by what you

230
00:11:53.300 --> 00:11:56.800
 will determine the quality of the model. That's MSC. And then

231
00:11:56.800 --> 00:11:59.400
 there's something called the optimizer algorithm the atom

232
00:11:59.400 --> 00:12:02.100
 algorithm again. I'm not gonna talk about it here. It's in that video. I told you

233
00:12:02.100 --> 00:12:04.200
 about it has to do with gradient descent.

234
00:12:05.100 --> 00:12:07.900
Let me scroll down some more.

235
00:12:09.300 --> 00:12:12.100
So after we evaluate the metrics

236
00:12:12.100 --> 00:12:12.700
 for the data.

237
00:12:13.600 --> 00:12:16.500
We can see for the training data set. It is 0.0.

238
00:12:17.200 --> 00:12:17.600
well

239
00:12:18.500 --> 00:12:19.700
That's 5073.

240
00:12:20.400 --> 00:12:23.000
So it's very close to zero. This is a good this is a good model.

241
00:12:24.700 --> 00:12:26.700
And for test that it's even smaller still.

242
00:12:30.800 --> 00:12:32.300
For tests, it's even smaller still.

243
00:12:33.300 --> 00:12:36.600
Have you as anyone heard of overtrained overfitting underfitting?

244
00:12:43.500 --> 00:12:46.400
anybody else if you've not heard of overfitting an

245
00:12:46.400 --> 00:12:47.900
 underfitting type and in the chip

246
00:12:53.800 --> 00:12:56.300
now very quickly. The reason we use training test

247
00:12:56.300 --> 00:13:00.000
 is to quickly identify over affirming

248
00:12:59.000 --> 00:13:00.400
 underfitting.

249
00:13:02.200 --> 00:13:05.400
I reason to underfitting is when the model has not learned

250
00:13:05.400 --> 00:13:06.200
 anything in particular.

251
00:13:07.600 --> 00:13:10.400
And you can detect underfitting at the training

252
00:13:10.400 --> 00:13:12.300
 stage. So if the number is high.

253
00:13:14.900 --> 00:13:18.200
You don't even to proceed the test dataset.

254
00:13:17.200 --> 00:13:19.300
 It's already bad.

255
00:13:20.400 --> 00:13:23.700
The way you will detect a overfitting is where you

256
00:13:23.700 --> 00:13:24.400
 have a good.

257
00:13:27.700 --> 00:13:28.500
training metric

258
00:13:29.500 --> 00:13:31.500
But then for the test metric it does worse.

259
00:13:32.900 --> 00:13:35.300
So if your training metric is here, which

260
00:13:35.300 --> 00:13:38.500
 is that's a good this is good and then it goes up that's overfitting.

261
00:13:39.600 --> 00:13:41.400
If it's high to start with that's underfitting.

262
00:13:42.100 --> 00:13:44.000
So that's why we need to look at training test.

263
00:13:46.600 --> 00:13:50.500
And then once we've done this we have created for ourselves a machine

264
00:13:49.500 --> 00:13:51.500
 learning model looking pretty.

265
00:13:53.200 --> 00:13:54.800
You can see here using the predict method.

266
00:13:59.900 --> 00:14:01.100
And then this is the final price.

267
00:14:02.800 --> 00:14:04.100
of one particular property

268
00:14:08.600 --> 00:14:11.300
So what we're

269
00:14:11.300 --> 00:14:14.300
 going to do is I'm going to post this notebook with

270
00:14:14.300 --> 00:14:16.200
 some additional comments in the dashboard.

271
00:14:17.800 --> 00:14:21.000
Before we get into our next session. I strongly recommend.

272
00:14:21.800 --> 00:14:23.000
that if you're not familiar with

273
00:14:24.100 --> 00:14:26.100
test dataset train dataset

274
00:14:26.700 --> 00:14:29.200
machine learning metrics and the grading this in

275
00:14:29.200 --> 00:14:30.200
 a particular that you watch that video.

276
00:14:32.100 --> 00:14:34.700
The video is called mathematics for machine learning you may have.

277
00:14:35.300 --> 00:14:38.500
Attended the event and there are also timestamps so

278
00:14:38.500 --> 00:14:40.400
 you can jump to the section that is relevant to you.
