WEBVTT - This file was automatically generated by VIMEO

0
00:00:00.200 --> 00:00:01.200
so today which is the

1
00:00:03.500 --> 00:00:04.400
18th of July

2
00:00:12.800 --> 00:00:13.900
we are going to

3
00:00:16.800 --> 00:00:19.000
focus our attention to neural networks

4
00:00:21.200 --> 00:00:22.300
and their applications

5
00:00:27.200 --> 00:00:28.300
now there's one component.

6
00:00:31.100 --> 00:00:34.500
In a neural network called a convolutional layer and what

7
00:00:34.500 --> 00:00:35.600
 kind of Illusion is a certain?

8
00:00:36.600 --> 00:00:37.800
Matrix operation

9
00:00:39.800 --> 00:00:40.200
and this

10
00:00:41.500 --> 00:00:44.300
convolution operation is key to

11
00:00:45.300 --> 00:00:48.400
all the applications of neural networks.

12
00:00:48.400 --> 00:00:49.100
 So if we want to

13
00:00:51.400 --> 00:00:52.200
you know detect

14
00:00:53.800 --> 00:00:54.600
a character

15
00:00:55.900 --> 00:00:56.700
in a picture

16
00:00:57.600 --> 00:01:00.600
I can Optical calculate recognition convolution plays

17
00:01:00.600 --> 00:01:00.900
 a key part.

18
00:01:02.200 --> 00:01:06.500
if we want to find out why certain factors

19
00:01:05.500 --> 00:01:08.600
 cause the price

20
00:01:08.600 --> 00:01:10.600
 of a realistic property increase or decrease

21
00:01:11.300 --> 00:01:13.800
again convolution plays an important part

22
00:01:18.900 --> 00:01:21.000
So that's a regression example, I gave you

23
00:01:21.400 --> 00:01:23.300
 a classification example.

24
00:01:28.100 --> 00:01:28.400
and

25
00:01:29.900 --> 00:01:32.500
another important component of a new network is the

26
00:01:32.500 --> 00:01:35.400
 linear transformation now to really understand what

27
00:01:37.300 --> 00:01:39.700
You know what in your network?

28
00:01:41.100 --> 00:01:41.700
is made of

29
00:01:43.300 --> 00:01:45.000
And how it works and why it's so effective.

30
00:01:46.900 --> 00:01:49.500
We need to look at neural Xbox from two perspectives.

31
00:01:50.500 --> 00:01:52.200
one will be a

32
00:01:55.100 --> 00:01:57.400
pictorial representation of a neural network

33
00:01:59.300 --> 00:02:02.200
And then there is the Matrix representation of the neural network. So

34
00:02:02.200 --> 00:02:03.700
 neural networks are Matrix operations.

35
00:02:04.900 --> 00:02:08.000
But to it really does help to see these

36
00:02:07.600 --> 00:02:10.900
 operations at the high level and pictureally

37
00:02:10.900 --> 00:02:13.200
 at least in the form of graphs and then the

38
00:02:13.200 --> 00:02:13.800
 form of diagrams.

39
00:02:18.700 --> 00:02:21.000
Added a process. We are going to

40
00:02:21.400 --> 00:02:22.500
 also use some.

41
00:02:24.200 --> 00:02:24.500
data

42
00:02:25.500 --> 00:02:29.600
So if you want to predict something on data if you want to classify an

43
00:02:29.600 --> 00:02:29.700
 image.

44
00:02:31.500 --> 00:02:34.200
Data will also need to have access to

45
00:02:34.200 --> 00:02:34.800
 that data.

46
00:02:36.300 --> 00:02:39.600
And the best way to read this data will be using the library pandas.

47
00:02:40.300 --> 00:02:41.500
Okay, so you

48
00:02:42.600 --> 00:02:45.700
how many of you here have any familiarity with the pandas Library?

49
00:02:46.900 --> 00:02:49.300
If you are familiarity with the Panthers Library type and

50
00:02:49.300 --> 00:02:50.300
 why otherwise type in it.

51
00:02:51.700 --> 00:02:53.600
Even if the answer is no by the way.

52
00:02:55.900 --> 00:02:58.200
It's very simple thing to use that. Well, at least we're not

53
00:02:58.200 --> 00:03:00.900
 going to use we're not going to do anything complicated with patterns. We're just going to read.

54
00:03:02.400 --> 00:03:03.100
some tabular data

55
00:03:05.100 --> 00:03:07.500
and look at things like mean like the

56
00:03:08.500 --> 00:03:10.600
what we call the summary statistics.

57
00:03:11.300 --> 00:03:14.600
Actually, we want to see what is the mean average is what is the median? What

58
00:03:14.600 --> 00:03:17.400
 are the number number of data? We have that kind

59
00:03:17.400 --> 00:03:17.500
 of thing.

60
00:03:20.500 --> 00:03:22.200
Now we didn't actually get to cover.

61
00:03:24.200 --> 00:03:27.100
PCA we've covered the most important parts of the

62
00:03:27.100 --> 00:03:29.500
 PC. We've covered the mathematics of the PC.

63
00:03:30.100 --> 00:03:31.900
although we haven't quite

64
00:03:33.100 --> 00:03:34.100
seen its application.

65
00:03:35.300 --> 00:03:38.200
So here's what I want us to do. I want to actually start up

66
00:03:38.200 --> 00:03:38.500
 with this.

67
00:03:41.200 --> 00:03:42.300
and let me rename this to

68
00:03:45.200 --> 00:03:46.700
example of PCA

69
00:03:48.100 --> 00:03:49.800
or dimensionality reduction

70
00:03:53.200 --> 00:03:54.900
You see one of the things we will have to do.

71
00:03:58.500 --> 00:03:59.300
for our

72
00:04:03.200 --> 00:04:06.200
regression example is to reduce the dimensionality of the data.

73
00:04:07.300 --> 00:04:10.300
So there are two it's a two-step process first. We will

74
00:04:10.300 --> 00:04:12.000
 import our data with pandas.

75
00:04:13.100 --> 00:04:15.000
Which will have many columns.

76
00:04:16.100 --> 00:04:18.400
In other words, it will have large dimensionality.

77
00:04:18.900 --> 00:04:21.500
Well, then use PCA to reduce the dimensionality.

78
00:04:22.600 --> 00:04:25.300
And once we have reduced the dimensionality instead of the original

79
00:04:25.300 --> 00:04:28.200
 columns will have these things called principle components which you

80
00:04:28.200 --> 00:04:28.600
 will look at today.

81
00:04:30.200 --> 00:04:33.800
and what we've reduced them in charity we can then use neural networks

82
00:04:33.800 --> 00:04:34.000
 to

83
00:04:34.900 --> 00:04:36.300
do all sorts of things on this data.

84
00:04:37.400 --> 00:04:38.400
You'll find clusters in that data.

85
00:04:39.400 --> 00:04:41.700
To classify the label of the data.

86
00:04:42.500 --> 00:04:43.900
to perform regression on the data

87
00:04:45.300 --> 00:04:48.300
and I'm hoping that in these next three sessions will get to

88
00:04:48.300 --> 00:04:52.100
 see all three examples regression classification and clustering.

89
00:04:54.200 --> 00:04:55.500
number two

90
00:04:56.200 --> 00:05:00.000
after I've showed you a very simple example of PCA we are

91
00:04:59.300 --> 00:05:00.400
 going to

92
00:05:02.600 --> 00:05:02.900
well

93
00:05:07.500 --> 00:05:08.100
introduce ourselves to

94
00:05:11.500 --> 00:05:12.300
neural networks

95
00:05:24.100 --> 00:05:25.900
So we get we will begin the high level.

96
00:05:32.800 --> 00:05:35.100
We will talk about something called the input layer.

97
00:05:37.700 --> 00:05:38.400
the hidden layers

98
00:05:40.800 --> 00:05:42.000
and the one output layer

99
00:05:48.100 --> 00:05:48.800
we will talk about

100
00:05:51.300 --> 00:05:52.500
something called the activation

101
00:05:55.600 --> 00:05:55.900
of

102
00:05:57.200 --> 00:06:00.000
Something called a perceptron again a couple

103
00:06:00.300 --> 00:06:03.900
 of terminology here like these are these

104
00:06:03.900 --> 00:06:06.700
 are terms and deep learning now. We

105
00:06:06.700 --> 00:06:07.900
 deep learning.

106
00:06:10.900 --> 00:06:13.800
Is a technique that uses neural networks?

107
00:06:13.800 --> 00:06:16.700
 So deep learning is a technique that uses matrices put

108
00:06:16.700 --> 00:06:18.000
 in some special arrangement.

109
00:06:19.200 --> 00:06:23.300
And these are the components of a neural network. And this is the most Atomic component

110
00:06:22.300 --> 00:06:23.900
 of a neural network.

111
00:06:25.100 --> 00:06:26.100
And we'll see what this is.

112
00:06:28.600 --> 00:06:31.200
And hopefully we'll be able to cover this fast enough so that

113
00:06:31.200 --> 00:06:34.600
 in our next session we can actually use these two

114
00:06:34.600 --> 00:06:34.900
 together.

115
00:06:36.300 --> 00:06:37.200
We can create a machine learning.

116
00:06:38.000 --> 00:06:38.600
pipeline

117
00:06:40.200 --> 00:06:40.500
let's begin.

118
00:06:45.800 --> 00:06:47.900
I'm going to switch over to colab.

119
00:06:53.600 --> 00:06:54.000
whoops

120
00:07:07.900 --> 00:07:09.400
Pi tutorial number 17

121
00:07:29.100 --> 00:07:31.800
I will import numpy.

122
00:07:35.700 --> 00:07:37.000
as MP and

123
00:07:39.100 --> 00:07:40.200
Matlab and

124
00:07:42.300 --> 00:07:42.900
math

125
00:07:43.800 --> 00:07:44.900
lot lib

126
00:07:50.700 --> 00:07:53.000
so we can see what principal components are.

127
00:07:53.800 --> 00:07:55.000
And then we can visualize them.

128
00:08:00.500 --> 00:08:01.800
import math, so it's

129
00:08:07.000 --> 00:08:07.000
ation

130
00:08:10.200 --> 00:08:12.100
What I'm going to show you is a matrix.

131
00:08:18.400 --> 00:08:19.500
There is two by six.

132
00:08:23.100 --> 00:08:26.500
So it has two columns and six six rows.

133
00:08:26.500 --> 00:08:30.100
 This is a very simple Matrix and it's just and it's

134
00:08:29.100 --> 00:08:30.800
 small enough so that we can

135
00:08:33.900 --> 00:08:36.100
You know get the get the point of PCA and then

136
00:08:36.100 --> 00:08:36.600
 of course.

137
00:08:38.200 --> 00:08:41.200
In the next example, we will look at a large.

138
00:08:42.300 --> 00:08:44.600
Dataset so this one only has two dimensions.

139
00:08:45.400 --> 00:08:49.300
Ultimately, what we want to do is we want to project this two-dimensional.

140
00:08:50.300 --> 00:08:53.600
Data onto one. I want dimensional into one

141
00:08:53.600 --> 00:08:55.100
 dimension. So we want to go from

142
00:08:56.800 --> 00:08:59.200
essentially two vectors to one vector. Let me

143
00:08:59.200 --> 00:08:59.900
 explain what I mean.

144
00:09:01.400 --> 00:09:03.200
So we are going to perform SVD on this.

145
00:09:04.100 --> 00:09:07.500
Okay, and remember we said that PC is a special case of SVD.

146
00:09:09.500 --> 00:09:11.000
To remind you what SVD is?

147
00:09:12.700 --> 00:09:15.000
Let me bring you back to the tutorial from our last session.

148
00:09:19.400 --> 00:09:21.400
suppose we have the Matrix a

149
00:09:23.600 --> 00:09:24.400
what we will do.

150
00:09:26.300 --> 00:09:29.000
Is factorize this Matrix again SVD is

151
00:09:29.100 --> 00:09:30.500
 a matrix factorization method.

152
00:09:31.900 --> 00:09:34.500
Well factored, it is three matrices USV.

153
00:09:37.200 --> 00:09:38.800
before I tell you what u and v are

154
00:09:40.300 --> 00:09:43.300
S is the Matrix that contains the

155
00:09:43.300 --> 00:09:44.300
 singular values?

156
00:09:48.100 --> 00:09:51.200
And the context of PC we call these singular values their principle

157
00:09:51.200 --> 00:09:51.700
 components.

158
00:09:53.900 --> 00:09:55.300
when we multiply

159
00:09:56.500 --> 00:09:57.700
the singular values

160
00:09:58.300 --> 00:10:01.600
In PC, which are called principle components when we

161
00:10:01.600 --> 00:10:05.000
 come when we multiply by that we need to argue values. Okay, so singular values

162
00:10:06.200 --> 00:10:07.400
are eigenvalues

163
00:10:08.200 --> 00:10:11.400
and these eigenvalues in principle component large are called principal

164
00:10:11.400 --> 00:10:11.700
 components.

165
00:10:12.400 --> 00:10:13.400
So these are eigenvalues.

166
00:10:16.900 --> 00:10:17.700
when we multiply

167
00:10:22.100 --> 00:10:25.000
in PCA when we multiply one of these eigenvalues by

168
00:10:26.300 --> 00:10:28.800
The Matrix of the left

169
00:10:30.400 --> 00:10:33.700
Or I should say the column vectors

170
00:10:33.700 --> 00:10:35.400
 in The Matrix Matrix on the left. We will get

171
00:10:39.500 --> 00:10:41.600
these vectors that will show us

172
00:10:42.400 --> 00:10:43.700
How the data has been reduced?

173
00:10:44.500 --> 00:10:47.800
But let me just quickly tell you how what the

174
00:10:47.800 --> 00:10:51.000
 factorization how the factorizations perform. So

175
00:10:50.400 --> 00:10:52.000
 in order for us to get to you.

176
00:10:54.700 --> 00:10:57.600
We multiply a by itself but we

177
00:10:57.600 --> 00:11:00.100
 actually transpose it first. So we take the

178
00:11:00.100 --> 00:11:02.100
 transpose of Matrix a multiplied by itself.

179
00:11:03.400 --> 00:11:06.400
To get Ada. I will call at a

180
00:11:06.400 --> 00:11:07.600
 the transpose of

181
00:11:08.500 --> 00:11:09.500
a times a

182
00:11:11.800 --> 00:11:14.200
then we will do it the other way round. So we'll multiply

183
00:11:14.200 --> 00:11:17.200
 a times the transpose of a we'll get a second Matrix.

184
00:11:19.700 --> 00:11:22.100
in order to get to matrices u and

185
00:11:22.100 --> 00:11:23.000
 v transpose

186
00:11:24.300 --> 00:11:27.300
we have to orthonormalize these matrices.

187
00:11:28.800 --> 00:11:31.800
And we talked about what it takes to orthonormalize these two

188
00:11:31.800 --> 00:11:32.100
 matrices.

189
00:11:35.900 --> 00:11:38.000
It ultimately comes down to a technique called

190
00:11:38.800 --> 00:11:41.300
The gram Schmidt algorithm we talked

191
00:11:41.300 --> 00:11:44.900
 about this in the previous session if you watched if you weren't here yes before

192
00:11:44.900 --> 00:11:47.100
 yesterday you watch the recording you.

193
00:11:48.400 --> 00:11:49.800
The grams she was process.

194
00:11:53.200 --> 00:11:54.400
the gram Schmidt algorithm

195
00:11:54.900 --> 00:11:56.400
will orthogonalize

196
00:11:57.900 --> 00:11:59.800
The Matrix and also normalize it

197
00:12:00.800 --> 00:12:02.500
again to normalize a vector

198
00:12:04.200 --> 00:12:04.700
is to

199
00:12:07.600 --> 00:12:10.400
divide that back divide the components of

200
00:12:10.400 --> 00:12:12.100
 that vector by the norm of that vector.

201
00:12:12.900 --> 00:12:13.400
basically

202
00:12:15.300 --> 00:12:16.600
the components will

203
00:12:17.300 --> 00:12:19.300
all have values between 0 and 1

204
00:12:20.200 --> 00:12:23.800
That's what normalization is and that's why I'm I made

205
00:12:23.800 --> 00:12:26.200
 an orthogonal Matrix is called orthonormal because it's

206
00:12:26.200 --> 00:12:28.200
 an orthogonal Matrix with components have been.

207
00:12:28.700 --> 00:12:29.500
normalized

208
00:12:34.400 --> 00:12:35.800
and of course in order to find the

209
00:12:38.200 --> 00:12:42.200
singular values aka the principal components aka

210
00:12:41.200 --> 00:12:46.100
 the eigenvalues we perform.

211
00:12:49.800 --> 00:12:51.800
You know, I get the composition like we saw.

212
00:12:55.100 --> 00:12:57.000
in the session before the last

213
00:13:02.900 --> 00:13:03.100
okay.

214
00:13:05.200 --> 00:13:08.200
Where we I mean, I don't have it here. But I hope you

215
00:13:08.200 --> 00:13:11.100
 remember this this tabs for oh, you know

216
00:13:11.100 --> 00:13:12.800
 decomposing a matrix.

217
00:13:14.500 --> 00:13:16.400
So we get 14.14 from

218
00:13:17.400 --> 00:13:20.500
Well, we can get this we can both we can get both eigenvalues from

219
00:13:20.500 --> 00:13:23.200
 decomposing. Excuse me. I can decomposition of both

220
00:13:23.200 --> 00:13:26.500
 matrices u and v decomposition of both matrices will

221
00:13:26.500 --> 00:13:27.000
 reveal both.

222
00:13:27.700 --> 00:13:29.400
of these eigenvalues

223
00:13:31.800 --> 00:13:34.800
All right. Let's return here. Now. We're

224
00:13:34.800 --> 00:13:37.600
 not going to do this. We're not going to use grab Schmidt

225
00:13:37.600 --> 00:13:40.300
 and normalization. We're just going to use a

226
00:13:40.300 --> 00:13:41.900
 built-in method of

227
00:13:43.100 --> 00:13:46.200
numpy called SVD. So there's a method called

228
00:13:46.200 --> 00:13:46.700
 NP.

229
00:13:48.100 --> 00:13:51.500
It's a SVD in the object linear algebra.

230
00:13:57.800 --> 00:13:59.600
And its return value will be 3.

231
00:14:00.800 --> 00:14:03.400
Things and we will start well, it

232
00:14:03.400 --> 00:14:04.900
 will be two matrices and one vector.

233
00:14:05.500 --> 00:14:06.700
But you will assign to you.

234
00:14:07.900 --> 00:14:08.800
As and V.

235
00:14:11.700 --> 00:14:14.700
So the return value of this method will be a matrix Vector

236
00:14:14.700 --> 00:14:16.200
 Matrix respectively.

237
00:14:18.800 --> 00:14:21.800
and after all and that's this SVD composition

238
00:14:29.100 --> 00:14:30.400
And then I'll print all three.

239
00:14:31.700 --> 00:14:33.200
Variables so print you.

240
00:14:36.100 --> 00:14:37.000
And S and V.

241
00:14:49.600 --> 00:14:50.700
So this is the Matrix U.

242
00:14:53.100 --> 00:14:54.300
the eigenvalues

243
00:14:55.100 --> 00:14:56.400
there will be put in the

244
00:14:57.300 --> 00:14:58.000
Matrix

245
00:14:59.500 --> 00:15:01.500
SS for Sigma, okay.

246
00:15:03.600 --> 00:15:06.700
In a PCA the S is a sigma use the

247
00:15:06.700 --> 00:15:07.000
 Greek letter.

248
00:15:07.600 --> 00:15:10.300
And the Matrix V.

249
00:15:20.500 --> 00:15:20.800
All right.

250
00:15:22.400 --> 00:15:25.200
The next thing we are going to do is I'm going to

251
00:15:25.200 --> 00:15:27.000
 put these eigenvalues.

252
00:15:29.800 --> 00:15:31.200
in an identity Matrix

253
00:15:32.200 --> 00:15:34.600
So again, you see here. This is an identity Matrix.

254
00:15:36.600 --> 00:15:39.500
But instead of once we have the argument value, so I'm going to produce this

255
00:15:39.500 --> 00:15:40.100
 Square Matrix.

256
00:15:42.600 --> 00:15:43.200
and to do this

257
00:15:44.900 --> 00:15:45.200
first

258
00:15:46.900 --> 00:15:47.800
I will assign.

259
00:15:48.500 --> 00:15:51.200
lowercase s to uppercase s because the

260
00:15:51.200 --> 00:15:54.500
 convention you know, we want to follow linear algebra convention

261
00:15:54.500 --> 00:15:54.700
 where

262
00:15:55.400 --> 00:15:58.100
a we use the we use a lowercase dollars to

263
00:15:58.100 --> 00:16:01.300
 denote the vectors uppercase that is to denote matrices.

264
00:16:02.200 --> 00:16:05.100
so I'm going to change uppercase as to small as

265
00:16:06.100 --> 00:16:08.400
uppercase s instead will be a matrix.

266
00:16:19.800 --> 00:16:21.600
If you see this is initial on form.

267
00:16:23.300 --> 00:16:25.400
We have ones across the principle diagonal.

268
00:16:27.400 --> 00:16:29.400
we need this to be a six by two because

269
00:16:32.200 --> 00:16:34.200
of the Matrix operation that we're about to perform.

270
00:16:35.100 --> 00:16:36.800
But this is the first eigenvalue.

271
00:16:37.800 --> 00:16:40.800
Second of a second value or if you like the first

272
00:16:40.800 --> 00:16:43.700
 singular value in the second singular Valley. Also

273
00:16:43.700 --> 00:16:45.400
 if you like the first principle component.

274
00:16:47.100 --> 00:16:48.300
And the second principle component.

275
00:16:50.800 --> 00:16:53.800
Once I start visualizing you will see how this how we're

276
00:16:53.800 --> 00:16:53.800
 able to.

277
00:16:55.900 --> 00:16:58.700
Reduce the dimensionality of our data. Now.

278
00:16:58.700 --> 00:17:01.200
 How many imagine this were an Excel sheet. We have

279
00:17:01.200 --> 00:17:02.200
 two columns of data.

280
00:17:04.400 --> 00:17:06.600
What we're about to do this example is collapse into one column.

281
00:17:07.900 --> 00:17:10.600
So we go from a two-dimensional letter to one dimensional data.

282
00:17:10.600 --> 00:17:13.500
 That's a lead. That's the linear transformation to go

283
00:17:13.500 --> 00:17:14.500
 from R2 to R.

284
00:17:15.700 --> 00:17:16.200
R 1

285
00:17:26.900 --> 00:17:29.200
Now I also want to do this. I want

286
00:17:29.200 --> 00:17:31.800
 to plot this Vector on its own axis.

287
00:17:32.700 --> 00:17:35.200
Plot this on its own axis. So this

288
00:17:35.200 --> 00:17:36.000
 will be let's see X

289
00:17:36.800 --> 00:17:37.700
and this will be y

290
00:17:41.800 --> 00:17:42.300
so

291
00:17:43.500 --> 00:17:46.800
Here, we will say performing SVD up.

292
00:17:54.300 --> 00:17:55.200
principal component

293
00:17:59.300 --> 00:18:00.100
SVD

294
00:18:01.600 --> 00:18:04.400
first we need to decompose The Matrix into three matrices.

295
00:18:07.300 --> 00:18:08.700
Then we are going to visualize.

296
00:18:19.200 --> 00:18:19.600
the

297
00:18:22.300 --> 00:18:22.800
column vectors

298
00:18:27.900 --> 00:18:28.500
of a

299
00:18:30.300 --> 00:18:30.900
just so we can have a

300
00:18:32.100 --> 00:18:32.800
intuitive

301
00:18:34.600 --> 00:18:35.100
understanding

302
00:18:39.300 --> 00:18:42.500
Now I'm going to use a technique called a list comprehension. So

303
00:18:42.500 --> 00:18:45.500
 if you do not if you haven't seen this before or you

304
00:18:45.500 --> 00:18:46.200
 don't remember it.

305
00:18:48.900 --> 00:18:50.500
let me show you I'm going to go a

306
00:18:51.800 --> 00:18:54.200
that's the Matrix and I'm going to use the square bracket, of

307
00:18:54.200 --> 00:18:54.300
 course.

308
00:18:55.700 --> 00:18:57.100
If you plug in a number.

309
00:19:00.400 --> 00:19:03.400
And these are the indices the index the every index

310
00:19:03.400 --> 00:19:05.000
 will correspond to a particular rule.

311
00:19:05.600 --> 00:19:07.300
of a matrix if I type is 0

312
00:19:08.900 --> 00:19:11.200
this will be the first row of the

313
00:19:11.200 --> 00:19:12.400
 original Matrix.

314
00:19:15.900 --> 00:19:19.100
And of course, we remember that we count from 0 and in

315
00:19:18.100 --> 00:19:19.500
 programming.

316
00:19:22.700 --> 00:19:25.100
if I want the last row I go I go 5

317
00:19:27.700 --> 00:19:29.200
now if I want all of the rows

318
00:19:32.300 --> 00:19:33.400
What this colon means is that.

319
00:19:34.500 --> 00:19:37.300
What precedes the color is the starting range? And what

320
00:19:37.300 --> 00:19:40.900
 comes after is this ending range if I type this if I

321
00:19:40.900 --> 00:19:41.200
 leave?

322
00:19:42.200 --> 00:19:45.300
Both sides of the colon empty. That means give me all of

323
00:19:45.300 --> 00:19:45.800
 the rows.

324
00:19:46.500 --> 00:19:47.300
So here we can see.

325
00:19:48.300 --> 00:19:51.400
The first row is negative three negative 3

326
00:19:51.400 --> 00:19:54.100
 the second was negative 2 negative one. So if I just want the first

327
00:19:54.100 --> 00:19:56.300
 two rows I can go 0 1

328
00:20:00.300 --> 00:20:03.000
Or rather zero to that. It has to be stopping an X.

329
00:20:03.500 --> 00:20:03.900
There we go.

330
00:20:05.500 --> 00:20:06.500
Now what I actually want.

331
00:20:07.500 --> 00:20:08.600
Is all of the rows.

332
00:20:10.600 --> 00:20:13.100
But I want to only extract the first column.

333
00:20:14.800 --> 00:20:16.200
So I'm going to call this C1.

334
00:20:18.200 --> 00:20:21.000
and it will be negative 3 negative 2 1 negative 1 2 3

335
00:20:22.200 --> 00:20:25.700
So what I'm going to say is I want all of the rows all of

336
00:20:25.700 --> 00:20:26.100
 the rows.

337
00:20:27.200 --> 00:20:28.100
from the first column

338
00:20:30.600 --> 00:20:31.200
and that will give me

339
00:20:32.300 --> 00:20:33.900
these components

340
00:20:40.600 --> 00:20:42.300
Animal, I'll also do it for the second column.

341
00:20:43.900 --> 00:20:46.300
So all of the rows from the second column

342
00:20:46.300 --> 00:20:47.200
 again, we count from 0.

343
00:20:48.400 --> 00:20:49.200
Which is why this is one.

344
00:20:52.200 --> 00:20:54.000
Now I'm going to I will plot these two.

345
00:20:55.300 --> 00:20:58.200
Vectors so C1 and C2 are both now vectors.

346
00:21:01.800 --> 00:21:04.500
Okay, however, I'm going to treat these as

347
00:21:04.500 --> 00:21:06.200
 points on the x-axis.

348
00:21:07.700 --> 00:21:10.500
And for C2 it will be points on the y-axis. Basically, we

349
00:21:10.500 --> 00:21:11.500
 would just draw two.

350
00:21:12.900 --> 00:21:15.400
one horizontal and one

351
00:21:15.400 --> 00:21:16.300
 vertical column

352
00:21:17.200 --> 00:21:20.100
and I'll see why and I'll show you why we want to do this.

353
00:21:21.400 --> 00:21:25.800
So to plot I'm going to use the PLT method PLT Dot.

354
00:21:27.100 --> 00:21:27.700
scatter

355
00:21:33.200 --> 00:21:37.100
so I want see you want to be on the x-axis. I'm

356
00:21:36.100 --> 00:21:37.900
 going to type in C1.

357
00:21:38.900 --> 00:21:40.300
for the y-axis

358
00:21:41.100 --> 00:21:41.900
we want

359
00:21:44.900 --> 00:21:46.100
I'm going to set it to 0.

360
00:21:50.600 --> 00:21:50.800
Okay.

361
00:21:51.700 --> 00:21:54.500
6 elements corresponding with the six points in

362
00:21:54.500 --> 00:21:55.000
 that vector

363
00:21:59.400 --> 00:22:02.700
and then I should have will see as plts.

364
00:22:09.800 --> 00:22:11.700
Yeah, thank you, Ron. I fixed that.

365
00:22:15.700 --> 00:22:18.300
Now I also want to color code these points.

366
00:22:19.100 --> 00:22:22.300
So this will be zero negative 3, excuse me,

367
00:22:22.300 --> 00:22:24.400
 negative 3 0 negative 2 0

368
00:22:25.200 --> 00:22:28.700
1 0 C. There's a points right? These are the points corresponding

369
00:22:28.700 --> 00:22:31.500
 to the x-axis. These are the points corresponding on the y-axis.

370
00:22:32.700 --> 00:22:35.400
I want to color code is I want to color code the X points

371
00:22:35.400 --> 00:22:35.700
 that are

372
00:22:37.600 --> 00:22:39.000
the left of the origin

373
00:22:39.600 --> 00:22:42.100
Beside it. Basically the numbers that are negative will have one

374
00:22:42.100 --> 00:22:44.200
 color the numbers that are positive will have another color.

375
00:22:46.400 --> 00:22:50.900
So I'm going to say c c is the parameter for

376
00:22:50.900 --> 00:22:51.000
 color.

377
00:22:52.700 --> 00:22:54.600
And the criteria for shooting the color.

378
00:22:56.300 --> 00:22:56.700
is

379
00:22:59.200 --> 00:23:01.000
C1 greater than 0

380
00:23:02.100 --> 00:23:04.500
so if the component has C1 or above 0

381
00:23:06.100 --> 00:23:09.000
we will get one color if they are below zero will get another color.

382
00:23:10.500 --> 00:23:12.800
and we will and let me just set the

383
00:23:14.500 --> 00:23:15.800
the no never mind.

384
00:23:16.400 --> 00:23:18.500
It's going to change Dimensions. Let me see what this looks like.

385
00:23:23.400 --> 00:23:24.200
Okay. There we go.

386
00:23:25.800 --> 00:23:29.400
and I'll do the same for the second column

387
00:23:29.400 --> 00:23:30.300
 of Matrix a

388
00:23:31.900 --> 00:23:34.500
maybe what I should have done is typed in make this a1a2.

389
00:23:35.400 --> 00:23:36.300
Make it clear.

390
00:23:37.400 --> 00:23:38.300
Let me run this again.

391
00:23:45.600 --> 00:23:46.900
and then a two

392
00:23:55.900 --> 00:23:58.500
Let's change the order of these arguments.

393
00:24:05.700 --> 00:24:07.300
So the idea is this.

394
00:24:12.400 --> 00:24:13.700
We have two dimensions of data.

395
00:24:14.700 --> 00:24:15.500
these two vectors

396
00:24:18.200 --> 00:24:21.300
and what we want to do is retain this

397
00:24:21.300 --> 00:24:22.400
 spread.

398
00:24:24.300 --> 00:24:24.900
this variation

399
00:24:26.400 --> 00:24:28.400
but in one axis in one vector

400
00:24:30.500 --> 00:24:32.100
So when it retains this value.

401
00:24:34.100 --> 00:24:34.300
these

402
00:24:35.800 --> 00:24:38.500
this spread in one diagonal

403
00:24:40.100 --> 00:24:41.900
So here this is one dimension of our data.

404
00:24:42.600 --> 00:24:44.000
the second dimension of our data

405
00:24:44.800 --> 00:24:46.100
however, we want to do is use the

406
00:24:46.800 --> 00:24:47.800
principal components

407
00:24:48.500 --> 00:24:49.500
Which are the eigenvalues?

408
00:24:50.700 --> 00:24:54.200
To collapse this into one access without

409
00:24:53.200 --> 00:24:56.100
 mmm while keeping.

410
00:24:56.700 --> 00:24:59.000
This spread as much as possible.

411
00:25:02.200 --> 00:25:03.200
So here's what I'll do next.

412
00:25:08.300 --> 00:25:09.100
I am going to take

413
00:25:24.400 --> 00:25:26.100
the first principle component

414
00:25:27.400 --> 00:25:28.400
multiplied by

415
00:25:30.900 --> 00:25:32.900
the first principle Direction

416
00:25:33.600 --> 00:25:34.500
Let me open this again.

417
00:25:36.800 --> 00:25:37.200
now you

418
00:25:44.100 --> 00:25:46.200
I know it has it's quite

419
00:25:49.300 --> 00:25:50.100
maybe messy.

420
00:25:51.200 --> 00:25:54.500
But the First Column Vector in U is the first principle Direction.

421
00:25:56.200 --> 00:25:57.600
What I'm good at you is I'm going to multiply.

422
00:25:59.700 --> 00:26:02.700
the first principle component by the

423
00:26:02.700 --> 00:26:04.700
 principal direction of the Matrix U

424
00:26:05.400 --> 00:26:08.300
Now the principal component is also.

425
00:26:11.200 --> 00:26:12.800
the first principle component

426
00:26:13.600 --> 00:26:16.600
is a component that has that has the largest values if

427
00:26:16.600 --> 00:26:17.300
 I come back here.

428
00:26:18.300 --> 00:26:21.100
If we look at this example, this was a completely different

429
00:26:21.100 --> 00:26:21.500
 Matrix.

430
00:26:22.600 --> 00:26:25.700
The largest singular value or large eigenvalue. This

431
00:26:25.700 --> 00:26:26.500
 Matrix is the number 14.

432
00:26:29.700 --> 00:26:32.100
This number also corresponds in the

433
00:26:32.100 --> 00:26:33.300
 context of PCA.

434
00:26:33.900 --> 00:26:34.400
to the

435
00:26:40.100 --> 00:26:41.400
variance and the data

436
00:26:42.200 --> 00:26:43.300
so the first argument value

437
00:26:45.600 --> 00:26:48.800
Is the first principle component and also the

438
00:26:48.800 --> 00:26:52.000
 represents the largest variance in

439
00:26:51.200 --> 00:26:52.200
 the data?

440
00:26:53.100 --> 00:26:56.100
Now a note instead why we need to keep the variance of the data will

441
00:26:56.100 --> 00:26:57.200
 have to look at a second example.

442
00:26:57.900 --> 00:26:58.300
so first

443
00:26:59.300 --> 00:27:03.100
We will focus on visualizing understanding dimensionality

444
00:27:02.100 --> 00:27:03.500
 reduction.

445
00:27:04.600 --> 00:27:07.300
And the later on we'll look another example where we have a larger

446
00:27:07.300 --> 00:27:07.800
 data set.

447
00:27:08.300 --> 00:27:11.500
And a covariance matrix is involved this example since it's

448
00:27:11.500 --> 00:27:12.600
 only two columns is too small.

449
00:27:13.400 --> 00:27:13.700
Okay.

450
00:27:14.800 --> 00:27:15.100
so let's

451
00:27:16.900 --> 00:27:19.300
See the principle. Excuse me.

452
00:27:19.300 --> 00:27:19.700
 Let's see the

453
00:27:21.300 --> 00:27:23.000
dimensionality reduction first and then we'll

454
00:27:25.100 --> 00:27:27.100
talk about covaries and correlation matrices.

455
00:27:29.200 --> 00:27:29.700
Okay.

456
00:27:34.500 --> 00:27:37.400
in order to visualize our first

457
00:27:37.400 --> 00:27:38.800
 principle component we will take

458
00:27:46.700 --> 00:27:48.200
s if you remember s was

459
00:27:51.100 --> 00:27:54.600
The okay. Why did I not run the celebrity cell

460
00:27:54.600 --> 00:27:54.700
 again?

461
00:27:58.900 --> 00:28:00.100
No, I have to run it from the top.

462
00:28:00.900 --> 00:28:01.300
Oh.

463
00:28:05.300 --> 00:28:06.300
yeah, so as

464
00:28:07.500 --> 00:28:08.800
Is the eigenvector?

465
00:28:11.600 --> 00:28:14.400
And this is the largest principle component, which will

466
00:28:14.400 --> 00:28:15.500
 be the first person component.

467
00:28:16.200 --> 00:28:19.400
so in order to get the first principle compound, we will use index 0

468
00:28:20.300 --> 00:28:21.800
We will then multiply this by.

469
00:28:24.700 --> 00:28:27.400
The First Column of the Matrix U. So

470
00:28:27.400 --> 00:28:30.100
 again if we return to here to this

471
00:28:33.500 --> 00:28:34.000
writing

472
00:28:35.200 --> 00:28:37.500
If this is the Matrix U we're going to multiply this.

473
00:28:38.700 --> 00:28:41.800
principle component by the first principle Direction

474
00:28:45.200 --> 00:28:46.700
So I'm going to multiply this by you.

475
00:28:48.200 --> 00:28:49.600
I want all of the rows.

476
00:28:50.500 --> 00:28:50.700
from

477
00:28:51.700 --> 00:28:52.300
the first column

478
00:28:57.100 --> 00:28:57.700
I run this.

479
00:28:59.300 --> 00:29:00.600
I now have a linear transformation.

480
00:29:01.600 --> 00:29:03.000
We've gone from Matrix 2.

481
00:29:03.900 --> 00:29:04.500
a vector

482
00:29:08.800 --> 00:29:11.600
So this is the vector corresponding to

483
00:29:11.600 --> 00:29:12.800
 the first principle component.

484
00:29:15.500 --> 00:29:19.400
I plot this now if I go PLT I

485
00:29:18.400 --> 00:29:20.300
 have to code here by the way.

486
00:29:31.200 --> 00:29:31.900
plot

487
00:29:35.300 --> 00:29:36.400
principal component

488
00:29:41.700 --> 00:29:41.900
one

489
00:29:46.500 --> 00:29:49.100
So I'm going to pass this twice.

490
00:29:49.900 --> 00:29:52.100
And let's just call this pc14 by the

491
00:29:52.100 --> 00:29:52.100
 way.

492
00:29:53.600 --> 00:29:54.700
pc1

493
00:29:57.900 --> 00:30:00.300
so the points on the both X and Y axis will

494
00:30:00.300 --> 00:30:00.800
 be pc1.

495
00:30:07.100 --> 00:30:09.000
It's a line that will go through the origin.

496
00:30:11.700 --> 00:30:12.700
And it will be linear.

497
00:30:15.100 --> 00:30:18.300
Where piece you want the components and pc1 are greater than

498
00:30:18.300 --> 00:30:18.500
 zero.

499
00:30:21.300 --> 00:30:24.200
I'm setting the X and Y limits to negative 5 to 5 for

500
00:30:24.200 --> 00:30:27.700
 both the X and Y axis because well the components of

501
00:30:27.700 --> 00:30:28.800
 the principal components.

502
00:30:29.600 --> 00:30:32.800
Grow from negative 4 to positive 4 so we

503
00:30:32.800 --> 00:30:34.200
 just want to make room two so we can see it.

504
00:30:39.200 --> 00:30:40.000
This is the result.

505
00:30:41.900 --> 00:30:43.100
of PCA

506
00:30:44.700 --> 00:30:46.300
we have taken those two dimensions.

507
00:30:48.300 --> 00:30:49.800
Which you can see we're quite spread.

508
00:30:53.100 --> 00:30:55.000
And we have combined both dimensions.

509
00:30:55.700 --> 00:30:58.500
We have combined both vectors into one

510
00:30:58.500 --> 00:30:58.700
 vector.

511
00:30:59.700 --> 00:31:02.600
while keeping the variance while retaining

512
00:31:04.400 --> 00:31:06.000
a significant amount of the data

513
00:31:10.900 --> 00:31:13.000
if we do the same thing for the second principle component.

514
00:31:15.500 --> 00:31:18.200
You will see that it retains data, but not

515
00:31:18.200 --> 00:31:18.800
 as much as the first one.

516
00:31:23.800 --> 00:31:24.700
This is amazing because

517
00:31:28.900 --> 00:31:31.200
if there was any correlation if there's

518
00:31:31.200 --> 00:31:34.400
 any significance any significant relationship between the

519
00:31:34.400 --> 00:31:34.900
 two first

520
00:31:36.800 --> 00:31:37.400
a columns

521
00:31:40.700 --> 00:31:44.100
Well now they are let's say merged or

522
00:31:43.100 --> 00:31:45.200
 combined it to one.

523
00:31:46.300 --> 00:31:46.700
into one

524
00:31:48.200 --> 00:31:51.400
Dimension so we've gone from two Dimensions to one dimension.

525
00:31:53.400 --> 00:31:56.700
This will become incredibly powerful, especially when we have a larger data

526
00:31:56.700 --> 00:31:56.800
 sets.

527
00:31:57.900 --> 00:32:01.000
We go, for example, 20 Dimensions or 15

528
00:32:00.600 --> 00:32:03.200
 Dimensions down to one or two or

529
00:32:03.200 --> 00:32:05.900
 three principle components making the job of the machine learning model.

530
00:32:06.700 --> 00:32:07.500
a lot easier

531
00:32:10.400 --> 00:32:13.800
now there's more to our discussions. So we haven't even gotten into

532
00:32:13.800 --> 00:32:14.500
 discussion of

533
00:32:16.100 --> 00:32:17.300
The covariance Matrix yet

534
00:32:18.100 --> 00:32:20.200
But we'll do that. Let's actually leave this here.

535
00:32:21.100 --> 00:32:25.500
For later reference. Hopefully you've understood what we mean by dimensionality

536
00:32:24.500 --> 00:32:25.800
 reduction.

537
00:32:27.200 --> 00:32:30.500
And let's switch to another example.

538
00:32:34.600 --> 00:32:37.300
So we can better appreciate principal components. Actually.

539
00:32:37.300 --> 00:32:39.300
 Let me see. What was the agendas our applied was to.

540
00:32:40.300 --> 00:32:43.000
Talk about neural networks. Okay, let's do this. First thing that we will

541
00:32:43.200 --> 00:32:43.600
 return to PC.

542
00:32:45.300 --> 00:32:48.200
Let's return to let's switch to neural networks for now and then we'll return

543
00:32:48.200 --> 00:32:48.800
 to PCA.

544
00:32:51.300 --> 00:32:51.500
Okay.

545
00:32:53.500 --> 00:32:56.000
I need to pull up another set of slides.

546
00:33:29.800 --> 00:33:32.300
This is from a different presentation. So let me jump to the

547
00:33:32.300 --> 00:33:33.000
 relevant.

548
00:33:37.900 --> 00:33:38.500
section

549
00:33:43.100 --> 00:33:44.500
neural networks

550
00:33:52.100 --> 00:33:52.400
All right.

551
00:34:11.400 --> 00:34:14.900
Anyone here have any experience working with neural

552
00:34:14.900 --> 00:34:15.100
 networks?

553
00:34:16.500 --> 00:34:19.200
by the way, this this slides are from astral Labs, which

554
00:34:19.200 --> 00:34:20.000
 is where I also teach

555
00:34:24.100 --> 00:34:25.600
One says yes, which is no.

556
00:34:26.700 --> 00:34:27.200
Anybody else?

557
00:34:32.100 --> 00:34:33.700
Or Pi torch, okay.

558
00:34:36.100 --> 00:34:37.100
now here, of course our

559
00:34:39.100 --> 00:34:40.400
Focus is not the library.

560
00:34:42.500 --> 00:34:45.500
But the the concepts of neural networks.

561
00:34:46.200 --> 00:34:49.500
So it's with neural networks that we were able to perform deep learning.

562
00:34:49.500 --> 00:34:52.800
 Yeah, let's we see why it's called Deep learning. What's why deep?

563
00:34:56.300 --> 00:34:59.500
Let's first start with the most Atomic the most basic

564
00:34:59.500 --> 00:35:01.400
 component of a new network. First of all.

565
00:35:12.500 --> 00:35:14.300
Let me take a step back. Let me take a step back.

566
00:35:15.700 --> 00:35:17.500
Let's talk about something a little more basic.

567
00:35:20.700 --> 00:35:22.500
This is a very large presentation.

568
00:35:23.500 --> 00:35:26.400
And talk about artificial intelligence. I'll make

569
00:35:26.400 --> 00:35:26.800
 this quick.

570
00:35:27.500 --> 00:35:28.600
Okay. Just want to give you some.

571
00:35:29.300 --> 00:35:29.600
mmm

572
00:35:35.100 --> 00:35:37.200
I'm gonna use two big words here etymological.

573
00:35:38.200 --> 00:35:39.500
and historical context

574
00:35:40.300 --> 00:35:41.300
for artificial intelligence

575
00:35:42.400 --> 00:35:45.200
When we say artificial intelligence, you know, what what is it that comes to mind

576
00:35:45.200 --> 00:35:46.000
 we picture.

577
00:35:46.900 --> 00:35:49.800
A situation where we have these humanoids these

578
00:35:49.800 --> 00:35:51.000
 Androids these robots.

579
00:35:51.800 --> 00:35:52.600
who are

580
00:35:54.500 --> 00:35:58.200
If not, as if not Superior to us intelligence, they

581
00:35:57.200 --> 00:36:00.300
 are equal to wasn't intelligence. Now. You've seen

582
00:36:00.300 --> 00:36:02.300
 this in many movies, you know these robots.

583
00:36:03.800 --> 00:36:05.300
They have very human-like.

584
00:36:11.600 --> 00:36:14.400
Intellectual capabilities and there are two two.

585
00:36:17.800 --> 00:36:20.100
What do we call it two?

586
00:36:25.300 --> 00:36:25.800
groups of

587
00:36:27.400 --> 00:36:27.800
thinking

588
00:36:28.600 --> 00:36:31.300
And one group we have people say who advocate for

589
00:36:31.300 --> 00:36:33.700
 developing artificial intelligence.

590
00:36:35.400 --> 00:36:38.200
Because you know we can train these robots. We can build these

591
00:36:38.200 --> 00:36:41.200
 robots who will help us, you know, improve the quality of

592
00:36:41.200 --> 00:36:41.500
 our life.

593
00:36:43.700 --> 00:36:46.700
You know to use economic terms increase improve our

594
00:36:46.700 --> 00:36:47.200
 utility.

595
00:36:48.600 --> 00:36:49.800
We can do less work.

596
00:36:51.500 --> 00:36:55.000
whether it's the Warriors work, you know working in

597
00:36:54.400 --> 00:36:56.000
 the minds or

598
00:36:57.200 --> 00:37:01.100
You know anything that is physically demanding or

599
00:37:00.100 --> 00:37:03.300
 even you know situations like this where you

600
00:37:03.300 --> 00:37:04.200
 have to do very laborious.

601
00:37:05.200 --> 00:37:05.900
technical work

602
00:37:07.300 --> 00:37:10.200
These robots can come here and do the work for us

603
00:37:10.200 --> 00:37:13.100
 or call at least at the various collaborate with us and you know, make our

604
00:37:13.100 --> 00:37:13.600
 lives easier.

605
00:37:15.800 --> 00:37:18.400
And the second group of people have this

606
00:37:18.400 --> 00:37:21.000
 perception, right if we make them too smart.

607
00:37:22.100 --> 00:37:25.400
Then it will be a matter of time before they come and take, you know,

608
00:37:25.400 --> 00:37:28.800
 come knocking on our doors and well you

609
00:37:28.800 --> 00:37:29.500
 get you get the picture.

610
00:37:30.800 --> 00:37:34.100
Process some people think that Google's Lambda neurotic has

611
00:37:34.100 --> 00:37:37.200
 become sentient. Yeah. There was this one software review engineer says,

612
00:37:37.200 --> 00:37:40.100
 I think it was as machine learning model that has become sentient.

613
00:37:41.100 --> 00:37:42.400
Let me tell you why that's not.

614
00:37:48.900 --> 00:37:51.100
Likely to be true or at least not so

615
00:37:51.100 --> 00:37:52.900
 not not just yet. Okay.

616
00:37:53.800 --> 00:37:55.500
There is a there is a one.

617
00:38:00.600 --> 00:38:01.400
Theoretical explanation why?

618
00:38:03.400 --> 00:38:06.400
We cannot produce computers they

619
00:38:06.400 --> 00:38:06.600
 can.

620
00:38:12.100 --> 00:38:12.900
extrapolate

621
00:38:14.100 --> 00:38:15.400
learn from extrapolation

622
00:38:16.400 --> 00:38:17.600
and there was also a

623
00:38:20.700 --> 00:38:23.200
practical explanation fire why it's not possible to

624
00:38:23.200 --> 00:38:25.300
 create such, you know sentient things.

625
00:38:26.400 --> 00:38:29.400
First let's talk about the three categories of

626
00:38:29.400 --> 00:38:30.400
 artificial intelligence.

627
00:38:31.700 --> 00:38:31.900
Okay.

628
00:38:32.800 --> 00:38:35.600
We have the most primitive, which is narrow AI.

629
00:38:36.400 --> 00:38:38.300
And the most Superior which is of course Superior.

630
00:38:39.400 --> 00:38:41.800
Narrow AI is really what we call machine learning.

631
00:38:43.300 --> 00:38:46.400
And another name for machine learning as you may know is statistical learning.

632
00:38:46.400 --> 00:38:49.100
 It's really just statistical techniques.

633
00:38:51.700 --> 00:38:52.100
used

634
00:38:53.300 --> 00:38:54.900
on large volumes of data

635
00:38:56.200 --> 00:38:59.900
Basically, it is a lot of the Redundant tasks that a statistics

636
00:38:59.900 --> 00:39:01.500
 would do a lot of the basic arithmetic.

637
00:39:03.300 --> 00:39:06.600
There is a decision would do and it is automated with software. That's

638
00:39:06.600 --> 00:39:10.000
 what machine learning is machine learning is in very

639
00:39:09.200 --> 00:39:11.100
 simple terms statistics.

640
00:39:12.800 --> 00:39:13.400
automated

641
00:39:14.600 --> 00:39:17.400
now obviously there's more to a status

642
00:39:17.400 --> 00:39:18.000
 job than

643
00:39:21.700 --> 00:39:23.100
calculating simple things like

644
00:39:25.800 --> 00:39:27.900
dispersion and variation

645
00:39:28.800 --> 00:39:31.200
But a lot of it is automated a

646
00:39:31.200 --> 00:39:34.700
 lot of the things that a session would do in order to perform analysis

647
00:39:34.700 --> 00:39:37.300
 is very mundane machine learning

648
00:39:37.300 --> 00:39:39.500
 automates machine learning about automating most of that.

649
00:39:41.100 --> 00:39:41.200
Okay.

650
00:39:43.300 --> 00:39:46.400
And it will only machine learning model will be useful so

651
00:39:46.400 --> 00:39:49.300
 long as there is no such as so long

652
00:39:49.300 --> 00:39:52.000
 as there's no significant concept shift now.

653
00:39:53.200 --> 00:39:56.200
Now that I've had Concepts Chef. This is in order to explain what

654
00:39:56.200 --> 00:39:59.800
 that is. We have to go to different realm. Basically. It is a significant shift

655
00:39:59.800 --> 00:40:02.400
 in the distribution of data and that is the

656
00:40:02.400 --> 00:40:05.200
 realm of ml Ops machine learning Ops, by the way, if you

657
00:40:05.200 --> 00:40:07.300
 want to know what machine learning obsess I recorded a

658
00:40:08.700 --> 00:40:09.400
Workshop

659
00:40:10.700 --> 00:40:13.300
I just haven't uploaded to the website yet. You

660
00:40:13.300 --> 00:40:16.200
 can watch that and you can you can see what that is. So it's about machine learning

661
00:40:16.200 --> 00:40:19.100
 put in production. So and these three next workshops. We're going to

662
00:40:19.100 --> 00:40:21.300
 look at machine learning on our computers.

663
00:40:22.600 --> 00:40:22.800
but

664
00:40:24.500 --> 00:40:27.200
we don't want to just be on the computer. We want it to be on the cloud. But then

665
00:40:27.200 --> 00:40:30.400
 once it's on the cloud you need to monitor it that's what envelopes is about. Anyway, I

666
00:40:30.400 --> 00:40:32.500
 just want to let's stick to here and

667
00:40:33.900 --> 00:40:36.500
I'll tell you what we can learn more about machine learning.

668
00:40:37.200 --> 00:40:39.100
Anyway, so we have three layers.

669
00:40:41.600 --> 00:40:42.700
narrow AI is

670
00:40:44.400 --> 00:40:46.900
using machine learning to do very

671
00:40:50.500 --> 00:40:52.500
well narrow tasks, for example.

672
00:40:59.200 --> 00:41:00.400
Speech to text.

673
00:41:01.600 --> 00:41:04.400
For example, it's like

674
00:41:04.400 --> 00:41:04.800
 a narrow.

675
00:41:06.200 --> 00:41:08.600
As a human being you're capable of doing many things, correct.

676
00:41:09.500 --> 00:41:11.100
You're able to perform many tasks.

677
00:41:11.700 --> 00:41:12.800
Make a sandwich.

678
00:41:13.800 --> 00:41:16.800
Make a telephone call recognize somebody's

679
00:41:16.800 --> 00:41:18.200
 face and greet them and so on.

680
00:41:19.100 --> 00:41:22.200
So all of these are narrow tasks that we are capable of doing

681
00:41:22.200 --> 00:41:25.300
 and now ai is using machine learning to machine

682
00:41:25.300 --> 00:41:28.200
 learning to do to use one of these to perform one

683
00:41:28.200 --> 00:41:28.900
 of these now tasks.

684
00:41:29.600 --> 00:41:32.100
Speech text is an example where you listen to

685
00:41:32.100 --> 00:41:32.400
 audio.

686
00:41:33.500 --> 00:41:35.300
And then you make a transcript of it.

687
00:41:36.500 --> 00:41:39.600
If you go to YouTube and you play a video there is the closed

688
00:41:39.600 --> 00:41:42.200
 caption option when you click the Switch.

689
00:41:43.500 --> 00:41:46.500
And there is the text corresponding to

690
00:41:46.500 --> 00:41:48.200
 the speaking person.

691
00:41:49.300 --> 00:41:52.700
It's not accurate all the time. There's a lot of mistakes in it, but that's an

692
00:41:52.700 --> 00:41:53.400
 example of an area.

693
00:41:54.200 --> 00:41:57.200
Another one is image classification what you feed the model of

694
00:41:57.200 --> 00:42:00.000
 pictures and the model will say this is a card. This is a truck. This is

695
00:42:00.200 --> 00:42:01.000
 a cat Etc.

696
00:42:03.300 --> 00:42:04.700
Now we are currently here.

697
00:42:08.200 --> 00:42:09.400
Generally AI.

698
00:42:10.400 --> 00:42:11.700
Is where we just stage where?

699
00:42:16.100 --> 00:42:19.500
the system we have whether it's using machine learning or a combination of

700
00:42:19.500 --> 00:42:22.900
 machine learning and rules-based systems rules bases

701
00:42:22.900 --> 00:42:23.100
 you

702
00:42:24.500 --> 00:42:24.700
like

703
00:42:26.400 --> 00:42:27.300
our programming

704
00:42:28.200 --> 00:42:31.200
but the the construction program if and else statements that

705
00:42:31.200 --> 00:42:31.400
 kind of thing.

706
00:42:32.400 --> 00:42:34.500
So generally is where we have a system.

707
00:42:35.400 --> 00:42:37.600
That can't that is equal to us.

708
00:42:39.500 --> 00:42:40.000
in a

709
00:42:41.400 --> 00:42:42.600
intellectual capacity

710
00:42:44.200 --> 00:42:45.800
so if you have a system that can

711
00:42:47.400 --> 00:42:50.300
learn like you think like you communicate like

712
00:42:50.300 --> 00:42:52.900
 you then we have reached General AI.

713
00:42:55.800 --> 00:42:58.100
Now why is it? Why are we

714
00:42:58.100 --> 00:43:00.400
 quite far from getting to Super AI?

715
00:43:02.300 --> 00:43:04.200
In order for us to get to Super AI.

716
00:43:05.800 --> 00:43:07.300
That is a stage where?

717
00:43:08.300 --> 00:43:10.600
Machines are superior to us and intelligence.

718
00:43:11.500 --> 00:43:14.400
Who was first be able to replicate our own intelligence?

719
00:43:16.200 --> 00:43:20.000
It makes sense, right? How can we exceed ourselves if we cannot replicate our

720
00:43:19.800 --> 00:43:21.400
 metal capabilities?

721
00:43:22.100 --> 00:43:22.400
that is

722
00:43:23.200 --> 00:43:26.100
getting to generally I so and why are we not able to get the general AI?

723
00:43:26.800 --> 00:43:27.100
because

724
00:43:28.100 --> 00:43:31.200
What is a human being like what what is it? That makes us

725
00:43:31.200 --> 00:43:33.000
 intelligent? It's our brain.

726
00:43:33.800 --> 00:43:34.100
Yes.

727
00:43:35.400 --> 00:43:38.500
Well, the thing is we don't know much about we don't know as much about us.

728
00:43:38.500 --> 00:43:40.600
 We don't know much about the brain as much as we

729
00:43:41.500 --> 00:43:42.600
want to know

730
00:43:43.500 --> 00:43:47.100
and this is the realm of biology and psychology. We

731
00:43:46.100 --> 00:43:50.200
 know a great deal about the paint but not enough to replicate

732
00:43:49.200 --> 00:43:50.400
 it.

733
00:43:50.900 --> 00:43:53.100
you know, you've heard the story, you know, what what they

734
00:43:54.700 --> 00:43:57.200
examine Einstein's brain just to see what's going on. We still don't

735
00:43:57.200 --> 00:44:00.300
 know. What what's we don't we still don't know all the intricacies of

736
00:44:00.300 --> 00:44:03.300
 human brain. So, how can we supersede? How can we ever produce

737
00:44:03.300 --> 00:44:06.200
 a just a super? Yeah, if you don't even know how our own

738
00:44:06.200 --> 00:44:06.500
 brain work.

739
00:44:07.300 --> 00:44:08.100
And we still here.

740
00:44:09.500 --> 00:44:12.400
So this is a practical explanation. There's also theoretical explanation but

741
00:44:13.400 --> 00:44:15.300
That's discussion for a later time.

742
00:44:19.600 --> 00:44:20.200
having said that

743
00:44:22.500 --> 00:44:25.200
of all the machine learning techniques that we have available to

744
00:44:25.200 --> 00:44:29.000
 us deep learning is the most sophisticated why because deep

745
00:44:28.100 --> 00:44:31.000
 learning is the is our

746
00:44:33.300 --> 00:44:35.300
attempt to replicate the human brain

747
00:44:36.500 --> 00:44:37.700
as close as possible.

748
00:44:39.200 --> 00:44:40.200
so deep learning

749
00:44:40.800 --> 00:44:43.100
Is how we can never get to this point if we can

750
00:44:43.100 --> 00:44:43.800
 get to this point?

751
00:44:44.700 --> 00:44:47.500
So now that we have covered this prerequisite, let's jump

752
00:44:47.500 --> 00:44:48.300
 to that slide now.

753
00:45:00.700 --> 00:45:01.500
Where did I put it?

754
00:45:08.400 --> 00:45:09.100
Come on.

755
00:45:12.700 --> 00:45:13.300
It should be.

756
00:45:15.600 --> 00:45:15.900
here

757
00:45:27.100 --> 00:45:30.500
No, so we said deep learning is our attempt to replicate.

758
00:45:31.500 --> 00:45:32.200
the human brain

759
00:45:35.100 --> 00:45:39.400
Of course the brain that we have understood up

760
00:45:38.400 --> 00:45:39.900
 to this point in time.

761
00:45:42.700 --> 00:45:43.900
So up here we have.

762
00:45:45.400 --> 00:45:46.100
a diagram

763
00:45:48.300 --> 00:45:51.400
or a pectoral model of a neuron

764
00:45:53.900 --> 00:45:56.400
so this is the most building block basic building block or

765
00:46:01.900 --> 00:46:04.100
you know organic component of the

766
00:46:05.600 --> 00:46:08.700
of the human brain, right? We don't want to go down to subatomic particles

767
00:46:08.700 --> 00:46:09.200
 and that kind of thing.

768
00:46:10.300 --> 00:46:12.600
So this is a neuron it has these.

769
00:46:14.500 --> 00:46:17.700
Let's say branches called the dendrites that

770
00:46:17.700 --> 00:46:18.800
 receives signals.

771
00:46:19.600 --> 00:46:21.800
electrochemical signals from other neurons

772
00:46:23.300 --> 00:46:26.100
and these signals travel down these dendrites.

773
00:46:26.900 --> 00:46:29.400
And then they are processed by this thing called a nucleus.

774
00:46:31.600 --> 00:46:33.400
If the signal is strong enough.

775
00:46:34.700 --> 00:46:37.400
Then another signal will travel down this Axon.

776
00:46:39.200 --> 00:46:40.100
and all to these other

777
00:46:41.100 --> 00:46:41.600
terminals

778
00:46:42.900 --> 00:46:45.700
and it is at these terminals where we

779
00:46:45.700 --> 00:46:47.200
 have connections with other neurons.

780
00:46:48.100 --> 00:46:51.400
So the way we are able to learn things and remember things is

781
00:46:51.400 --> 00:46:53.400
 by these electrochemical signals.

782
00:46:54.200 --> 00:46:55.100
firing off

783
00:46:55.800 --> 00:46:56.600
all of our brain

784
00:46:57.600 --> 00:47:00.400
and so we have these neurons come together to form thoughts to

785
00:47:00.400 --> 00:47:02.400
 form memories to form ideas.

786
00:47:03.600 --> 00:47:03.900
Okay.

787
00:47:07.700 --> 00:47:11.200
Well, you see here down here is a very abstract diagrammatical

788
00:47:10.200 --> 00:47:12.400
 representation of the above.

789
00:47:14.100 --> 00:47:16.600
What we want to do is a mathematically model this neuron.

790
00:47:17.600 --> 00:47:19.500
In deep learning we call the neuron.

791
00:47:20.300 --> 00:47:21.500
the perceptron

792
00:47:22.800 --> 00:47:25.600
So in our brain we have neurons in machine deep

793
00:47:25.600 --> 00:47:28.200
 learning we have these things called perceptrons which are supposed

794
00:47:28.200 --> 00:47:28.300
 to be.

795
00:47:29.700 --> 00:47:30.500
replicas

796
00:47:31.700 --> 00:47:33.700
Of the neuron that's a perceptron.

797
00:47:35.200 --> 00:47:37.300
so the dendrites

798
00:47:39.500 --> 00:47:42.000
here are the inputs. What are the

799
00:47:42.000 --> 00:47:42.300
 inputs?

800
00:47:44.700 --> 00:47:46.500
The inputs are the dimensions.

801
00:47:47.300 --> 00:47:48.700
Or The Columns of the data.

802
00:47:49.900 --> 00:47:52.400
If you if you think for example about

803
00:47:52.400 --> 00:47:55.000
 an Excel sheet, I can't use that realistic example.

804
00:47:56.400 --> 00:47:59.500
Size of the property distance to the city center number

805
00:47:59.500 --> 00:48:00.100
 of bedrooms.

806
00:48:02.700 --> 00:48:04.400
These are all columns right column vectors.

807
00:48:06.300 --> 00:48:08.100
And each column Vector is an input.

808
00:48:09.700 --> 00:48:12.300
And what's a circle that circle is the well, I'll

809
00:48:12.300 --> 00:48:15.400
 show you what the circle is. It's a combination of two things, but it will

810
00:48:15.400 --> 00:48:16.000
 be processed here.

811
00:48:17.600 --> 00:48:18.200
and then one

812
00:48:20.200 --> 00:48:22.500
value or one vector will be produced.

813
00:48:23.300 --> 00:48:23.700
from this

814
00:48:25.200 --> 00:48:28.800
N number of vectors, so it's a linear transformation in other words and then

815
00:48:28.800 --> 00:48:31.300
 here at the axon you have the option of either

816
00:48:31.300 --> 00:48:34.300
 outputting one vector or branching out to three

817
00:48:34.300 --> 00:48:35.100
 why?

818
00:48:36.500 --> 00:48:39.300
Do we want to have one of the other world depends on how we have struck to

819
00:48:39.300 --> 00:48:39.900
 do neural network?

820
00:48:41.800 --> 00:48:44.700
Let's look at a more mathematical representation of this model

821
00:48:44.700 --> 00:48:46.000
 and it's this.

822
00:48:54.200 --> 00:48:57.000
here we have x sub 1 all the way to x sub n

823
00:49:00.200 --> 00:49:02.500
where x sub I

824
00:49:03.500 --> 00:49:04.800
represents a component

825
00:49:05.900 --> 00:49:08.000
of some Vector X. Bye.

826
00:49:09.800 --> 00:49:12.000
We notice the excess here are italic.

827
00:49:13.100 --> 00:49:15.300
So I get to use a real estate example.

828
00:49:16.300 --> 00:49:19.200
Let's say x the vector x sub 1

829
00:49:20.200 --> 00:49:20.600
is the

830
00:49:24.600 --> 00:49:25.500
number of bedrooms

831
00:49:26.800 --> 00:49:29.500
and then x sub 1 could be three bedrooms.

832
00:49:31.200 --> 00:49:35.000
Then the vector X up 2 could be the size of the property in

833
00:49:34.900 --> 00:49:36.200
 square feet.

834
00:49:37.100 --> 00:49:37.600
So this will be

835
00:49:38.900 --> 00:49:40.000
2,000 square feet

836
00:49:41.300 --> 00:49:44.600
Right, so this could be three three bedrooms 2,000 square

837
00:49:44.600 --> 00:49:44.800
 feet.

838
00:49:45.500 --> 00:49:46.900
So basically all of the

839
00:49:49.300 --> 00:49:51.400
values of the first property

840
00:49:53.900 --> 00:49:56.400
then we have these W's anyone know what these WS are.

841
00:49:58.700 --> 00:50:00.300
Any guesses what w stands for?

842
00:50:03.600 --> 00:50:05.500
Precisely the weights right? We said that

843
00:50:06.700 --> 00:50:09.400
these are all factors that influence the

844
00:50:09.400 --> 00:50:10.200
 price of the property.

845
00:50:11.000 --> 00:50:11.200
but

846
00:50:12.600 --> 00:50:16.500
the number of bedroom the size of the apartment the

847
00:50:15.500 --> 00:50:17.000
 floor.

848
00:50:19.500 --> 00:50:20.300
Of the apartment.

849
00:50:21.900 --> 00:50:23.700
Whether it has a balcony or not.

850
00:50:24.700 --> 00:50:27.200
It's proximity to the city center. These are

851
00:50:27.200 --> 00:50:30.000
 all important factors, but some are more

852
00:50:30.300 --> 00:50:30.700
 important than others.

853
00:50:31.500 --> 00:50:34.100
All right. So for example being close to city center

854
00:50:34.100 --> 00:50:35.200
 is the most important factor.

855
00:50:35.900 --> 00:50:38.300
So for example, if x sub 3 is the proximity to

856
00:50:38.300 --> 00:50:40.800
 the city center, then it will have a much higher weight.

857
00:50:41.700 --> 00:50:44.300
whereas let's say except five, which

858
00:50:44.300 --> 00:50:44.600
 is the

859
00:50:49.200 --> 00:50:50.400
I don't know the lighting system, right?

860
00:50:51.600 --> 00:50:52.400
if it's

861
00:50:54.100 --> 00:50:56.000
if the use fluorescent instead of LED

862
00:50:57.700 --> 00:51:00.100
It's a very insignificant. It's something you can change,

863
00:51:00.100 --> 00:51:03.500
 you know in an hour or two, but let's say for example

864
00:51:03.500 --> 00:51:04.000
 Factor.

865
00:51:06.300 --> 00:51:09.600
It's it's it will have a much smaller weight.

866
00:51:11.200 --> 00:51:14.300
What you do then is you take the product of each X

867
00:51:14.300 --> 00:51:15.200
 and its weight.

868
00:51:17.300 --> 00:51:18.300
And then you sum.

869
00:51:19.400 --> 00:51:22.300
So you have product product and then some of the products?

870
00:51:23.200 --> 00:51:24.300
That's the summation here.

871
00:51:25.400 --> 00:51:28.300
And then that value that you will get will be the activation.

872
00:51:29.000 --> 00:51:29.500
now

873
00:51:32.200 --> 00:51:35.200
this activation will be fed into what we call an activation function.

874
00:51:36.900 --> 00:51:38.900
and an activation function is basically

875
00:51:39.900 --> 00:51:42.400
a function that will determine

876
00:51:42.400 --> 00:51:46.000
 the threshold of the signals if the signal is strong enough. We'll

877
00:51:45.400 --> 00:51:47.800
 pass it on to the next.

878
00:51:50.300 --> 00:51:52.100
perceptron otherwise

879
00:51:53.400 --> 00:51:55.600
We will not send any signal we listen to zero.

880
00:51:57.400 --> 00:52:01.100
For example, why let me give you an example why you would

881
00:52:00.100 --> 00:52:03.100
 need to choose an activation function why you would need to

882
00:52:03.100 --> 00:52:04.900
 process a signal or not?

883
00:52:06.400 --> 00:52:07.900
Imagine you're looking at a picture.

884
00:52:09.800 --> 00:52:12.300
and let me pull up a picture here picture of a

885
00:52:15.100 --> 00:52:15.400
character

886
00:52:24.400 --> 00:52:25.600
Any kind doesn't matter.

887
00:52:27.900 --> 00:52:30.100
And we want to know what is the number?

888
00:52:30.800 --> 00:52:31.500
in this picture

889
00:52:32.900 --> 00:52:35.600
the way we're going to the way the model is going to look at

890
00:52:35.600 --> 00:52:38.500
 this picture is to go through the entire picture

891
00:52:38.500 --> 00:52:41.300
 and of course if you remember from session,

892
00:52:43.300 --> 00:52:46.500
I think our very first session we said that pictures will

893
00:52:46.500 --> 00:52:48.700
 be represented as a matrices, right? This will be a matrix.

894
00:52:49.800 --> 00:52:52.200
And we said a white pixel is 255.

895
00:52:52.900 --> 00:52:54.300
And a black pixel is zero.

896
00:52:55.500 --> 00:52:57.900
So if we for example look at this.

897
00:53:01.300 --> 00:53:03.100
Section of the picture because we have to go.

898
00:53:03.900 --> 00:53:05.100
from one corner to the next

899
00:53:06.400 --> 00:53:07.700
and process the picture

900
00:53:09.100 --> 00:53:11.000
one super pixel at a time

901
00:53:11.700 --> 00:53:13.400
a super pixel is a collection of pixels.

902
00:53:14.300 --> 00:53:16.300
If we look at this super pixel, everything is white.

903
00:53:17.500 --> 00:53:18.200
There's nothing of

904
00:53:19.300 --> 00:53:20.000
there's nothing here.

905
00:53:20.700 --> 00:53:22.000
So it's activation will be zero.

906
00:53:22.900 --> 00:53:25.300
There's nothing worthwhile. However, if

907
00:53:25.300 --> 00:53:26.400
 we make our way here.

908
00:53:29.700 --> 00:53:32.200
Well now we will have a positive activation.

909
00:53:33.600 --> 00:53:36.100
Because we have a line. That's the idea. That's the purpose

910
00:53:36.100 --> 00:53:37.000
 of this activation function.

911
00:53:40.700 --> 00:53:43.400
and when we use this activation, we

912
00:53:43.400 --> 00:53:45.300
 will be able to narrow down our

913
00:53:48.800 --> 00:53:50.600
we will have another linear transformation.

914
00:53:52.200 --> 00:53:55.400
We will only have the number four and you will

915
00:53:55.400 --> 00:53:57.200
 discard the area around.

916
00:53:58.700 --> 00:54:01.400
And then later, what we will do is we will flatten this

917
00:54:01.400 --> 00:54:03.200
 Matrix. We'll have another linear transformation.

918
00:54:05.100 --> 00:54:07.500
Where we'll go for example from a 16 by 16.

919
00:54:08.500 --> 00:54:09.400
to a

920
00:54:10.800 --> 00:54:11.600
single vector

921
00:54:17.200 --> 00:54:18.000
So 32.

922
00:54:18.800 --> 00:54:20.300
1 by 32 vector

923
00:54:23.400 --> 00:54:26.400
now why why do we need to do this new transformation? We'll come

924
00:54:26.400 --> 00:54:29.100
 back this to the state has to do with distributions. We will leave that

925
00:54:29.100 --> 00:54:29.400
 for later.

926
00:54:30.300 --> 00:54:33.700
Well what I want us to First understand is this the main

927
00:54:33.700 --> 00:54:37.000
 component of the neural network perceptron now,

928
00:54:36.300 --> 00:54:39.200
 why is it called now work? Well because it's a network of

929
00:54:39.200 --> 00:54:39.900
 perceptrons.

930
00:54:41.600 --> 00:54:42.300
if you take

931
00:54:43.300 --> 00:54:43.700
those

932
00:54:44.900 --> 00:54:47.300
I think I was demonstrating something. That's

933
00:54:47.300 --> 00:54:49.100
 why I have this here. Let me give it up these.

934
00:54:50.400 --> 00:54:51.100
Sorry about that.

935
00:54:55.200 --> 00:54:55.800
if we take

936
00:54:58.100 --> 00:55:01.700
Those perceptions and put them into a network. We will get a neural network.

937
00:55:01.700 --> 00:55:03.300
 So all of these circles that you see here.

938
00:55:04.200 --> 00:55:07.300
These are all perceptron perceptron all the

939
00:55:07.300 --> 00:55:07.700
 perceptrons.

940
00:55:09.600 --> 00:55:12.100
It's called Deep learning because you know, we have

941
00:55:12.100 --> 00:55:13.300
 these layers.

942
00:55:14.700 --> 00:55:17.100
And they can if we stack.

943
00:55:18.400 --> 00:55:22.400
Enough of these layers will have a deep network of perceptrons.

944
00:55:21.400 --> 00:55:24.000
 So that's what's called Deep learning.

945
00:55:29.900 --> 00:55:32.100
And this is an example of a fully connected

946
00:55:32.100 --> 00:55:35.600
 Network and it's called fully connected because every perceptual is

947
00:55:35.600 --> 00:55:38.600
 can it's connected every other perceptron in the

948
00:55:38.600 --> 00:55:39.100
 subsequent Network.

949
00:55:41.700 --> 00:55:44.300
Run us are all the layers in this example are then

950
00:55:44.300 --> 00:55:47.000
 yes. See this densely. This is all that's correct.

951
00:55:54.500 --> 00:55:57.400
now in a neural network

952
00:55:58.400 --> 00:56:00.500
and a fully connected Network now there are different.

953
00:56:01.200 --> 00:56:04.200
Combinations and why you would choose

954
00:56:04.200 --> 00:56:05.600
 one combination versus the next?

955
00:56:08.900 --> 00:56:11.700
We'll have to do with the nature of the data is a pictorial.

956
00:56:11.700 --> 00:56:14.600
 Is it sequential like if you want to do Time series?

957
00:56:15.400 --> 00:56:18.200
Time series is where for example like

958
00:56:18.200 --> 00:56:19.900
 stocks right the price of the stock.

959
00:56:20.700 --> 00:56:21.700
Goes up and down.

960
00:56:22.600 --> 00:56:24.600
or the weather

961
00:56:26.300 --> 00:56:28.300
you want to see why does the weather go up or down?

962
00:56:29.600 --> 00:56:32.300
And if you see if you could predict it for the next

963
00:56:32.300 --> 00:56:33.100
 month or the next.

964
00:56:34.100 --> 00:56:34.400
week

965
00:56:35.600 --> 00:56:38.600
well, the change in temperatures associated with data is

966
00:56:38.600 --> 00:56:39.600
 associated with the

967
00:56:41.500 --> 00:56:44.300
Calendar right obviously server, it's warmer when

968
00:56:44.300 --> 00:56:44.900
 there is cooler.

969
00:56:46.500 --> 00:56:49.500
or another example is again speech to

970
00:56:49.500 --> 00:56:50.200
 text or

971
00:56:52.500 --> 00:56:55.200
Autocomplete, you know when you go to Gmail and you type in

972
00:56:55.200 --> 00:56:57.900
 an email you say for example. Hello, Ron.

973
00:56:59.800 --> 00:57:03.100
Please see the and then Gmail completely remaining

974
00:57:02.100 --> 00:57:04.600
 sentence. It was a CD attached.

975
00:57:05.200 --> 00:57:07.400
It will like fill in the blank. It will complete your sense.

976
00:57:08.300 --> 00:57:09.700
That is also accomplished using.

977
00:57:13.200 --> 00:57:14.200
a sequential Network

978
00:57:15.800 --> 00:57:18.400
So again, I'll show you different Arrangements. I'll show their pictures

979
00:57:18.400 --> 00:57:18.800
 of them, but

980
00:57:19.500 --> 00:57:21.100
there are different ways we can assemble this.

981
00:57:22.700 --> 00:57:25.300
But here again, if you see here we

982
00:57:25.300 --> 00:57:26.500
 have the input layer then we have.

983
00:57:27.400 --> 00:57:30.100
A series of hidden layers and then finally an upper layer.

984
00:57:30.900 --> 00:57:33.700
Now why this is have four, for

985
00:57:33.700 --> 00:57:36.400
 example, let's say you are doing again one more

986
00:57:36.400 --> 00:57:39.200
 time. I'll do the real estate example. You have all

987
00:57:39.200 --> 00:57:41.300
 of the features of the property.

988
00:57:42.200 --> 00:57:45.300
And then you're going to learn every single hidden layer in

989
00:57:45.300 --> 00:57:45.400
 here.

990
00:57:46.600 --> 00:57:47.700
Is going to learn a wheat?

991
00:57:48.600 --> 00:57:50.500
Basically on one thing I forgot to mention.

992
00:57:51.200 --> 00:57:51.400
here

993
00:57:52.800 --> 00:57:55.500
These weights are randomly assigned.

994
00:57:57.100 --> 00:57:58.300
Because the model does not know.

995
00:57:59.400 --> 00:58:02.400
The significance of the number of bedrooms versus the

996
00:58:02.400 --> 00:58:05.400
 significance of the proximity to the city center to

997
00:58:05.400 --> 00:58:07.200
 the model. These are all numbers there be nothing.

998
00:58:07.900 --> 00:58:10.900
So it will first randomly assign weights and

999
00:58:10.900 --> 00:58:12.700
 then using a what we call.

1000
00:58:13.500 --> 00:58:14.800
back propagation

1001
00:58:16.500 --> 00:58:18.200
and a technique in

1002
00:58:19.700 --> 00:58:22.300
Multivared calculus called gradient descent it

1003
00:58:22.300 --> 00:58:23.200
 will adjust the weights.

1004
00:58:24.300 --> 00:58:27.400
Basically, it will make one round of prediction

1005
00:58:27.400 --> 00:58:28.800
 compare it to a sample.

1006
00:58:31.200 --> 00:58:34.300
And then it will adjust the weights because if you remember we

1007
00:58:34.300 --> 00:58:37.800
 talked about MSC in our last Workshop. We

1008
00:58:37.800 --> 00:58:40.600
 said that we want msct conversions you have to convergence

1009
00:58:40.600 --> 00:58:43.300
 is zero, then what the model predicts and what

1010
00:58:43.300 --> 00:58:46.700
 the actual prices are will be equivalent. If

1011
00:58:46.700 --> 00:58:49.300
 the errors are 0 they will be equivalent and that's what

1012
00:58:49.300 --> 00:58:50.100
 we want ideally.

1013
00:58:51.100 --> 00:58:54.200
Initially the model when we first feed the data to the

1014
00:58:54.200 --> 00:58:55.900
 network the MSC will be large.

1015
00:58:56.600 --> 00:58:59.400
But the model will keep looking at the sample again and again

1016
00:58:59.400 --> 00:59:01.500
 and again and adjust the weights.

1017
00:59:02.300 --> 00:59:05.300
Adjust the weight again and again and again and again until it has

1018
00:59:05.300 --> 00:59:08.100
 understood why one feature is important by other features

1019
00:59:08.100 --> 00:59:08.600
 not important.

1020
00:59:09.500 --> 00:59:12.400
Now again, if you're just trying to predict the

1021
00:59:12.400 --> 00:59:14.400
 number you'll have only one output.

1022
00:59:15.200 --> 00:59:16.100
Which will be the price.

1023
00:59:17.600 --> 00:59:19.800
if however we're doing something like

1024
00:59:22.200 --> 00:59:24.300
c r n

1025
00:59:24.900 --> 00:59:27.200
a convolutional new network where

1026
00:59:27.200 --> 00:59:29.300
 you're going to go from a matrix data.

1027
00:59:30.500 --> 00:59:31.000
to

1028
00:59:32.600 --> 00:59:35.600
to a Vertex to a vector

1029
00:59:36.700 --> 00:59:39.000
if you're going to be doing this transformation, this is a

1030
00:59:39.200 --> 00:59:42.200
 linear transformation. Then what what you will append at

1031
00:59:42.200 --> 00:59:45.000
 the end of the outpole layer could be another Network. You can actually

1032
00:59:45.400 --> 00:59:48.800
 mix two networks together especially for complex problems.

1033
00:59:51.800 --> 00:59:52.300
moving on

1034
00:59:54.100 --> 00:59:56.400
this is where the activation function will get this later on.

1035
00:59:59.400 --> 01:00:00.300
and like I said

1036
01:00:01.400 --> 01:00:04.200
Okay, let me also take you

1037
01:00:04.200 --> 01:00:04.800
 through this very quickly.

1038
01:00:07.200 --> 01:00:08.800
so we have three types of

1039
01:00:11.400 --> 01:00:14.900
fundamental networks. We have the multi-layer perceptron

1040
01:00:14.900 --> 01:00:17.400
 multi-layer perceptrons are sufficient for

1041
01:00:18.300 --> 01:00:21.400
Tabular data, so if you have an Excel sheet with five six

1042
01:00:21.400 --> 01:00:21.900
 seven columns.

1043
01:00:22.600 --> 01:00:25.500
And you want to do regression or classification? You can create an

1044
01:00:25.500 --> 01:00:25.900
 MLP.

1045
01:00:27.100 --> 01:00:30.800
And it's very trivial to create an employee with libraries like pytorch

1046
01:00:30.800 --> 01:00:32.000
 and tensorflow.

1047
01:00:34.700 --> 01:00:35.800
if you're going to be dealing with

1048
01:00:37.100 --> 01:00:37.600
sick

1049
01:00:38.700 --> 01:00:40.800
images especially images of

1050
01:00:41.400 --> 01:00:44.500
three color channels and you will use a convolutional neural network

1051
01:00:44.500 --> 01:00:46.500
 again the word convolution has to do with

1052
01:00:47.700 --> 01:00:50.800
This Matrix operation in a convolutional network.

1053
01:00:50.800 --> 01:00:53.800
 You have two types of matrices The Matrix

1054
01:00:53.800 --> 01:00:54.900
 representing the picture.

1055
01:00:56.100 --> 01:00:59.300
And the Matrix which is the kernel which is

1056
01:00:59.300 --> 01:00:59.900
 used to.

1057
01:01:03.900 --> 01:01:06.200
or lack of whatever words purse to

1058
01:01:09.400 --> 01:01:12.400
observe the super pixels

1059
01:01:12.400 --> 01:01:13.000
 of the picture

1060
01:01:13.700 --> 01:01:14.900
I think we've seen this before.

1061
01:01:15.700 --> 01:01:18.500
I think I've shown your kernel but not we will look at it. And

1062
01:01:18.500 --> 01:01:21.000
 then and one of the sessions where we will

1063
01:01:21.100 --> 01:01:24.700
 do you will use CNN for image recognition and then there's a recurrential network.

1064
01:01:24.700 --> 01:01:25.900
 Long story short.

1065
01:01:27.100 --> 01:01:30.400
You would use MLP if you have simple tabular data, everything is

1066
01:01:30.400 --> 01:01:31.000
 numerical.

1067
01:01:33.300 --> 01:01:34.400
Cnns that we have.

1068
01:01:37.800 --> 01:01:38.100
You know.

1069
01:01:41.600 --> 01:01:44.200
Square matrices anything that needs to be put in

1070
01:01:44.200 --> 01:01:47.500
 scrimmage like a picture or videos after all videos are sequence

1071
01:01:47.500 --> 01:01:47.700
 of pictures.

1072
01:01:48.400 --> 01:01:50.300
And our events if you have sequential data.

1073
01:01:50.900 --> 01:01:52.600
for time series for

1074
01:01:53.600 --> 01:01:56.500
speech to text or text to speech for set table

1075
01:01:56.500 --> 01:01:56.900
 analysis.

1076
01:01:57.700 --> 01:01:59.800
by the way, if any of you are interested in

1077
01:02:01.300 --> 01:02:02.200
predicting stock

1078
01:02:02.900 --> 01:02:05.900
Is what predicting stock prices it's very problematic because

1079
01:02:05.900 --> 01:02:08.300
 something you really can't really can't do

1080
01:02:08.300 --> 01:02:09.400
 but beside if you're interested in.

1081
01:02:10.400 --> 01:02:13.000
You know buying and selling stocks.

1082
01:02:14.800 --> 01:02:17.100
Well the price of a stock.

1083
01:02:17.800 --> 01:02:20.600
Is strongly influenced by public opinion?

1084
01:02:22.800 --> 01:02:25.700
And so there are some companies that use sentiment

1085
01:02:25.700 --> 01:02:28.600
 analysis to see what people are saying about certain topic.

1086
01:02:30.200 --> 01:02:33.700
Like what was that stock game Gamestop?

1087
01:02:34.900 --> 01:02:35.600
right

1088
01:02:36.900 --> 01:02:37.800
and people will

1089
01:02:39.800 --> 01:02:42.200
what was the what they were using when they were on Twitter?

1090
01:02:45.600 --> 01:02:46.900
opposite of Short Selling

1091
01:02:50.600 --> 01:02:51.500
opposite of shooting

1092
01:02:53.900 --> 01:02:56.000
longing right they were

1093
01:02:56.200 --> 01:02:59.100
 trying to Long the game start by showing people were talking

1094
01:02:59.100 --> 01:03:02.900
 about the old Nokia or all the you

1095
01:03:02.900 --> 01:03:02.900
 know,

1096
01:03:03.800 --> 01:03:04.500
electronic

1097
01:03:06.600 --> 01:03:09.000
forms of entertainment and so they were making

1098
01:03:09.200 --> 01:03:13.100
 the making the price go up. So as people's basically you

1099
01:03:12.100 --> 01:03:15.600
 could you set them analysis to see you are people

1100
01:03:15.600 --> 01:03:17.500
 sentiment positive about this topic.

1101
01:03:18.200 --> 01:03:19.800
The price is going up. Let's buy it.

1102
01:03:20.800 --> 01:03:23.300
If the if people are saying negative things, let's say

1103
01:03:23.300 --> 01:03:26.400
 I don't know the CEO of this particular company said something

1104
01:03:26.400 --> 01:03:29.500
 very terrible. Well something very racist or

1105
01:03:29.500 --> 01:03:32.600
 sexist or whatever and the price is going to drop and so

1106
01:03:32.600 --> 01:03:35.400
 you can use sentiment last to detect that and then start

1107
01:03:35.400 --> 01:03:38.700
 shorting or just start selling.

1108
01:03:41.500 --> 01:03:42.300
Exactly wrong.

1109
01:03:44.200 --> 01:03:45.100
So now of course.

1110
01:03:47.400 --> 01:03:49.200
Getting access to the data is another story.

1111
01:03:51.600 --> 01:03:54.300
Then a bunch of Nursery going to stop exactly

1112
01:03:54.300 --> 01:03:57.700
 yeah as as to retaliate.

1113
01:03:57.700 --> 01:04:00.200
 It's very fascinating story. But yeah that my point is this

1114
01:04:01.500 --> 01:04:04.600
without going to off topic here you if you

1115
01:04:04.600 --> 01:04:07.000
 have if you want to process sequential data like

1116
01:04:07.300 --> 01:04:08.500
 anything do with time.

1117
01:04:09.300 --> 01:04:12.000
or words or

1118
01:04:13.700 --> 01:04:16.500
Yeah, sound bites you won't use RNs.

1119
01:04:16.500 --> 01:04:19.200
 But of course, like I said you can we can combine.

1120
01:04:20.500 --> 01:04:24.200
Like for one problem that I'm working on I'm using a drnn.

1121
01:04:25.200 --> 01:04:28.600
Which is a combination of convolutional networks and

1122
01:04:28.600 --> 01:04:31.300
 recurring units because what I want to do is go from

1123
01:04:31.300 --> 01:04:32.000
 pictures.

1124
01:04:33.200 --> 01:04:33.400
to

1125
01:04:34.800 --> 01:04:36.200
a sequence of characters

1126
01:04:36.900 --> 01:04:39.200
Basically, I think I talked about this. I

1127
01:04:39.200 --> 01:04:41.400
 want to look at a mathematic a picture of an equation.

1128
01:04:42.400 --> 01:04:46.300
And converted to a later code

1129
01:04:46.300 --> 01:04:47.500
 which is a sequence of characters.

1130
01:04:49.800 --> 01:04:53.200
Yeah, and thank you one for sharing the the background

1131
01:04:52.200 --> 01:04:54.900
 of that GameStop scenario.

1132
01:04:56.100 --> 01:04:59.300
So yeah, we'll set up allows us you can see you know, what people are talking about and

1133
01:04:59.300 --> 01:05:00.100
 then react.

1134
01:05:02.400 --> 01:05:03.500
preemptively

1135
01:05:04.500 --> 01:05:04.700
Okay.

1136
01:05:06.100 --> 01:05:09.100
Now there are stuff. This slides have

1137
01:05:09.100 --> 01:05:10.900
 to do with the math Market representation.

1138
01:05:12.400 --> 01:05:14.800
Here's an example of using a CNN for pictures.

1139
01:05:16.100 --> 01:05:18.800
So for example, you take this picture of a cat.

1140
01:05:19.400 --> 01:05:22.400
And this picture of the cat is colored. So you have at

1141
01:05:22.400 --> 01:05:24.000
 least three color channels red green and blue.

1142
01:05:24.600 --> 01:05:25.200
and each

1143
01:05:27.400 --> 01:05:30.500
channel will be put into its own Matrix, which we call

1144
01:05:30.500 --> 01:05:30.600
 a

1145
01:05:31.400 --> 01:05:33.100
two-dimensional convolutional

1146
01:05:34.400 --> 01:05:37.600
layer, it's called two of you because we have two dimensions

1147
01:05:37.600 --> 01:05:38.900
 for each color Channel.

1148
01:05:39.600 --> 01:05:42.500
And it's called the convolution because we're going to convolve.

1149
01:05:43.900 --> 01:05:45.000
the kernel

1150
01:05:46.800 --> 01:05:47.000
with

1151
01:05:48.000 --> 01:05:48.300
each

1152
01:05:49.500 --> 01:05:50.100
color Channel

1153
01:05:51.100 --> 01:05:54.700
now I will show you convolution in a separate session, but the

1154
01:05:54.700 --> 01:05:57.100
 idea is in this particular architecture you will go

1155
01:05:57.100 --> 01:05:57.400
 from

1156
01:05:58.500 --> 01:06:02.500
the picture to a flat Factor

1157
01:06:01.500 --> 01:06:03.900
 again a linear transformation

1158
01:06:05.300 --> 01:06:06.300
and this flat

1159
01:06:07.700 --> 01:06:08.300
vector

1160
01:06:08.900 --> 01:06:10.400
will correspond to the distribution

1161
01:06:11.700 --> 01:06:15.000
Again by the way, yes matrices

1162
01:06:14.100 --> 01:06:17.800
 are very important for new Networks.

1163
01:06:18.400 --> 01:06:21.800
But at the end of the day, this is still machine learning statistical

1164
01:06:21.800 --> 01:06:24.500
 learning. So distributions are very very important.

1165
01:06:26.500 --> 01:06:29.700
if you curious about distributions, that's

1166
01:06:29.700 --> 01:06:31.400
 the next course, which begins

1167
01:06:32.300 --> 01:06:36.900
Sometime later in the second

1168
01:06:35.900 --> 01:06:38.200
 or third week Vlog is anyway, let's

1169
01:06:38.200 --> 01:06:38.600
 come back here.

1170
01:06:40.900 --> 01:06:42.900
And this let me show you one last thing and

1171
01:06:45.400 --> 01:06:48.200
Let's begin with the basics of the what code we're not quite done

1172
01:06:48.200 --> 01:06:49.700
 with this. There's quite a lot to cover here.

1173
01:06:50.300 --> 01:06:52.000
But let me just show you.

1174
01:06:55.400 --> 01:06:57.000
the mathematical representation of

1175
01:06:58.400 --> 01:06:59.300
a neural network

1176
01:07:02.100 --> 01:07:03.200
I can find that that is.

1177
01:07:08.400 --> 01:07:08.900
Here we go.

1178
01:07:11.500 --> 01:07:13.000
This equation is from a book.

1179
01:07:14.600 --> 01:07:16.700
from charu Agarwal a very

1180
01:07:18.800 --> 01:07:21.500
popular author of all kinds of machine learning

1181
01:07:21.500 --> 01:07:21.800
 books

1182
01:07:23.500 --> 01:07:26.700
And he's very well written I would strongly recommend

1183
01:07:26.700 --> 01:07:29.100
 any of his books. He has a book on.

1184
01:07:30.100 --> 01:07:33.700
Machine learning for text. I recommend it. He has a book on data mining.

1185
01:07:33.700 --> 01:07:36.300
 I recommend it. He's a book on using

1186
01:07:36.300 --> 01:07:39.600
 machine learning or data money for healthcare. I recommend

1187
01:07:39.600 --> 01:07:42.500
 that too. In fact, I recommend all of his books, but here

1188
01:07:42.500 --> 01:07:46.000
 let's just pay attention to this the formula.

1189
01:07:45.100 --> 01:07:47.300
 We want to understand the formula here.

1190
01:07:48.500 --> 01:07:50.200
We can see we have w.

1191
01:07:52.200 --> 01:07:55.200
Now this author I guess does not use the Bold.

1192
01:07:56.600 --> 01:07:58.700
Convention, I would prefer that they did.

1193
01:08:00.100 --> 01:08:03.700
But that's the Matrix the transpose of the Matrix multiplied by

1194
01:08:03.700 --> 01:08:05.500
 the scalar value, which here is the mean?

1195
01:08:06.200 --> 01:08:06.600
of the data

1196
01:08:07.500 --> 01:08:11.000
this five function is the activation function what I

1197
01:08:10.100 --> 01:08:12.700
 try to really say, is that a neural network?

1198
01:08:13.500 --> 01:08:14.600
Is linear algebra?

1199
01:08:17.600 --> 01:08:20.500
So what we're going to do is look at we will look at the mathematical representation

1200
01:08:20.500 --> 01:08:20.800
 of it.

1201
01:08:21.700 --> 01:08:24.500
And look at it in its python

1202
01:08:24.500 --> 01:08:25.100
 implementation.

1203
01:08:26.100 --> 01:08:28.600
but we'll start with the python implementation first because that's

1204
01:08:29.400 --> 01:08:31.200
we want to get the ball rolling ball rolling.

1205
01:08:32.100 --> 01:08:32.300
Okay.

1206
01:08:35.300 --> 01:08:36.400
so we want to understand the

1207
01:08:37.600 --> 01:08:40.400
terms here the symbols here. What's what exactly

1208
01:08:40.400 --> 01:08:41.800
 like what's this calculation?

1209
01:08:42.600 --> 01:08:42.900
and then

1210
01:08:44.400 --> 01:08:47.800
you know, we will have a thorough and foundational understanding

1211
01:08:47.800 --> 01:08:48.000
 of

1212
01:08:48.900 --> 01:08:49.800
a neural networks

1213
01:08:50.800 --> 01:08:51.100
Okay.

1214
01:08:56.900 --> 01:08:59.000
We have 20 minutes left. So let me do

1215
01:08:59.200 --> 01:08:59.900
 this as fast and

1216
01:09:02.100 --> 01:09:02.700
the optimal way

1217
01:09:07.300 --> 01:09:08.600
let me open up the reference code.

1218
01:09:40.900 --> 01:09:43.100
I have a few notebooks. I want

1219
01:09:43.100 --> 01:09:44.600
 to see this simplest one.

1220
01:10:02.100 --> 01:10:03.600
This is for image classification.

1221
01:10:06.700 --> 01:10:09.600
And this is for predicting realistic properties.

1222
01:10:37.200 --> 01:10:38.400
Yeah, I think we can use this.

1223
01:10:45.900 --> 01:10:47.600
Okay, let me use this notebook.

1224
01:10:48.900 --> 01:10:50.700
So as opposed to typing it from scratch.

1225
01:10:51.500 --> 01:10:54.100
I'm going to show you the whole thing and then we will break it

1226
01:10:54.100 --> 01:10:54.400
 down.

1227
01:10:55.700 --> 01:10:57.000
I'll break down the code but

1228
01:11:04.200 --> 01:11:07.500
There is the we need to cover some more we

1229
01:11:07.500 --> 01:11:08.000
 need to cover.

1230
01:11:15.300 --> 01:11:18.000
some prerequisites in statistics and

1231
01:11:18.600 --> 01:11:20.000
multivared calculus

1232
01:11:20.500 --> 01:11:23.100
I already have a video for that. So I will tell you

1233
01:11:23.100 --> 01:11:24.100
 what to find. It's on YouTube.

1234
01:11:25.300 --> 01:11:27.200
I'll post a video in the slack Channel.

1235
01:11:29.400 --> 01:11:29.900
So let's begin.

1236
01:11:33.100 --> 01:11:34.200
We have pandas.

1237
01:11:36.200 --> 01:11:40.100
Pandas is a library that extends the capabilities

1238
01:11:39.100 --> 01:11:40.800
 of numpy.

1239
01:11:42.400 --> 01:11:43.700
It extended by.

1240
01:11:45.400 --> 01:11:48.200
Turning a numpy array into a thing called

1241
01:11:48.200 --> 01:11:49.000
 a data frame.

1242
01:11:49.800 --> 01:11:52.700
And it's data frames

1243
01:11:52.700 --> 01:11:55.400
 are really good because you can convert an

1244
01:11:55.400 --> 01:11:57.400
 Excel sheet into a python.

1245
01:11:59.100 --> 01:11:59.500
object

1246
01:12:00.400 --> 01:12:03.100
so you can see the column names of the CSV file

1247
01:12:03.100 --> 01:12:05.100
 or Excel file or tsv file?

1248
01:12:06.200 --> 01:12:08.600
You can perform summary statistics.

1249
01:12:09.400 --> 01:12:10.500
for some reason statistics

1250
01:12:11.300 --> 01:12:14.700
or what actually what I should say is descriptive statistics.

1251
01:12:14.700 --> 01:12:17.200
 So we have the descriptive statistics, which is very basic stuff.

1252
01:12:17.900 --> 01:12:20.400
And then there's inferential statistics. This is

1253
01:12:20.400 --> 01:12:21.000
 the most sophisticated.

1254
01:12:22.600 --> 01:12:25.000
In descriptive statistics, we want to see for example, what is

1255
01:12:25.400 --> 01:12:27.200
 what are the averages the mean median and mode?

1256
01:12:28.100 --> 01:12:29.300
That's what we call measures of.

1257
01:12:30.500 --> 01:12:33.300
Central tendency like what is

1258
01:12:33.300 --> 01:12:33.800
 it centered?

1259
01:12:34.600 --> 01:12:35.800
and the second part of

1260
01:12:37.400 --> 01:12:40.100
descriptive statistics is measure of variance.

1261
01:12:41.800 --> 01:12:44.700
Or other measures of this Persian.

1262
01:12:45.300 --> 01:12:48.300
How much variation do we have in data, you know how much variation we

1263
01:12:48.300 --> 01:12:51.600
 have? And what is the center of the data? We can approximate the

1264
01:12:51.600 --> 01:12:54.500
 shape of data and knowing the shape of data is

1265
01:12:54.500 --> 01:12:55.700
 a huge huge deal.

1266
01:12:58.400 --> 01:13:02.000
And if you want to know why I'll in that video. It's

1267
01:13:01.300 --> 01:13:04.400
 a one video and that one video. I

1268
01:13:04.400 --> 01:13:07.300
 will talk about distributions and gradient descent

1269
01:13:07.300 --> 01:13:08.800
 and that should give you a good enough.

1270
01:13:11.600 --> 01:13:14.900
Review of probably talking about in the

1271
01:13:14.900 --> 01:13:16.300
 subsequent sessions or even in here.

1272
01:13:17.100 --> 01:13:19.500
So we will have what pan doesn't have carrots caress.

1273
01:13:20.100 --> 01:13:24.000
Is a library birth on top of tensorflow basically the

1274
01:13:23.700 --> 01:13:26.300
 abstracts away some of the complexities of potential

1275
01:13:26.300 --> 01:13:29.900
 flow. So if you want to build a quick simple neural network,

1276
01:13:29.900 --> 01:13:32.900
 you may you want to go with Keras as opposed

1277
01:13:32.900 --> 01:13:33.300
 to tensorflow.

1278
01:13:36.500 --> 01:13:39.500
And here we are going to create a sequential model why sequential because

1279
01:13:39.500 --> 01:13:42.000
 we can have a sequence of layers.

1280
01:13:43.600 --> 01:13:44.300
input layer

1281
01:13:45.500 --> 01:13:48.400
a few or several hidden layers and then an

1282
01:13:48.400 --> 01:13:48.800
 upper layer.

1283
01:13:50.800 --> 01:13:53.500
Of course, but we want to we need to put layers inside

1284
01:13:53.500 --> 01:13:56.400
 the sequence. So we're going to import everything again

1285
01:13:56.400 --> 01:13:59.200
 the star meets everything. So from carrots that

1286
01:13:59.200 --> 01:14:00.800
 layers we will import everything all the layers.

1287
01:14:03.400 --> 01:14:06.700
Another important part of machine learning is evaluating the

1288
01:14:06.700 --> 01:14:06.800
 model.

1289
01:14:08.400 --> 01:14:09.700
But I believe that for later.

1290
01:14:10.600 --> 01:14:12.100
This is for mother to be evaluation.

1291
01:14:13.500 --> 01:14:14.900
We have the CSV data.

1292
01:14:15.800 --> 01:14:17.800
Which you I'll show you right right down below.

1293
01:14:18.600 --> 01:14:19.900
And I will give this file to you later.

1294
01:14:21.800 --> 01:14:23.600
We will import instead of using pandas.

1295
01:14:24.500 --> 01:14:27.100
Sorry about that. We will import the data using pandas.

1296
01:14:28.300 --> 01:14:31.400
And you will visualize the first five rows. That's

1297
01:14:31.400 --> 01:14:32.300
 what the head method does.

1298
01:14:34.600 --> 01:14:35.600
You can see here we have.

1299
01:14:36.700 --> 01:14:37.500
the year built

1300
01:14:38.500 --> 01:14:39.500
the number of stories

1301
01:14:40.300 --> 01:14:41.000
and the number of

1302
01:14:41.800 --> 01:14:43.300
one story house two story house

1303
01:14:43.900 --> 01:14:45.000
the number of bedrooms

1304
01:14:45.800 --> 01:14:48.300
the number of full bathrooms. I have

1305
01:14:48.300 --> 01:14:51.000
 patterns four bathrooms our bathrooms that contain, you know

1306
01:14:51.700 --> 01:14:52.100
 showers or bats.

1307
01:14:53.300 --> 01:14:55.000
Now one thing you'll notice is that these numbers.

1308
01:14:56.700 --> 01:14:57.800
are normalized

1309
01:15:01.100 --> 01:15:04.600
so no value goes between no everybody numbers between

1310
01:15:04.600 --> 01:15:05.300
 0 and 1.

1311
01:15:07.300 --> 01:15:09.400
now of course is a way we can normalize this data using

1312
01:15:12.100 --> 01:15:13.200
psychic learn

1313
01:15:13.900 --> 01:15:15.600
Cyclone is a library for machine learning.

1314
01:15:16.500 --> 01:15:19.100
But this data this this data has been normalized.

1315
01:15:21.200 --> 01:15:21.700
So for example

1316
01:15:24.900 --> 01:15:27.100
If let's say one of the let's say

1317
01:15:27.100 --> 01:15:30.200
 we were we have one property that

1318
01:15:30.200 --> 01:15:30.800
 has six.

1319
01:15:32.600 --> 01:15:34.800
Bathrooms, it's a very spacious house.

1320
01:15:36.400 --> 01:15:37.400
Got Penthouse or something.

1321
01:15:38.400 --> 01:15:38.700
then

1322
01:15:39.900 --> 01:15:42.800
if when we know what is it or if

1323
01:15:42.800 --> 01:15:42.900
 you

1324
01:15:47.600 --> 01:15:50.200
If we if we normalize that this will be this will have

1325
01:15:50.200 --> 01:15:51.100
 the number one.

1326
01:15:52.900 --> 01:15:55.200
the highest number of full bathrooms

1327
01:15:55.800 --> 01:15:59.200
So if you want to know how many yeah, if

1328
01:15:59.200 --> 01:16:01.800
 you want to know for example, how many bathrooms?

1329
01:16:02.600 --> 01:16:05.800
This apartment has you will multiply six by

1330
01:16:05.800 --> 01:16:06.700
 zero point.

1331
01:16:09.700 --> 01:16:10.500
one two five

1332
01:16:13.200 --> 01:16:16.800
Okay, this is not even one. So I think the part I

1333
01:16:16.800 --> 01:16:18.000
 think the property with the most.

1334
01:16:20.300 --> 01:16:23.100
Number of bathrooms was something like 8 or 10. I

1335
01:16:23.100 --> 01:16:24.000
 have to look at the original data.

1336
01:16:24.700 --> 01:16:25.600
But you get the idea.

1337
01:16:26.700 --> 01:16:26.800
Yeah.

1338
01:16:30.800 --> 01:16:33.300
And suppose we have one that same property

1339
01:16:33.300 --> 01:16:37.200
 had four stories. That was a very large house. So 0.35. Well,

1340
01:16:36.200 --> 01:16:38.300
 that's just the one story house.

1341
01:16:41.300 --> 01:16:44.200
Now we did some data cleaning before

1342
01:16:44.200 --> 01:16:48.200
 we got this. So if you see here this data set is called how data

1343
01:16:47.200 --> 01:16:48.300
 set.

1344
01:16:49.700 --> 01:16:50.900
house data set

1345
01:16:52.100 --> 01:16:52.800
scaled

1346
01:16:55.400 --> 01:16:58.300
so in this notebook in this Jupiter notebook, I'm skipping

1347
01:16:58.300 --> 01:16:58.500
 the

1348
01:16:59.900 --> 01:17:01.100
data cleaning part

1349
01:17:01.900 --> 01:17:04.100
Anyway, this is the this is an

1350
01:17:04.100 --> 01:17:07.700
 end dimensional data as you can see. It's so big that I

1351
01:17:07.700 --> 01:17:09.600
 cannot even fit and we have these three dots.

1352
01:17:10.400 --> 01:17:10.700
I think

1353
01:17:13.400 --> 01:17:16.400
there were I don't know they were

1354
01:17:16.400 --> 01:17:17.400
 definitely more than 20.

1355
01:17:18.200 --> 01:17:21.400
Columns this Matrix. So it's a very large Matrix.

1356
01:17:22.400 --> 01:17:24.900
For this we would PC would be very helpful.

1357
01:17:27.600 --> 01:17:30.100
Anyway, that's the data set. We can preview with the

1358
01:17:30.100 --> 01:17:30.700
 word pandas.

1359
01:17:34.200 --> 01:17:34.200
and

1360
01:17:37.700 --> 01:17:40.400
let me skip to discussion about labels and features because we haven't

1361
01:17:40.400 --> 01:17:41.000
 talked about yet.

1362
01:17:41.700 --> 01:17:44.300
And let me skip the train test split part. Let me show you how to create

1363
01:17:44.300 --> 01:17:45.900
 a sequence of neural networks.

1364
01:17:47.300 --> 01:17:50.100
So here what we do is we have the sequential class and we know

1365
01:17:50.100 --> 01:17:53.500
 classes because classes begin with an uppercase

1366
01:17:53.500 --> 01:17:53.700
 letter.

1367
01:17:54.200 --> 01:17:57.800
Anytime you see some function or a

1368
01:17:57.800 --> 01:17:58.000
 variable.

1369
01:17:58.700 --> 01:18:00.500
Spelling uppercase. That's a class.

1370
01:18:02.300 --> 01:18:05.800
Which means we can instantiate it and this will become an object and

1371
01:18:05.800 --> 01:18:08.400
 what we do what we know about what we know about objectives that

1372
01:18:08.400 --> 01:18:09.500
 we have attribution properties.

1373
01:18:11.800 --> 01:18:14.600
So one of the properties of the sequential

1374
01:18:14.600 --> 01:18:15.600
 object is at

1375
01:18:16.900 --> 01:18:19.100
So here we are adding a densely connected.

1376
01:18:20.600 --> 01:18:20.800
Excuse me.

1377
01:18:21.900 --> 01:18:22.700
a dense

1378
01:18:24.300 --> 01:18:25.200
input layer

1379
01:18:27.300 --> 01:18:27.800
we have

1380
01:18:30.700 --> 01:18:32.000
63 inputs

1381
01:18:35.100 --> 01:18:35.800
and 50

1382
01:18:38.100 --> 01:18:41.500
outputs so if I go to that slide

1383
01:18:43.800 --> 01:18:45.200
That's fully connected Network.

1384
01:18:49.900 --> 01:18:50.600
If you see here.

1385
01:18:52.900 --> 01:18:54.000
We are talking about.

1386
01:18:58.800 --> 01:18:59.400
this part

1387
01:19:02.200 --> 01:19:05.600
So we have 63 Dimensions here. We have Harmony to

1388
01:19:05.600 --> 01:19:08.600
 me here the input dimensions are what one two,

1389
01:19:08.600 --> 01:19:10.900
 three, four, five six, seven eight.

1390
01:19:11.800 --> 01:19:14.800
And the output dimensions are one two, three, four,

1391
01:19:14.800 --> 01:19:16.900
 five six, seven eight nine.

1392
01:19:17.600 --> 01:19:20.200
So input dimensions are 8 output dimensions are

1393
01:19:20.200 --> 01:19:20.400
 9.

1394
01:19:22.100 --> 01:19:25.500
Now there is by the way, there is no precise formula for

1395
01:19:25.500 --> 01:19:27.500
 deciding the number of output dimensions.

1396
01:19:30.200 --> 01:19:33.300
And there are some experimentation involved.

1397
01:19:34.100 --> 01:19:37.300
Tensorflow has something called The tensional Playground, which is

1398
01:19:37.300 --> 01:19:40.200
 a very nice visual way for you to

1399
01:19:40.200 --> 01:19:43.400
 see how neural networks work. But again because we

1400
01:19:43.400 --> 01:19:46.100
 don't have much time. I'm gonna leave that for a later session anyway.

1401
01:19:47.100 --> 01:19:47.900
input dimensions

1402
01:19:48.600 --> 01:19:51.600
output Dimensions, so we actually create the first

1403
01:19:51.600 --> 01:19:53.600
 hidden layer and the first input layer in tensorflow.

1404
01:19:54.800 --> 01:19:55.500
in one go

1405
01:19:58.200 --> 01:19:59.000
Okay, so here.

1406
01:20:03.900 --> 01:20:06.200
This is the first input layer and the head

1407
01:20:06.200 --> 01:20:09.400
 layer in one go. So the input Dimensions the number of incoming.

1408
01:20:10.400 --> 01:20:13.300
features and the number of perceptions

1409
01:20:16.600 --> 01:20:19.200
activation functional architecture. I'm really sorry about this.

1410
01:20:19.200 --> 01:20:22.300
 I don't mind it's very sensitive activation function. Like I told

1411
01:20:22.300 --> 01:20:25.100
 you is how you process the signal.

1412
01:20:28.200 --> 01:20:29.800
the value the rectified

1413
01:20:33.800 --> 01:20:35.400
linear something. I forgot the name.

1414
01:20:36.300 --> 01:20:37.500
is used

1415
01:20:38.400 --> 01:20:39.300
is actually a very

1416
01:20:41.900 --> 01:20:43.700
effective activation function

1417
01:20:45.300 --> 01:20:48.200
But in order for me to show you why it's affected by we have to leave that

1418
01:20:48.200 --> 01:20:49.000
 for a separate session.

1419
01:20:50.200 --> 01:20:53.400
Well, let me just run through this as quickly as I can. So the first

1420
01:20:55.700 --> 01:20:57.100
input slash hidden layer

1421
01:20:58.200 --> 01:20:59.200
And then the other hidden layers.

1422
01:21:00.300 --> 01:21:02.400
So this one has this one will receive.

1423
01:21:04.200 --> 01:21:07.500
A 50 inputs from the previous layer and output

1424
01:21:07.500 --> 01:21:08.400
 100 layers.

1425
01:21:09.200 --> 01:21:11.600
Why 100 because we want to learn as many weights as we can.

1426
01:21:12.600 --> 01:21:15.400
And then another hidden layer and then file the upper layer

1427
01:21:15.400 --> 01:21:17.200
 and here we can see we've set the output to 1.

1428
01:21:18.300 --> 01:21:21.200
Why 1 well because we want a single value. We want

1429
01:21:21.200 --> 01:21:21.800
 a single.

1430
01:21:22.500 --> 01:21:23.300
numeric

1431
01:21:23.900 --> 01:21:26.300
estimate for any given

1432
01:21:26.300 --> 01:21:26.600
 property

1433
01:21:29.200 --> 01:21:31.400
you can see here. We once we have

1434
01:21:32.100 --> 01:21:35.000
created a sequence of layers. We will then compile it.

1435
01:21:37.400 --> 01:21:38.000
then we have

1436
01:21:39.200 --> 01:21:43.000
a loss function again. The Lost function is how we evaluate the

1437
01:21:42.300 --> 01:21:45.700
 prediction quality of this model. We

1438
01:21:45.700 --> 01:21:47.800
 said that we want the MSU to converge to zero.

1439
01:21:49.200 --> 01:21:49.900
So if you see here.

1440
01:21:51.300 --> 01:21:51.900
When we train it.

1441
01:21:53.100 --> 01:21:56.700
It's a scientific notation. It says 0.00.

1442
01:21:58.100 --> 01:22:00.800
0 1 7 7 160

1443
01:22:02.100 --> 01:22:03.300
and as

1444
01:22:04.500 --> 01:22:05.700
it keeps on learning.

1445
01:22:07.300 --> 01:22:10.400
You can see the MSC starts to grow smaller. So

1446
01:22:10.400 --> 01:22:13.400
 you can see this and went from E minus

1447
01:22:13.400 --> 01:22:16.300
 zero fourth e minus zero five. So that's one more

1448
01:22:16.300 --> 01:22:17.100
 zero next to it.

1449
01:22:17.700 --> 01:22:20.700
So it's even closer and closer to zero every

1450
01:22:22.900 --> 01:22:25.200
Progression of time sometimes it will

1451
01:22:25.200 --> 01:22:26.500
 go up a little bit. But ultimately.

1452
01:22:27.800 --> 01:22:29.300
It should converge to zero.

1453
01:22:30.500 --> 01:22:33.200
If it does not conversion 0 and it's instead it

1454
01:22:33.200 --> 01:22:36.700
 diverges. Well, then there was something wrong with the code

1455
01:22:36.700 --> 01:22:36.900
 you've done.

1456
01:22:39.800 --> 01:22:40.100
Okay.

1457
01:22:41.500 --> 01:22:41.700
anyway

1458
01:22:43.800 --> 01:22:46.400
This is how we this is one metric by

1459
01:22:46.400 --> 01:22:49.500
 what you will determine the quality of the model. That's MSC.

1460
01:22:49.500 --> 01:22:52.400
 And then there's something called the optimizer algorithm

1461
01:22:52.400 --> 01:22:55.300
 the atom algorithm again. I'm not going to talk about it here. It's in

1462
01:22:55.300 --> 01:22:57.800
 that video. I told you about it has to do with gradient descent.

1463
01:23:00.800 --> 01:23:01.600
Let me scroll down some more.

1464
01:23:03.100 --> 01:23:06.300
So after we evaluate the metrics for the

1465
01:23:06.300 --> 01:23:06.400
 data.

1466
01:23:07.300 --> 01:23:10.200
We can see for the training data set. It is 0.0.

1467
01:23:10.900 --> 01:23:11.300
well

1468
01:23:12.200 --> 01:23:15.500
That's 5073. So it's very close to zero.

1469
01:23:15.500 --> 01:23:16.700
 This is a good this is a good model.

1470
01:23:18.400 --> 01:23:20.500
And for test study it's even smaller still.

1471
01:23:24.400 --> 01:23:26.100
For test it's even smaller still.

1472
01:23:27.200 --> 01:23:30.300
Have you as anyone heard of overtrained overfitting underfitting?

1473
01:23:37.200 --> 01:23:40.700
Anybody else if you've not heard of overfitting an underfitting

1474
01:23:40.700 --> 01:23:42.200
 type and in the chat in the window?

1475
01:23:47.400 --> 01:23:50.400
Now very quickly. The reason we use training test is

1476
01:23:50.400 --> 01:23:53.700
 to quickly identify over affirming

1477
01:23:53.700 --> 01:23:54.100
 underfitting.

1478
01:23:55.600 --> 01:23:58.300
I reason underfitting is when the

1479
01:23:58.300 --> 01:23:59.800
 model has not learned anything in particular.

1480
01:24:01.300 --> 01:24:04.500
And you can detect underfitting at the training stage.

1481
01:24:04.500 --> 01:24:06.000
 So if the number is high.

1482
01:24:08.600 --> 01:24:12.000
You don't even to proceed the test dataset.

1483
01:24:11.400 --> 01:24:13.100
 It's already bad.

1484
01:24:14.100 --> 01:24:17.500
The way you will detect a overfitting is where you have

1485
01:24:17.500 --> 01:24:18.200
 a good.

1486
01:24:19.900 --> 01:24:22.300
training metric

1487
01:24:23.300 --> 01:24:25.200
But then for the test metric it does worse.

1488
01:24:26.600 --> 01:24:29.500
So if your training metric is here, which is that's

1489
01:24:29.500 --> 01:24:32.100
 a good this is good and then it goes up. That's over a very

1490
01:24:33.300 --> 01:24:35.100
If it's high to start with that's underfitting.

1491
01:24:35.800 --> 01:24:37.700
So that's why we need to look at trade and test.

1492
01:24:40.300 --> 01:24:44.700
And unless we've done this we have created for ourselves a machine learning

1493
01:24:44.700 --> 01:24:45.300
 model they can predict.

1494
01:24:46.900 --> 01:24:48.400
You can see here using the predict method.

1495
01:24:53.600 --> 01:24:54.700
And then this is the final price.

1496
01:24:56.500 --> 01:24:57.600
of one particular property

1497
01:25:02.300 --> 01:25:05.000
So what we're going

1498
01:25:05.200 --> 01:25:08.400
 to do is I'm going to post this notebook with some

1499
01:25:08.400 --> 01:25:09.900
 additional comments in the dashboard.

1500
01:25:11.500 --> 01:25:14.600
And we want to actually know.

1501
01:25:16.800 --> 01:25:20.000
Before we get into our next session. I strongly recommend.

1502
01:25:20.900 --> 01:25:22.000
that if you're not familiar with

1503
01:25:23.200 --> 01:25:25.200
test dataset train dataset

1504
01:25:25.800 --> 01:25:28.300
Machinery metrics and Grading this in

1505
01:25:28.300 --> 01:25:29.300
 a particular that you watch that video.

1506
01:25:30.400 --> 01:25:30.800
Let me just

1507
01:25:32.500 --> 01:25:36.200
pull up the link very quickly. Although I'm gonna Post in the slack

1508
01:25:35.200 --> 01:25:36.300
 Channel.

1509
01:25:38.500 --> 01:25:40.900
The video is called mathematics for machine learning you may have.

1510
01:25:41.600 --> 01:25:42.500
attended the event

1511
01:25:45.400 --> 01:25:45.700
Okay.

1512
01:25:50.200 --> 01:25:53.300
And there are also timestamps so you can jump to the section that is

1513
01:25:53.300 --> 01:25:53.700
 relevant to you.

1514
01:26:09.000 --> 01:26:09.200
now

1515
01:26:12.400 --> 01:26:15.100
So please watch this it's only an hour. Although you may

1516
01:26:15.100 --> 01:26:16.400
 not even have to watch the entire hour.

1517
01:26:17.100 --> 01:26:20.000
Just have a look at that before you join the next session.

1518
01:26:22.000 --> 01:26:22.100
so

1519
01:26:23.800 --> 01:26:25.000
If I return to our agenda.

1520
01:26:26.700 --> 01:26:28.000
so now we've covered a

1521
01:26:29.300 --> 01:26:30.500
we've had the high level overview of

1522
01:26:31.400 --> 01:26:34.400
deep learning and it's infrastructure called new

1523
01:26:34.400 --> 01:26:36.800
 networks. We've talked about these three components.

1524
01:26:39.200 --> 01:26:42.700
You know next Workshop. I'm going to show you how to do PCA for.

1525
01:26:46.900 --> 01:26:48.700
a more useful data set and then

1526
01:26:50.200 --> 01:26:52.400
we will go ahead and use neural networks for

1527
01:26:54.400 --> 01:26:56.200
first regression and then

1528
01:26:56.900 --> 01:26:57.800
image classification

1529
01:26:59.200 --> 01:27:02.300
and our last session we can use we will look at neural networks

1530
01:27:02.300 --> 01:27:02.600
 for

1531
01:27:05.100 --> 01:27:06.900
sequential data

1532
01:27:07.900 --> 01:27:09.000
I think we have enough.

1533
01:27:09.600 --> 01:27:12.100
I think for the most part everyone's more comfortable with the

1534
01:27:12.100 --> 01:27:13.000
 mathematics so we can

1535
01:27:14.100 --> 01:27:17.300
Move more quickly, of course, I have other notebooks.

1536
01:27:18.600 --> 01:27:21.000
There are many other examples of Google networks that you know,

1537
01:27:21.900 --> 01:27:24.100
 we won't be able to look at because there's just not enough time, but I'm

1538
01:27:24.100 --> 01:27:27.200
 gonna include them as examples and they will all be accompanied by

1539
01:27:27.200 --> 01:27:27.600
 comments.

1540
01:27:28.400 --> 01:27:29.700
so you can watch those and then

1541
01:27:31.400 --> 01:27:31.600
you know.

1542
01:27:35.100 --> 01:27:38.700
Use those as reference to help you solve the assignment for

1543
01:27:38.700 --> 01:27:39.300
 module 4.

1544
01:27:40.500 --> 01:27:42.500
Okay, and by the way speaking of assignments.

1545
01:27:43.300 --> 01:27:46.100
The assignments from module two and three will be put on the dashboard today.

1546
01:27:48.200 --> 01:27:51.400
And the assignment for module 4 at the end of module 4.

1547
01:27:52.400 --> 01:27:55.500
So you'll have one week after the end of module 4 to complete the

1548
01:27:55.500 --> 01:27:55.800
 assignment.

1549
01:27:58.900 --> 01:27:59.000
Okay.

1550
01:28:00.600 --> 01:28:02.000
Any quest for comments, by the way?

1551
01:28:03.400 --> 01:28:06.400
About today's session about the subsequent sessions about

1552
01:28:06.400 --> 01:28:08.200
 the previous sessions or anything in particular.

1553
01:28:09.700 --> 01:28:10.600
You see there's so much.

1554
01:28:11.800 --> 01:28:12.600
topics to cover

1555
01:28:13.600 --> 01:28:16.000
I in this session the past session I don't

1556
01:28:16.200 --> 01:28:18.100
 even have time to

1557
01:28:20.300 --> 01:28:22.100
You know make it to give you an exercise.

1558
01:28:23.400 --> 01:28:26.200
So I want to at least use this minute to answer any questions

1559
01:28:26.200 --> 01:28:26.500
 you have.

1560
01:28:27.600 --> 01:28:30.900
Anything that you would like to be covered anything that

1561
01:28:30.900 --> 01:28:32.300
 has but there is of interest to you.

1562
01:28:36.900 --> 01:28:39.600
I'm just auditing and then oh, okay. So

1563
01:28:39.600 --> 01:28:41.500
 the access to the dashboard thing is automated.

1564
01:28:42.700 --> 01:28:45.700
So if you were not verified.

1565
01:28:54.500 --> 01:28:57.800
You would need to yeah, you would need to be verified to access

1566
01:28:57.800 --> 01:28:58.400
 it. Okay?

1567
01:29:01.700 --> 01:29:04.400
I'm going to ask the yeah, I'm

1568
01:29:04.400 --> 01:29:04.900
 gonna ask.

1569
01:29:06.900 --> 01:29:09.200
You might you should have everyone should have

1570
01:29:09.200 --> 01:29:12.400
 received an email from support about upgrading this certification.

1571
01:29:13.200 --> 01:29:16.300
If you did not receive it or if what I'll

1572
01:29:16.300 --> 01:29:16.900
 do is I'll ask.

1573
01:29:18.100 --> 01:29:19.700
And the organizer to send another email.

1574
01:29:20.500 --> 01:29:22.800
And then she'll follow up with you.

1575
01:29:24.600 --> 01:29:24.800
Okay.

1576
01:29:26.600 --> 01:29:29.200
But yeah, if you do think of if there's something that

1577
01:29:29.200 --> 01:29:29.600
 you want to cover.

1578
01:29:31.600 --> 01:29:34.000
That is not part of the syllabus or something. We haven't

1579
01:29:34.300 --> 01:29:34.900
 covered up this point.

1580
01:29:35.900 --> 01:29:38.400
Let me know because I I want to

1581
01:29:38.400 --> 01:29:41.500
 make the you know, the curriculum as an

1582
01:29:41.500 --> 01:29:42.500
 interesting to you as possible.

1583
01:29:43.500 --> 01:29:45.000
Okay, let me just post this link.

1584
01:29:48.300 --> 01:29:51.100
And yes, so, please watch this video before the next session.

1585
01:29:53.600 --> 01:29:54.300
Thank you for joining me.

1586
01:29:58.500 --> 01:30:02.300
And I hope to see you on Wednesday everybody.

1587
01:30:01.300 --> 01:30:02.900
 Thank you so much.

1588
01:30:04.200 --> 01:30:05.000
For now. Take care. Bye.
