WEBVTT - This file was automatically generated by VIMEO

0
00:00:00.200 --> 00:00:02.700
So we said deep learning is our attempt to replicate.

1
00:00:03.600 --> 00:00:04.400
the human brain

2
00:00:07.300 --> 00:00:08.400
of course the brain that

3
00:00:09.100 --> 00:00:12.100
we have understood up to this point in time.

4
00:00:12.800 --> 00:00:15.800
Let's first start with the most Atomic the most

5
00:00:15.800 --> 00:00:18.500
 basic component of a new life so up

6
00:00:18.500 --> 00:00:19.000
 here we have

7
00:00:20.200 --> 00:00:24.100
a diagram or a pectoral model of a

8
00:00:23.100 --> 00:00:24.900
 neuron

9
00:00:27.400 --> 00:00:30.000
so this is the most building block basic building block or

10
00:00:32.900 --> 00:00:35.200
in organic component of the

11
00:00:36.600 --> 00:00:39.700
of the human brain, right? We don't want to go down to subatomic particles

12
00:00:39.700 --> 00:00:40.200
 and that kind of thing.

13
00:00:41.400 --> 00:00:43.600
So this is a neuron it has these.

14
00:00:45.600 --> 00:00:48.600
Let's say branches called the dendrites that

15
00:00:48.600 --> 00:00:49.700
 receives signals.

16
00:00:50.600 --> 00:00:52.700
electrical chemical signals from other neurons

17
00:00:54.200 --> 00:00:57.100
and these signals travel down these dendrites.

18
00:00:57.900 --> 00:01:00.300
And then they are processed by this thing called a nucleus.

19
00:01:02.600 --> 00:01:04.400
If the signal is strong enough.

20
00:01:05.700 --> 00:01:08.400
Then another signal will travel down this Axon.

21
00:01:10.200 --> 00:01:11.200
and out to these other

22
00:01:12.100 --> 00:01:12.700
terminals

23
00:01:13.800 --> 00:01:16.800
and it is at these terminals where we

24
00:01:16.800 --> 00:01:18.200
 have connections with other neurons.

25
00:01:19.400 --> 00:01:22.200
So the way we are able to learn things and remember things is

26
00:01:22.200 --> 00:01:24.400
 by these electrochemical signals.

27
00:01:25.200 --> 00:01:26.100
firing off

28
00:01:26.800 --> 00:01:27.600
all of our brain

29
00:01:28.500 --> 00:01:31.500
and so we have these neurons come together to form thoughts to

30
00:01:31.500 --> 00:01:33.400
 form memories to form ideas.

31
00:01:34.700 --> 00:01:34.900
Okay.

32
00:01:38.800 --> 00:01:42.100
Well, you see here down here is a very abstract diagrammatical

33
00:01:41.100 --> 00:01:43.400
 representation of the above.

34
00:01:45.100 --> 00:01:47.700
But we want to do is a mathematically model this neuron.

35
00:01:48.600 --> 00:01:50.500
And deep learning we call the neuron.

36
00:01:51.300 --> 00:01:52.400
the perceptron

37
00:01:53.700 --> 00:01:56.600
So in our brain we have neurons in machine deep

38
00:01:56.600 --> 00:01:59.200
 learning we have these things called perceptrons which are supposed to

39
00:01:59.200 --> 00:01:59.200
 be.

40
00:02:00.700 --> 00:02:01.400
replicas

41
00:02:02.700 --> 00:02:04.700
Of the neuron that's a perceptron.

42
00:02:06.200 --> 00:02:08.200
so the dendrites

43
00:02:10.500 --> 00:02:13.300
here are the inputs. What are the inputs?

44
00:02:15.700 --> 00:02:17.500
The inputs are the dimensions.

45
00:02:18.300 --> 00:02:19.800
Or The Columns of the data.

46
00:02:20.900 --> 00:02:23.400
If you if you think for example about

47
00:02:23.400 --> 00:02:26.100
 an Excel sheet I can use that real estate example.

48
00:02:27.400 --> 00:02:30.500
Size of the property distance to the city center number

49
00:02:30.500 --> 00:02:31.100
 of bedrooms.

50
00:02:33.600 --> 00:02:35.400
These are all columns right column vectors.

51
00:02:37.300 --> 00:02:39.000
And each column Vector is an input.

52
00:02:40.700 --> 00:02:43.300
And what's the circle that circle is the well, I'll

53
00:02:43.300 --> 00:02:46.300
 show you what the circle is. It's a combination of two things, but it will

54
00:02:46.300 --> 00:02:47.100
 be processed here.

55
00:02:48.500 --> 00:02:49.300
and then one

56
00:02:51.200 --> 00:02:53.500
value or one vector will be produced.

57
00:02:54.300 --> 00:02:54.700
from this

58
00:02:56.200 --> 00:02:59.800
N number of vectors, so it's a linear transformation in other words and then

59
00:02:59.800 --> 00:03:02.300
 here at the axon you have the option of either

60
00:03:02.300 --> 00:03:05.300
 outputting one vector or branching out to three

61
00:03:05.300 --> 00:03:06.100
 why?

62
00:03:07.100 --> 00:03:10.200
Do we want to have one of the other world depends on how we have struck to

63
00:03:10.200 --> 00:03:11.000
 do neural network?

64
00:03:12.700 --> 00:03:16.500
Let's look at a more mathematical representation of this model and

65
00:03:15.500 --> 00:03:17.000
 it's this.

66
00:03:25.400 --> 00:03:28.000
here we have x sub 1 all the way to x sub n

67
00:03:30.900 --> 00:03:33.500
where X for each x sub I

68
00:03:34.400 --> 00:03:35.800
represents a component

69
00:03:36.800 --> 00:03:39.000
of some Vector x sub I

70
00:03:40.800 --> 00:03:42.900
we notice the excess here are I thought

71
00:03:44.300 --> 00:03:46.400
so again to use a real estate example.

72
00:03:47.300 --> 00:03:50.200
let's say x the vector x sub 1

73
00:03:51.200 --> 00:03:51.600
is the

74
00:03:55.600 --> 00:03:56.400
number of bedrooms

75
00:03:57.800 --> 00:04:00.500
and then x sub 1 could be three bedrooms.

76
00:04:02.200 --> 00:04:06.100
Then the vector X up 2 could be the size of the property in

77
00:04:05.100 --> 00:04:07.200
 square feet.

78
00:04:08.100 --> 00:04:08.600
So this will be

79
00:04:09.900 --> 00:04:10.900
2,000 square feet

80
00:04:12.300 --> 00:04:15.300
Right, so this could be three three bedrooms two thousand

81
00:04:15.300 --> 00:04:15.600
 square feet.

82
00:04:16.500 --> 00:04:17.900
So basically all of the

83
00:04:20.200 --> 00:04:22.400
values of the first property

84
00:04:24.900 --> 00:04:27.500
then we have these W. Is anyone know what these W's are?

85
00:04:29.700 --> 00:04:31.400
Any guesses what w stands for?

86
00:04:34.600 --> 00:04:36.700
Precisely the weights right? We said that

87
00:04:37.700 --> 00:04:40.400
these are all factors that influence the

88
00:04:40.400 --> 00:04:41.200
 price of the property.

89
00:04:41.900 --> 00:04:42.100
but

90
00:04:43.700 --> 00:04:47.500
the number of bedroom the size of the apartment the

91
00:04:46.500 --> 00:04:47.900
 floor.

92
00:04:50.400 --> 00:04:51.300
Of the apartment.

93
00:04:52.700 --> 00:04:54.800
Whether it has a balcony or not.

94
00:04:55.700 --> 00:04:58.200
It's proximity to the city center. These are

95
00:04:58.200 --> 00:05:01.200
 all important factors, but some are more important

96
00:05:01.200 --> 00:05:01.700
 than others.

97
00:05:02.500 --> 00:05:05.200
Right. So for example being close to city center

98
00:05:05.200 --> 00:05:06.200
 is the most important factor.

99
00:05:06.900 --> 00:05:09.300
So for example, if x sub 3 is the proximity to

100
00:05:09.300 --> 00:05:11.800
 the city center, then it will have a much higher weight.

101
00:05:12.800 --> 00:05:15.200
Whereas let's say x sub five, which

102
00:05:15.200 --> 00:05:15.500
 is the

103
00:05:20.200 --> 00:05:21.400
I don't know the lighting system, right?

104
00:05:22.600 --> 00:05:23.400
if it's

105
00:05:25.100 --> 00:05:27.000
if the use fluorescent instead of LED

106
00:05:28.600 --> 00:05:31.300
It's a very insignificant. It's something you can change, you

107
00:05:31.300 --> 00:05:34.500
 know in an hour or two, but let's say for example

108
00:05:34.500 --> 00:05:35.100
 the factor.

109
00:05:37.200 --> 00:05:40.500
It's it's it will have a much smaller weight.

110
00:05:42.200 --> 00:05:45.600
What you do then is you take the product of each X and

111
00:05:45.600 --> 00:05:46.200
 its weight.

112
00:05:48.300 --> 00:05:49.200
And then you sum.

113
00:05:50.300 --> 00:05:53.300
So you have product product and then some of the products?

114
00:05:54.200 --> 00:05:55.300
That's the summation here.

115
00:05:56.400 --> 00:05:59.200
And then that value that you will get will be the activation.

116
00:05:59.900 --> 00:06:00.400
now

117
00:06:03.200 --> 00:06:06.100
this activation will be fed into what we call an activation function.

118
00:06:07.900 --> 00:06:10.000
and an activation function is basically

119
00:06:10.900 --> 00:06:13.400
a function that will determine

120
00:06:13.400 --> 00:06:16.200
 the threshold of the signals if the signal is strong enough.

121
00:06:16.200 --> 00:06:18.900
 We'll pass it on to the next.

122
00:06:20.300 --> 00:06:23.200
perceptron otherwise

123
00:06:24.400 --> 00:06:26.500
We will not send any signal we send the zero.

124
00:06:28.300 --> 00:06:32.000
For example, why let me give you an example why you would

125
00:06:31.200 --> 00:06:34.000
 need to choose an activation function why you would need to

126
00:06:34.200 --> 00:06:35.900
 process a signal or not?

127
00:06:37.500 --> 00:06:39.000
Imagine you're looking at a picture.

128
00:06:40.800 --> 00:06:43.300
and let me pull up a picture here picture of a

129
00:06:46.000 --> 00:06:46.500
character

130
00:06:55.300 --> 00:06:56.600
any character doesn't matter.

131
00:06:57.300 --> 00:07:00.100
And we want to know

132
00:07:00.100 --> 00:07:00.900
 what is the number?

133
00:07:01.700 --> 00:07:02.400
in this picture

134
00:07:04.200 --> 00:07:07.800
the way we're going the way the model is going to look at this picture is

135
00:07:07.800 --> 00:07:10.400
 to go through the entire picture and of

136
00:07:10.400 --> 00:07:12.200
 course if you remember from session,

137
00:07:14.300 --> 00:07:17.500
I think our very first session we said that pictures will

138
00:07:17.500 --> 00:07:19.800
 be represented as a matrices, right? This will be a matrix.

139
00:07:20.800 --> 00:07:23.200
And then we said a white pixel is 255.

140
00:07:23.900 --> 00:07:25.400
And a black pixel is zero.

141
00:07:26.500 --> 00:07:28.900
So if we for example look at this.

142
00:07:32.400 --> 00:07:34.000
Section of the picture because we have to go.

143
00:07:34.900 --> 00:07:36.100
from one corner to the next

144
00:07:37.300 --> 00:07:38.700
and process the picture

145
00:07:40.100 --> 00:07:41.900
one super pixel at a time

146
00:07:42.700 --> 00:07:44.300
a super pixel of the collection of pixels

147
00:07:45.300 --> 00:07:47.300
If we look at this super pixel, everything is white.

148
00:07:48.400 --> 00:07:49.100
There's nothing of

149
00:07:50.400 --> 00:07:53.000
there's nothing here. So it's activation will be zero.

150
00:07:53.900 --> 00:07:54.900
There's nothing worthwhile.

151
00:07:55.600 --> 00:07:57.500
However, if we make our way here.

152
00:08:00.700 --> 00:08:03.200
Well now we will have a positive activation.

153
00:08:04.700 --> 00:08:07.100
Because we have a line. That's the idea. That's the purpose of

154
00:08:07.100 --> 00:08:08.000
 this activation function.

155
00:08:11.700 --> 00:08:14.300
and when we use this activation, we

156
00:08:14.300 --> 00:08:16.200
 will be able to narrow down our

157
00:08:19.800 --> 00:08:21.500
we will have another linear transformation.

158
00:08:23.200 --> 00:08:26.400
We will only have the number four and you will

159
00:08:26.400 --> 00:08:28.200
 discard the area around.

160
00:08:29.800 --> 00:08:32.500
And then later on what we will do is we will flatten this

161
00:08:32.500 --> 00:08:34.200
 Matrix. We'll have another linear transformation.

162
00:08:36.500 --> 00:08:38.400
Where we'll go for example from a 16 by 16.

163
00:08:39.500 --> 00:08:40.400
to a

164
00:08:41.800 --> 00:08:42.600
single vector

165
00:08:48.200 --> 00:08:49.000
So 32.

166
00:08:49.800 --> 00:08:51.300
1 by 32 vector

167
00:08:54.300 --> 00:08:57.300
Now why why do we need to do this new transformation we'll come

168
00:08:57.300 --> 00:09:00.000
 back to to the state has to do with distributions. We will leave that

169
00:09:00.200 --> 00:09:00.300
 for later.

170
00:09:01.200 --> 00:09:04.700
Well what I want us to First understand is this the main

171
00:09:04.700 --> 00:09:07.200
 component of the neural network perceptron.

172
00:09:07.800 --> 00:09:10.200
Now why is it called now work? Well, because it's a network of

173
00:09:10.200 --> 00:09:10.900
 perceptrons.

174
00:09:12.700 --> 00:09:13.300
if you take

175
00:09:14.300 --> 00:09:14.600
those

176
00:09:15.800 --> 00:09:18.400
and I think I was demonstrating something. That's

177
00:09:18.400 --> 00:09:20.100
 why I have this here and give it up these.

178
00:09:21.400 --> 00:09:22.100
Sorry about that.

179
00:09:26.200 --> 00:09:26.800
if we take

180
00:09:29.100 --> 00:09:32.700
Those perceptions and put them into an hour. We will get a neural network.

181
00:09:32.700 --> 00:09:34.200
 So all of these circles that you see here.

182
00:09:35.200 --> 00:09:38.300
These are all perceptron perceptron all the

183
00:09:38.300 --> 00:09:38.700
 perceptrons.

184
00:09:40.600 --> 00:09:43.100
It's called Deep learning because you know, we have

185
00:09:43.100 --> 00:09:44.300
 these layers.

186
00:09:46.300 --> 00:09:48.100
And they can if we stuck.

187
00:09:49.400 --> 00:09:53.300
Enough of these layers will have a deep network of perceptrons.

188
00:09:52.300 --> 00:09:54.800
 So that's what's called Deep learning.

189
00:10:00.800 --> 00:10:03.500
And this is an example of a fully connected Network

190
00:10:03.500 --> 00:10:06.500
 and it's called fully connected because every perceptual is

191
00:10:06.500 --> 00:10:09.600
 can it's connected every other perceptron in the

192
00:10:09.600 --> 00:10:10.100
 subsequent Network.

193
00:10:12.700 --> 00:10:15.400
Run us are all the leaders in this example are then

194
00:10:15.400 --> 00:10:17.900
 yes. See this densely. There's all that's correct.

195
00:10:25.500 --> 00:10:28.400
now in a neural network

196
00:10:29.400 --> 00:10:31.500
and a fully connected Network now there are different.

197
00:10:32.200 --> 00:10:35.300
Combinations and why you would choose one

198
00:10:35.300 --> 00:10:36.500
 combination versus the next?

199
00:10:39.900 --> 00:10:42.600
We'll have to do with the nature of data. Is it pictorial?

200
00:10:42.600 --> 00:10:45.600
 Is it sequential like if you want to do Time series?

201
00:10:46.400 --> 00:10:49.200
Time series is where for example like

202
00:10:49.200 --> 00:10:50.900
 stocks right the price of the stock.

203
00:10:51.700 --> 00:10:52.600
Goes up and down.

204
00:10:54.600 --> 00:10:55.600
or the weather

205
00:10:57.300 --> 00:10:59.400
You want to see why does the weather go up or down?

206
00:11:00.600 --> 00:11:03.300
And if you could predict it for the next

207
00:11:03.300 --> 00:11:04.100
 month or the next?

208
00:11:05.100 --> 00:11:05.400
week

209
00:11:06.600 --> 00:11:09.600
well, the change in temperature is associated with data is

210
00:11:09.600 --> 00:11:10.600
 associated with the

211
00:11:12.500 --> 00:11:15.600
Calendar right obviously several it's warmer when

212
00:11:15.600 --> 00:11:16.000
 there is cooler.

213
00:11:17.600 --> 00:11:20.600
or another example is against speech to

214
00:11:20.600 --> 00:11:21.100
 text or

215
00:11:23.500 --> 00:11:26.400
Other complete, you know, when you go to Gmail and you type in an

216
00:11:26.400 --> 00:11:28.800
 email say for example, hello, Ron.

217
00:11:30.900 --> 00:11:34.200
Please see thee and then Gmail completely remaining

218
00:11:33.200 --> 00:11:35.700
 sentence. It was a CD attached.

219
00:11:36.200 --> 00:11:38.400
It will like fill in the blank. It will complete your sense.

220
00:11:39.200 --> 00:11:40.700
That is also accomplished using.

221
00:11:44.100 --> 00:11:45.300
a sequential Network

222
00:11:46.800 --> 00:11:49.400
So again, I'll show you different Arrangements. I'll show their pictures

223
00:11:49.400 --> 00:11:49.800
 of them, but

224
00:11:50.500 --> 00:11:52.200
there are different ways we can assemble this.

225
00:11:54.100 --> 00:11:57.400
But here again the if you see here, we have the input layer then we

226
00:11:57.400 --> 00:11:57.400
 have.

227
00:11:58.300 --> 00:12:01.100
A series of hidden layers and then finally an apple layer.

228
00:12:01.900 --> 00:12:04.700
Now why this is how four for

229
00:12:04.700 --> 00:12:07.500
 example, let's say you are doing again one more

230
00:12:07.500 --> 00:12:10.300
 time. I'll do the real estate example. You have all

231
00:12:10.300 --> 00:12:12.300
 of the features of the property.

232
00:12:13.100 --> 00:12:16.300
And then you're going to learn every single hidden layer

233
00:12:16.300 --> 00:12:16.500
 in here.

234
00:12:17.500 --> 00:12:18.700
Is going to learn a weight.

235
00:12:19.600 --> 00:12:22.400
Basically on one thing I forgot to mention here.

236
00:12:23.800 --> 00:12:26.500
These weights are randomly assigned.

237
00:12:28.100 --> 00:12:29.200
Because the model does not know.

238
00:12:30.400 --> 00:12:33.400
The significance of the number of bedrooms versus the

239
00:12:33.400 --> 00:12:36.400
 significance of the proximity to the city center to

240
00:12:36.400 --> 00:12:38.200
 the model. These are all numbers that mean nothing.

241
00:12:38.800 --> 00:12:41.800
So it will first randomly assign weights and

242
00:12:41.800 --> 00:12:43.700
 then using a what we call.

243
00:12:44.500 --> 00:12:45.900
back propagation

244
00:12:47.500 --> 00:12:49.100
and a technique in

245
00:12:50.700 --> 00:12:53.400
Multiverse calculus called gradient descent. It

246
00:12:53.400 --> 00:12:54.200
 will adjust the weights.

247
00:12:55.300 --> 00:12:58.300
Basically, it will make one round of prediction

248
00:12:58.300 --> 00:12:59.800
 compare it to a sample.

249
00:13:02.200 --> 00:13:05.300
And then it will adjust the weights because if you remember we

250
00:13:05.300 --> 00:13:08.700
 talked about MSC in our last Workshop. We

251
00:13:08.700 --> 00:13:11.400
 said that we want MSD conversions you have to convergence

252
00:13:11.400 --> 00:13:14.200
 is zero, then what the model predicts and what

253
00:13:14.200 --> 00:13:17.700
 the actual prices are will be equivalent. If

254
00:13:17.700 --> 00:13:20.300
 the errors are 0 they will be equivalent and that's what

255
00:13:20.300 --> 00:13:21.000
 we want ideally.

256
00:13:22.500 --> 00:13:25.200
Initially the model when we first feed the data to the

257
00:13:25.200 --> 00:13:26.800
 network the MSC will be large.

258
00:13:27.500 --> 00:13:30.400
But the model will keep looking at the sample again and again

259
00:13:30.400 --> 00:13:32.500
 and again and adjust the weights.

260
00:13:33.200 --> 00:13:36.300
I adjust the weight again and again and again and again until it has

261
00:13:36.300 --> 00:13:39.400
 understood why one feature is important by other features not

262
00:13:39.400 --> 00:13:39.500
 important.

263
00:13:40.500 --> 00:13:43.400
Now again, if you're just trying to predict the

264
00:13:43.400 --> 00:13:45.400
 number you'll have only one output.

265
00:13:46.200 --> 00:13:47.100
Which will be the price.

266
00:13:48.500 --> 00:13:50.800
if however we're doing something like

267
00:13:53.100 --> 00:13:55.300
see RNN

268
00:13:55.900 --> 00:13:58.500
a convolutional new network where you're

269
00:13:58.500 --> 00:14:00.200
 going to go from a matrix data.

270
00:14:01.600 --> 00:14:02.600
to a

271
00:14:03.500 --> 00:14:06.600
to a Vertex to a vector

272
00:14:07.700 --> 00:14:10.200
if you're going to be doing this transformation, this is a linear

273
00:14:10.200 --> 00:14:13.200
 transformation. Then what what you will append at

274
00:14:13.200 --> 00:14:16.400
 the end of the outpole layer could be another Network. You can actually mix

275
00:14:16.400 --> 00:14:19.600
 two networks together especially for complex problems.
