WEBVTT - This file was automatically generated by VIMEO

0
00:00:00.200 --> 00:00:01.200
What is a linear algebra?

1
00:00:01.700 --> 00:00:02.700
linear algebra

2
00:00:03.400 --> 00:00:05.100
is actually a subset.

3
00:00:06.100 --> 00:00:06.300
of

4
00:00:07.300 --> 00:00:10.200
abstract algebra, which I'm going to be talking about

5
00:00:10.200 --> 00:00:10.500
 today.

6
00:00:12.200 --> 00:00:14.500
So even though this code is called linear algebra.

7
00:00:15.800 --> 00:00:18.300
We really can't have a meaningful discussion.

8
00:00:19.100 --> 00:00:19.900
If we are not.

9
00:00:20.900 --> 00:00:23.300
Speaking the same language or if we

10
00:00:23.300 --> 00:00:24.800
 do not understand each other.

11
00:00:25.600 --> 00:00:25.800
because

12
00:00:27.300 --> 00:00:30.400
In the context of linear algebra. We are going to use certain words

13
00:00:30.400 --> 00:00:31.300
 certain terms.

14
00:00:32.300 --> 00:00:34.900
And unless we know those terms then you know.

15
00:00:37.200 --> 00:00:40.100
We're not going to make much progress. Now. Where do we

16
00:00:40.100 --> 00:00:41.600
 find the definitions for those terms?

17
00:00:42.200 --> 00:00:43.300
abstract algebra

18
00:00:44.200 --> 00:00:45.700
So they are part of each other.

19
00:00:47.200 --> 00:00:49.700
Okay linear algebra, like I said is a subset.

20
00:00:50.300 --> 00:00:53.400
Of abstract algebra in other words if you learn abstract algebra,

21
00:00:53.400 --> 00:00:55.100
 you're on your weight. You're learning linearity.

22
00:00:56.100 --> 00:00:58.400
Now in this course, there are two major components.

23
00:01:00.700 --> 00:01:03.400
There are two things that we are going to try to cover in depth.

24
00:01:04.600 --> 00:01:05.000
the first

25
00:01:06.400 --> 00:01:08.900
category is the systems of linear equations.

26
00:01:11.500 --> 00:01:12.300
one of the very

27
00:01:14.900 --> 00:01:16.700
impressive things in linear algebra

28
00:01:17.600 --> 00:01:19.500
Is that it gives us techniques?

29
00:01:20.800 --> 00:01:24.000
For solving multiple equations at

30
00:01:23.500 --> 00:01:26.500
 the same time. It allows us to find the

31
00:01:26.500 --> 00:01:26.800
 roots.

32
00:01:27.800 --> 00:01:29.400
of multiple equations at the same time

33
00:01:30.500 --> 00:01:31.800
why is that important well and

34
00:01:32.600 --> 00:01:36.100
you know, what's the point of what's so important about

35
00:01:35.100 --> 00:01:37.300
 finding roots of an equation?

36
00:01:38.900 --> 00:01:41.100
I think I'll come back to that answer once.

37
00:01:42.100 --> 00:01:45.200
We go to mark down because I want to show

38
00:01:45.200 --> 00:01:46.900
 you some math expressions.

39
00:01:47.900 --> 00:01:49.700
Okay, but equations.

40
00:01:50.900 --> 00:01:53.100
Which can sometimes stand for models?

41
00:01:54.600 --> 00:01:56.100
And I hope you know that models.

42
00:01:56.900 --> 00:02:00.600
Are how we mathematically represent some

43
00:01:59.600 --> 00:02:03.000
 thing in the real world by

44
00:02:02.600 --> 00:02:04.700
 finding the roots of an equation?

45
00:02:06.300 --> 00:02:09.400
And if that occasion happens to me a model we would

46
00:02:09.400 --> 00:02:10.900
 have found the essence of that model.

47
00:02:12.900 --> 00:02:15.200
And with linear algebra, we were able to do this

48
00:02:15.200 --> 00:02:19.600
 for a seat like a sequencer equations. So

49
00:02:19.600 --> 00:02:22.200
 that's the first one. Okay again, I'm like we always do

50
00:02:22.200 --> 00:02:25.600
 in every session we start at the high level and then delve

51
00:02:25.600 --> 00:02:26.800
 deeper right because we won't have a

52
00:02:27.600 --> 00:02:28.200
a picture

53
00:02:29.100 --> 00:02:29.300
of

54
00:02:31.100 --> 00:02:31.600
the discussion

55
00:02:33.600 --> 00:02:35.400
the second part is Matrix operations.

56
00:02:37.100 --> 00:02:40.100
This will be the bulk of the course and for that matter

57
00:02:40.100 --> 00:02:43.500
 this this encompasses the bulk of any linear algebra

58
00:02:43.500 --> 00:02:43.600
 course.

59
00:02:44.800 --> 00:02:48.000
So I'll tell you what the subsets are like what

60
00:02:47.200 --> 00:02:50.000
 roughly goes into each category.

61
00:02:50.800 --> 00:02:53.400
But here are the major components

62
00:02:53.400 --> 00:02:56.300
 that will be covered in both categories systems of

63
00:02:56.300 --> 00:02:59.600
 linear equation. Like I said allows you to or the set

64
00:02:59.600 --> 00:03:02.300
 of techniques that allow us to solve multiple equations at the

65
00:03:02.300 --> 00:03:02.500
 same time.

66
00:03:04.700 --> 00:03:05.100
and

67
00:03:07.300 --> 00:03:10.100
we are going to we will achieve this.

68
00:03:11.200 --> 00:03:12.600
by performing

69
00:03:15.400 --> 00:03:17.900
arithmetic operations on multiple rows

70
00:03:19.300 --> 00:03:22.600
and what we want to do is eventually get to something called reduced

71
00:03:22.600 --> 00:03:24.800
 Echelon form or reduced row Echelon form.

72
00:03:28.100 --> 00:03:31.300
Another part of another topic. Another thing

73
00:03:31.300 --> 00:03:31.900
 that we will learn is

74
00:03:32.600 --> 00:03:33.700
Lu decomposition

75
00:03:35.100 --> 00:03:37.200
You know the lower triangle upper triangle the composition.

76
00:03:39.100 --> 00:03:42.000
and when it comes to Matrix operations we have

77
00:03:42.700 --> 00:03:43.200
a set of

78
00:03:44.100 --> 00:03:45.800
special types of matrices

79
00:03:46.700 --> 00:03:49.700
a very important one is a is the

80
00:03:49.700 --> 00:03:52.300
 square Matrix and the diagonal matrix.

81
00:03:53.300 --> 00:03:56.500
These types of matrices especially the

82
00:03:56.500 --> 00:03:59.500
 square Matrix is going to be very Central to the entire

83
00:03:59.500 --> 00:04:02.500
 course because Square matrices of applications

84
00:04:02.500 --> 00:04:04.300
 and neural networks.

85
00:04:05.300 --> 00:04:05.600
in

86
00:04:07.400 --> 00:04:10.700
statistics if you've heard of things like covariance and

87
00:04:10.700 --> 00:04:11.600
 correlation matrices

88
00:04:13.400 --> 00:04:15.600
They have applications. Oh.

89
00:04:16.200 --> 00:04:18.000
principal component analysis, which is

90
00:04:20.200 --> 00:04:21.100
let's say our

91
00:04:22.600 --> 00:04:25.500
what's the word I'm looking for it is

92
00:04:25.500 --> 00:04:28.300
 where we are headed towards so well, and actually it's

93
00:04:28.300 --> 00:04:31.800
 the slide that I want to come to so principle component analysis now,

94
00:04:31.800 --> 00:04:34.100
 it's not easy for me

95
00:04:34.100 --> 00:04:34.300
 to

96
00:04:38.200 --> 00:04:40.900
like going to PC at least not not just yet.

97
00:04:41.500 --> 00:04:41.700
but

98
00:04:43.800 --> 00:04:46.700
If we are able to find the principal

99
00:04:46.700 --> 00:04:47.300
 components.

100
00:04:48.500 --> 00:04:49.000
of a matrix

101
00:04:50.100 --> 00:04:54.000
That's a big deal. We can do things like pattern recognition whether

102
00:04:53.600 --> 00:04:57.000
 it's from pictures or tabular

103
00:04:56.300 --> 00:04:59.500
 data or sound bites.

104
00:05:00.300 --> 00:05:03.300
We it is used in stock price prediction. So

105
00:05:03.300 --> 00:05:06.800
 if any of you are interested in quantitative Finance PC is

106
00:05:06.800 --> 00:05:09.900
 actually again a very big component of

107
00:05:12.100 --> 00:05:15.800
A quantitative Finance optimization optimization is

108
00:05:15.800 --> 00:05:19.400
 is a subset of linear or a super

109
00:05:18.400 --> 00:05:21.100
 set of linear program which will talk about

110
00:05:21.100 --> 00:05:25.200
 today but PCA will give you a powerful ability

111
00:05:24.200 --> 00:05:27.400
 in many different domains

112
00:05:27.400 --> 00:05:28.600
 that concerns data.

113
00:05:31.800 --> 00:05:34.000
Now in order for us to even be able to comprehend.

114
00:05:37.400 --> 00:05:40.600
How PCA works and how you're able to extract the

115
00:05:40.600 --> 00:05:43.400
 principle components from a matrix or from

116
00:05:43.400 --> 00:05:43.700
 our data?

117
00:05:44.700 --> 00:05:46.400
We have this layer.

118
00:05:47.900 --> 00:05:49.600
these layers of prerequisites

119
00:05:51.900 --> 00:05:54.200
So for example PCA is a subset or

120
00:05:54.200 --> 00:05:57.400
 a subtype of singular value decomposition.

121
00:05:58.900 --> 00:06:01.100
So in order for us to understand PC, we

122
00:06:01.100 --> 00:06:04.000
 must understand how what SVD is

123
00:06:04.300 --> 00:06:04.600
 generally speaking.

124
00:06:05.500 --> 00:06:07.500
and the prerequisite for understanding SVD

125
00:06:08.100 --> 00:06:11.000
is knowing something called a characteristic polynomial.

126
00:06:11.700 --> 00:06:14.100
The determinant function from which we can

127
00:06:14.100 --> 00:06:17.400
 extract eigenvalues and eigenvectors. I have a good sense. You

128
00:06:17.400 --> 00:06:19.200
 most of you heard of eigenvales and eigenvectors.

129
00:06:20.400 --> 00:06:20.600
Okay.

130
00:06:21.900 --> 00:06:24.900
And of course, we can only extract eigenvalues

131
00:06:24.900 --> 00:06:27.300
 eigenvectors or rather. We can apply determinate function

132
00:06:27.300 --> 00:06:30.500
 to only a square Matrix, which is one of the many matrices that

133
00:06:30.500 --> 00:06:33.100
 we will encounter, but perhaps the one we

134
00:06:33.100 --> 00:06:34.200
 will focus mostly on.

135
00:06:35.900 --> 00:06:37.900
So if you want to pick up a book.

136
00:06:38.600 --> 00:06:39.500
on machine learning

137
00:06:41.400 --> 00:06:44.400
all the new algebra or if you would come across a Blog and you would

138
00:06:44.400 --> 00:06:47.500
 to try to make sense. You know, what what is the formula of PCA? You

139
00:06:47.500 --> 00:06:49.700
 would get stuck at least I did.

140
00:06:50.500 --> 00:06:51.500
And then I have to figure out.

141
00:06:53.300 --> 00:06:53.900
the prerequisites

142
00:06:54.600 --> 00:06:54.800
and

143
00:06:55.700 --> 00:06:56.800
this is this is what it looks like.

144
00:06:57.700 --> 00:07:01.400
So the point I'm trying to make is in this course, we're going to learn linear

145
00:07:00.400 --> 00:07:01.800
 algebra.

146
00:07:04.300 --> 00:07:08.400
With the goal of eventually understanding techniques

147
00:07:07.400 --> 00:07:09.200
 like PCA.

148
00:07:11.300 --> 00:07:12.100
That will then open.

149
00:07:13.400 --> 00:07:14.700
These opportunities for us.

150
00:07:16.200 --> 00:07:19.600
You know to apply to diff all kinds to many kinds of problems in

151
00:07:19.600 --> 00:07:20.200
 the real world.

152
00:07:20.900 --> 00:07:23.300
Okay, so as opposed to just learning these

153
00:07:23.300 --> 00:07:24.200
 topics.

154
00:07:24.900 --> 00:07:25.600
and abstract

155
00:07:26.300 --> 00:07:29.400
And an abstract matter we want to see how every single

156
00:07:29.400 --> 00:07:31.900
 concept applies to practical problems.

157
00:07:32.800 --> 00:07:35.800
And we will do this with python. So you will Implement some

158
00:07:35.800 --> 00:07:38.200
 of these not or not. We cannot Implement all

159
00:07:38.200 --> 00:07:39.500
 of these techniques with

160
00:07:40.800 --> 00:07:41.900
python

161
00:07:42.600 --> 00:07:45.400
so for example, you can we can implement the terminal

162
00:07:45.400 --> 00:07:48.600
 function of python characteristic polynomial is a

163
00:07:48.600 --> 00:07:48.900
 little bit more.

164
00:07:49.700 --> 00:07:50.300
challenging

165
00:07:51.100 --> 00:07:52.900
but that's all we want to do so in this

166
00:07:53.900 --> 00:07:56.600
Form you would not only learn linear

167
00:07:56.600 --> 00:07:59.500
 algebra an abstract algebra. By the way, you will

168
00:07:59.500 --> 00:08:02.400
 also look become much better at Python and specifically no

169
00:08:02.400 --> 00:08:02.600
 pi.
