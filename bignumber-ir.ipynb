{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./LMP/MATH-tutorial-8.html\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lxml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7p/61f8kxcj41g0wh3cwxyr0j240000gn/T/ipykernel_2953/144282154.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m \u001b[0;31m# Read the collection\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlxml\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0metree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    594\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mxml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0metree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mElementTree\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mET\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mET\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mxmlCollectionPath\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetroot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lxml'"
     ]
    }
   ],
   "source": [
    "# Note:\n",
    "# To run this script and generate the results files (index.txt, results.boolean.txt, \n",
    "# and resukts.ranked.txt), set the path to the XML collection file here. The code \n",
    "# for generating the output files are in sections (6), (7), and (8).\n",
    "xmlCollectionPath = './LMP/MATH-tutorial-8.html'\n",
    "print(xmlCollectionPath)\n",
    "\n",
    "# ------------------------------------------------------------------------------------------ #\n",
    "# (1) Import Libraries and Collection\n",
    "# ------------------------------------------------------------------------------------------ #\n",
    "\n",
    "# Import the libraries\n",
    "import math\n",
    "import re\n",
    "from stemming.porter2 import stem\n",
    "\n",
    "# List of stop words\n",
    "stopWords = [\n",
    "    \"a\",\n",
    "    \"a's\",\n",
    "    \"able\",\n",
    "    \"about\",\n",
    "    \"above\",\n",
    "    \"according\",\n",
    "    \"accordingly\",\n",
    "    \"across\",\n",
    "    \"actually\",\n",
    "    \"after\",\n",
    "    \"afterwards\",\n",
    "    \"again\",\n",
    "    \"against\",\n",
    "    \"ain't\",\n",
    "    \"all\",\n",
    "    \"allow\",\n",
    "    \"allows\",\n",
    "    \"almost\",\n",
    "    \"alone\",\n",
    "    \"along\",\n",
    "    \"already\",\n",
    "    \"also\",\n",
    "    \"although\",\n",
    "    \"always\",\n",
    "    \"am\",\n",
    "    \"among\",\n",
    "    \"amongst\",\n",
    "    \"an\",\n",
    "    \"and\",\n",
    "    \"another\",\n",
    "    \"any\",\n",
    "    \"anybody\",\n",
    "    \"anyhow\",\n",
    "    \"anyone\",\n",
    "    \"anything\",\n",
    "    \"anyway\",\n",
    "    \"anyways\",\n",
    "    \"anywhere\",\n",
    "    \"apart\",\n",
    "    \"appear\",\n",
    "    \"appreciate\",\n",
    "    \"appropriate\",\n",
    "    \"are\",\n",
    "    \"aren't\",\n",
    "    \"around\",\n",
    "    \"as\",\n",
    "    \"aside\",\n",
    "    \"ask\",\n",
    "    \"asking\",\n",
    "    \"associated\",\n",
    "    \"at\",\n",
    "    \"available\",\n",
    "    \"away\",\n",
    "    \"awfully\",\n",
    "    \"b\",\n",
    "    \"be\",\n",
    "    \"became\",\n",
    "    \"because\",\n",
    "    \"become\",\n",
    "    \"becomes\",\n",
    "    \"becoming\",\n",
    "    \"been\",\n",
    "    \"before\",\n",
    "    \"beforehand\",\n",
    "    \"behind\",\n",
    "    \"being\",\n",
    "    \"believe\",\n",
    "    \"below\",\n",
    "    \"beside\",\n",
    "    \"besides\",\n",
    "    \"best\",\n",
    "    \"better\",\n",
    "    \"between\",\n",
    "    \"beyond\",\n",
    "    \"both\",\n",
    "    \"brief\",\n",
    "    \"but\",\n",
    "    \"by\",\n",
    "    \"c\",\n",
    "    \"c'mon\",\n",
    "    \"c's\",\n",
    "    \"came\",\n",
    "    \"can\",\n",
    "    \"can't\",\n",
    "    \"cannot\",\n",
    "    \"cant\",\n",
    "    \"cause\",\n",
    "    \"causes\",\n",
    "    \"certain\",\n",
    "    \"certainly\",\n",
    "    \"changes\",\n",
    "    \"clearly\",\n",
    "    \"co\",\n",
    "    \"com\",\n",
    "    \"come\",\n",
    "    \"comes\",\n",
    "    \"concerning\",\n",
    "    \"consequently\",\n",
    "    \"consider\",\n",
    "    \"considering\",\n",
    "    \"contain\",\n",
    "    \"containing\",\n",
    "    \"contains\",\n",
    "    \"corresponding\",\n",
    "    \"could\",\n",
    "    \"couldn't\",\n",
    "    \"course\",\n",
    "    \"currently\",\n",
    "    \"d\",\n",
    "    \"definitely\",\n",
    "    \"described\",\n",
    "    \"despite\",\n",
    "    \"did\",\n",
    "    \"didn't\",\n",
    "    \"different\",\n",
    "    \"do\",\n",
    "    \"does\",\n",
    "    \"doesn't\",\n",
    "    \"doing\",\n",
    "    \"don't\",\n",
    "    \"done\",\n",
    "    \"down\",\n",
    "    \"downwards\",\n",
    "    \"during\",\n",
    "    \"e\",\n",
    "    \"each\",\n",
    "    \"edu\",\n",
    "    \"eg\",\n",
    "    \"eight\",\n",
    "    \"either\",\n",
    "    \"else\",\n",
    "    \"elsewhere\",\n",
    "    \"enough\",\n",
    "    \"entirely\",\n",
    "    \"especially\",\n",
    "    \"et\",\n",
    "    \"etc\",\n",
    "    \"even\",\n",
    "    \"ever\",\n",
    "    \"every\",\n",
    "    \"everybody\",\n",
    "    \"everyone\",\n",
    "    \"everything\",\n",
    "    \"everywhere\",\n",
    "    \"ex\",\n",
    "    \"exactly\",\n",
    "    \"example\",\n",
    "    \"except\",\n",
    "    \"f\",\n",
    "    \"far\",\n",
    "    \"few\",\n",
    "    \"fifth\",\n",
    "    \"first\",\n",
    "    \"five\",\n",
    "    \"followed\",\n",
    "    \"following\",\n",
    "    \"follows\",\n",
    "    \"for\",\n",
    "    \"former\",\n",
    "    \"formerly\",\n",
    "    \"forth\",\n",
    "    \"four\",\n",
    "    \"from\",\n",
    "    \"further\",\n",
    "    \"furthermore\",\n",
    "    \"g\",\n",
    "    \"get\",\n",
    "    \"gets\",\n",
    "    \"getting\",\n",
    "    \"given\",\n",
    "    \"gives\",\n",
    "    \"go\",\n",
    "    \"goes\",\n",
    "    \"going\",\n",
    "    \"gone\",\n",
    "    \"got\",\n",
    "    \"gotten\",\n",
    "    \"greetings\",\n",
    "    \"h\",\n",
    "    \"had\",\n",
    "    \"hadn't\",\n",
    "    \"happens\",\n",
    "    \"hardly\",\n",
    "    \"has\",\n",
    "    \"hasn't\",\n",
    "    \"have\",\n",
    "    \"haven't\",\n",
    "    \"having\",\n",
    "    \"he\",\n",
    "    \"he's\",\n",
    "    \"hello\",\n",
    "    \"help\",\n",
    "    \"hence\",\n",
    "    \"her\",\n",
    "    \"here\",\n",
    "    \"here's\",\n",
    "    \"hereafter\",\n",
    "    \"hereby\",\n",
    "    \"herein\",\n",
    "    \"hereupon\",\n",
    "    \"hers\",\n",
    "    \"herself\",\n",
    "    \"hi\",\n",
    "    \"him\",\n",
    "    \"himself\",\n",
    "    \"his\",\n",
    "    \"hither\",\n",
    "    \"hopefully\",\n",
    "    \"how\",\n",
    "    \"howbeit\",\n",
    "    \"however\",\n",
    "    \"i\",\n",
    "    \"i'd\",\n",
    "    \"i'll\",\n",
    "    \"i'm\",\n",
    "    \"i've\",\n",
    "    \"ie\",\n",
    "    \"if\",\n",
    "    \"ignored\",\n",
    "    \"immediate\",\n",
    "    \"in\",\n",
    "    \"inasmuch\",\n",
    "    \"inc\",\n",
    "    \"indeed\",\n",
    "    \"indicate\",\n",
    "    \"indicated\",\n",
    "    \"indicates\",\n",
    "    \"inner\",\n",
    "    \"insofar\",\n",
    "    \"instead\",\n",
    "    \"into\",\n",
    "    \"inward\",\n",
    "    \"is\",\n",
    "    \"isn't\",\n",
    "    \"it\",\n",
    "    \"it'd\",\n",
    "    \"it'll\",\n",
    "    \"it's\",\n",
    "    \"its\",\n",
    "    \"itself\",\n",
    "    \"j\",\n",
    "    \"just\",\n",
    "    \"k\",\n",
    "    \"keep\",\n",
    "    \"keeps\",\n",
    "    \"kept\",\n",
    "    \"know\",\n",
    "    \"knows\",\n",
    "    \"known\",\n",
    "    \"l\",\n",
    "    \"last\",\n",
    "    \"lately\",\n",
    "    \"later\",\n",
    "    \"latter\",\n",
    "    \"latterly\",\n",
    "    \"least\",\n",
    "    \"less\",\n",
    "    \"lest\",\n",
    "    \"let\",\n",
    "    \"let's\",\n",
    "    \"like\",\n",
    "    \"liked\",\n",
    "    \"likely\",\n",
    "    \"little\",\n",
    "    \"look\",\n",
    "    \"looking\",\n",
    "    \"looks\",\n",
    "    \"ltd\",\n",
    "    \"m\",\n",
    "    \"mainly\",\n",
    "    \"many\",\n",
    "    \"may\",\n",
    "    \"maybe\",\n",
    "    \"me\",\n",
    "    \"mean\",\n",
    "    \"meanwhile\",\n",
    "    \"merely\",\n",
    "    \"might\",\n",
    "    \"more\",\n",
    "    \"moreover\",\n",
    "    \"most\",\n",
    "    \"mostly\",\n",
    "    \"much\",\n",
    "    \"must\",\n",
    "    \"my\",\n",
    "    \"myself\",\n",
    "    \"n\",\n",
    "    \"name\",\n",
    "    \"namely\",\n",
    "    \"nd\",\n",
    "    \"near\",\n",
    "    \"nearly\",\n",
    "    \"necessary\",\n",
    "    \"need\",\n",
    "    \"needs\",\n",
    "    \"neither\",\n",
    "    \"never\",\n",
    "    \"nevertheless\",\n",
    "    \"new\",\n",
    "    \"next\",\n",
    "    \"nine\",\n",
    "    \"no\",\n",
    "    \"nobody\",\n",
    "    \"non\",\n",
    "    \"none\",\n",
    "    \"noone\",\n",
    "    \"nor\",\n",
    "    \"normally\",\n",
    "    \"not\",\n",
    "    \"nothing\",\n",
    "    \"novel\",\n",
    "    \"now\",\n",
    "    \"nowhere\",\n",
    "    \"o\",\n",
    "    \"obviously\",\n",
    "    \"of\",\n",
    "    \"off\",\n",
    "    \"often\",\n",
    "    \"oh\",\n",
    "    \"ok\",\n",
    "    \"okay\",\n",
    "    \"old\",\n",
    "    \"on\",\n",
    "    \"once\",\n",
    "    \"one\",\n",
    "    \"ones\",\n",
    "    \"only\",\n",
    "    \"onto\",\n",
    "    \"or\",\n",
    "    \"other\",\n",
    "    \"others\",\n",
    "    \"otherwise\",\n",
    "    \"ought\",\n",
    "    \"our\",\n",
    "    \"ours\",\n",
    "    \"ourselves\",\n",
    "    \"out\",\n",
    "    \"outside\",\n",
    "    \"over\",\n",
    "    \"overall\",\n",
    "    \"own\",\n",
    "    \"p\",\n",
    "    \"particular\",\n",
    "    \"particularly\",\n",
    "    \"per\",\n",
    "    \"perhaps\",\n",
    "    \"placed\",\n",
    "    \"please\",\n",
    "    \"plus\",\n",
    "    \"possible\",\n",
    "    \"presumably\",\n",
    "    \"probably\",\n",
    "    \"provides\",\n",
    "    \"q\",\n",
    "    \"que\",\n",
    "    \"quite\",\n",
    "    \"qv\",\n",
    "    \"r\",\n",
    "    \"rather\",\n",
    "    \"rd\",\n",
    "    \"re\",\n",
    "    \"really\",\n",
    "    \"reasonably\",\n",
    "    \"regarding\",\n",
    "    \"regardless\",\n",
    "    \"regards\",\n",
    "    \"relatively\",\n",
    "    \"respectively\",\n",
    "    \"right\",\n",
    "    \"s\",\n",
    "    \"said\",\n",
    "    \"same\",\n",
    "    \"saw\",\n",
    "    \"say\",\n",
    "    \"saying\",\n",
    "    \"says\",\n",
    "    \"second\",\n",
    "    \"secondly\",\n",
    "    \"see\",\n",
    "    \"seeing\",\n",
    "    \"seem\",\n",
    "    \"seemed\",\n",
    "    \"seeming\",\n",
    "    \"seems\",\n",
    "    \"seen\",\n",
    "    \"self\",\n",
    "    \"selves\",\n",
    "    \"sensible\",\n",
    "    \"sent\",\n",
    "    \"serious\",\n",
    "    \"seriously\",\n",
    "    \"seven\",\n",
    "    \"several\",\n",
    "    \"shall\",\n",
    "    \"she\",\n",
    "    \"should\",\n",
    "    \"shouldn't\",\n",
    "    \"since\",\n",
    "    \"six\",\n",
    "    \"so\",\n",
    "    \"some\",\n",
    "    \"somebody\",\n",
    "    \"somehow\",\n",
    "    \"someone\",\n",
    "    \"something\",\n",
    "    \"sometime\",\n",
    "    \"sometimes\",\n",
    "    \"somewhat\",\n",
    "    \"somewhere\",\n",
    "    \"soon\",\n",
    "    \"sorry\",\n",
    "    \"specified\",\n",
    "    \"specify\",\n",
    "    \"specifying\",\n",
    "    \"still\",\n",
    "    \"sub\",\n",
    "    \"such\",\n",
    "    \"sup\",\n",
    "    \"sure\",\n",
    "    \"t\",\n",
    "    \"t's\",\n",
    "    \"take\",\n",
    "    \"taken\",\n",
    "    \"tell\",\n",
    "    \"tends\",\n",
    "    \"th\",\n",
    "    \"than\",\n",
    "    \"thank\",\n",
    "    \"thanks\",\n",
    "    \"thanx\",\n",
    "    \"that\",\n",
    "    \"that's\",\n",
    "    \"thats\",\n",
    "    \"the\",\n",
    "    \"their\",\n",
    "    \"theirs\",\n",
    "    \"them\",\n",
    "    \"themselves\",\n",
    "    \"then\",\n",
    "    \"thence\",\n",
    "    \"there\",\n",
    "    \"there's\",\n",
    "    \"thereafter\",\n",
    "    \"thereby\",\n",
    "    \"therefore\",\n",
    "    \"therein\",\n",
    "    \"theres\",\n",
    "    \"thereupon\",\n",
    "    \"these\",\n",
    "    \"they\",\n",
    "    \"they'd\",\n",
    "    \"they'll\",\n",
    "    \"they're\",\n",
    "    \"they've\",\n",
    "    \"think\",\n",
    "    \"third\",\n",
    "    \"this\",\n",
    "    \"thorough\",\n",
    "    \"thoroughly\",\n",
    "    \"those\",\n",
    "    \"though\",\n",
    "    \"three\",\n",
    "    \"through\",\n",
    "    \"throughout\",\n",
    "    \"thru\",\n",
    "    \"thus\",\n",
    "    \"to\",\n",
    "    \"together\",\n",
    "    \"too\",\n",
    "    \"took\",\n",
    "    \"toward\",\n",
    "    \"towards\",\n",
    "    \"tried\",\n",
    "    \"tries\",\n",
    "    \"truly\",\n",
    "    \"try\",\n",
    "    \"trying\",\n",
    "    \"twice\",\n",
    "    \"two\",\n",
    "    \"u\",\n",
    "    \"un\",\n",
    "    \"under\",\n",
    "    \"unfortunately\",\n",
    "    \"unless\",\n",
    "    \"unlikely\",\n",
    "    \"until\",\n",
    "    \"unto\",\n",
    "    \"up\",\n",
    "    \"upon\",\n",
    "    \"us\",\n",
    "    \"use\",\n",
    "    \"used\",\n",
    "    \"useful\",\n",
    "    \"uses\",\n",
    "    \"using\",\n",
    "    \"usually\",\n",
    "    \"uucp\",\n",
    "    \"v\",\n",
    "    \"value\",\n",
    "    \"various\",\n",
    "    \"very\",\n",
    "    \"via\",\n",
    "    \"viz\",\n",
    "    \"vs\",\n",
    "    \"w\",\n",
    "    \"want\",\n",
    "    \"wants\",\n",
    "    \"was\",\n",
    "    \"wasn't\",\n",
    "    \"way\",\n",
    "    \"we\",\n",
    "    \"we'd\",\n",
    "    \"we'll\",\n",
    "    \"we're\",\n",
    "    \"we've\",\n",
    "    \"welcome\",\n",
    "    \"well\",\n",
    "    \"went\",\n",
    "    \"were\",\n",
    "    \"weren't\",\n",
    "    \"what\",\n",
    "    \"what's\",\n",
    "    \"whatever\",\n",
    "    \"when\",\n",
    "    \"whence\",\n",
    "    \"whenever\",\n",
    "    \"where\",\n",
    "    \"where's\",\n",
    "    \"whereafter\",\n",
    "    \"whereas\",\n",
    "    \"whereby\",\n",
    "    \"wherein\",\n",
    "    \"whereupon\",\n",
    "    \"wherever\",\n",
    "    \"whether\",\n",
    "    \"which\",\n",
    "    \"while\",\n",
    "    \"whither\",\n",
    "    \"who\",\n",
    "    \"who's\",\n",
    "    \"whoever\",\n",
    "    \"whole\",\n",
    "    \"whom\",\n",
    "    \"whose\",\n",
    "    \"why\",\n",
    "    \"will\",\n",
    "    \"willing\",\n",
    "    \"wish\",\n",
    "    \"with\",\n",
    "    \"within\",\n",
    "    \"without\",\n",
    "    \"won't\",\n",
    "    \"wonder\",\n",
    "    \"would\",\n",
    "    \"would\",\n",
    "    \"wouldn't\",\n",
    "    \"x\",\n",
    "    \"y\",\n",
    "    \"yes\",\n",
    "    \"yet\",\n",
    "    \"you\",\n",
    "    \"you'd\",\n",
    "    \"you'll\",\n",
    "    \"you're\",\n",
    "    \"you've\",\n",
    "    \"your\",\n",
    "    \"yours\",\n",
    "    \"yourself\",\n",
    "    \"yourselves\",\n",
    "    \"z\",\n",
    "    \"zero\"\n",
    "]\n",
    "\n",
    "# Read the collection\n",
    "from lxml import etree\n",
    "import xml.etree.ElementTree as ET\n",
    "root = ET.parse( xmlCollectionPath ).getroot()\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------ #\n",
    "# (2) Reading the Documents, Creating the Inverted Index \n",
    "# ------------------------------------------------------------------------------------------ #\n",
    "def posIndex(tokens, invertedIndex, docID):\n",
    "    \"\"\"\n",
    "    Function to add to the inverted index the position \n",
    "    of terms of a document.\n",
    "    \"\"\"\n",
    "    for tokenIndex, token in enumerate(tokens):\n",
    "        # If token has already been seen\n",
    "        if token in invertedIndex.keys():\n",
    "            # Increment the general frequency of the token\n",
    "            invertedIndex[token][0] += 1\n",
    "\n",
    "            # Add position of new encountered term\n",
    "\n",
    "            # If docID is already in the dictionary of the token,\n",
    "            # add the next token index\n",
    "            if docID in invertedIndex[token][1].keys():\n",
    "                invertedIndex[token][1][docID].append(tokenIndex)\n",
    "            # Otherwise, add new docID and token index of current\n",
    "            # token encounter\n",
    "            else:\n",
    "                invertedIndex[token][1][docID] = []\n",
    "                invertedIndex[token][1][docID].append(tokenIndex)\n",
    "        # Otherwise, add token to dictionary\n",
    "        else:   \n",
    "            # Initiate vector (list) for token\n",
    "            invertedIndex[token] = []\n",
    "            # Increment general frequency by 1\n",
    "            invertedIndex[token].append(1)\n",
    "            # Append to vector positional vector (including pos of first token encounter)\n",
    "            invertedIndex[token].append({ docID: [tokenIndex]})\n",
    "    return invertedIndex\n",
    "\n",
    "\n",
    "def preprocessor(document):\n",
    "    \"\"\"\n",
    "    Function to preprocess documents and return lists where\n",
    "    the tokens have been made lowercase, stripped of non-letter\n",
    "    characters, and stemmed.\n",
    "    \"\"\"\n",
    "    # Case Folding\n",
    "    documentLowerCase = document.casefold()\n",
    "\n",
    "    # Remove punctuation\n",
    "    # documentLowerCase = documentLowerCase.search(r'(\\.|\\:|\\;|\\,|\\!|\\?|\\\"|\\'|\\[|\\]|\\{|\\})', documentLowerCase)\n",
    "    documentNoPunct = re.sub(r'([^\\w]|\\d|\\s)', ' ', documentLowerCase)\n",
    "\n",
    "    # Tokenize\n",
    "    tokens = re.split(r'\\s+', documentNoPunct)\n",
    "    tokens1 = tokens[1:]\n",
    "\n",
    "    # Tokens without stop words\n",
    "    # Tokens with stemming\n",
    "    tokensWithoutStopWords = []\n",
    "\n",
    "    for token in tokens1:\n",
    "        if token not in stopWords:\n",
    "            tokensWithoutStopWords.append(stem(token))\n",
    "\n",
    "    return tokensWithoutStopWords\n",
    "\n",
    "\n",
    "# Put the documents into 'documents' list\n",
    "# for indexing\n",
    "documents = []\n",
    "for elem in root.findall('DOC'):\n",
    "    headlineContent = elem.find('HEADLINE').text\n",
    "    textContent = elem.find('TEXT').text\n",
    "    docno = elem.find('DOCNO').text\n",
    "    documents.append( \n",
    "        {\n",
    "            \"docID\": docno,\n",
    "            \"text\": headlineContent + \" \" + textContent,\n",
    "            \"tokens\": preprocessor( headlineContent + \" \" + textContent )\n",
    "        } \n",
    "    )\n",
    "\n",
    "# Build the invertedIndex\n",
    "invertedIndex = {}\n",
    "for document in documents:\n",
    "    # invertedIndex = termCountAndIndex(tokens, invertedIndex, docIndex)\n",
    "    invertedIndex = posIndex(document['tokens'], invertedIndex, document['docID'])\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------ #\n",
    "# (3) Intersect and Poitional Intersect Functions\n",
    "# ------------------------------------------------------------------------------------------ #\n",
    "def nextPointer(pointer, index, plist):\n",
    "    \"\"\"\n",
    "    Function to return the next pointer for a posting\n",
    "    \"\"\"\n",
    "    if index+1 > len(plist)-1:\n",
    "        pointer = None\n",
    "    else:\n",
    "        index += 1\n",
    "        pointer = plist[int(index)]\n",
    "    return pointer, index\n",
    "\n",
    "\n",
    "def getDocuments(posting):\n",
    "    \"\"\"\n",
    "    Returns the documents IDs of a posting as a \n",
    "    list data type.\n",
    "    \"\"\"\n",
    "    return list(posting[1].keys())\n",
    "\n",
    "\n",
    "def getPositions(posting):\n",
    "    \"\"\"\n",
    "    Returns the term positions (indices) for a\n",
    "    posting as a list data type.\n",
    "    \"\"\"\n",
    "    return list(posting[1].values())\n",
    "\n",
    "\n",
    "def intersect(posting1, posting2):\n",
    "    \"\"\"\n",
    "    Function to intersect two postings. This is\n",
    "    the implementation of the intersect algorithm. \n",
    "    \"\"\"\n",
    "    answer = []\n",
    "    p1docs = getDocuments(posting1)\n",
    "    p2docs = getDocuments(posting2)\n",
    "\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    docPointer1 = p1docs[i]\n",
    "    docPointer2 = p2docs[j]\n",
    "\n",
    "    while docPointer1 != None and docPointer2 != None:\n",
    "        if int(docPointer1) == int(docPointer2):\n",
    "            answer.append(docPointer1)\n",
    "           \n",
    "            docPointer1, i = nextPointer(docPointer1, i, p1docs)\n",
    "            docPointer2, j = nextPointer(docPointer2, j, p2docs)\n",
    "\n",
    "        elif int(docPointer1) < int(docPointer2):\n",
    "            docPointer1, i = nextPointer(docPointer1, i, p1docs)\n",
    "        else:\n",
    "            docPointer2, j = nextPointer(docPointer2, j, p2docs)\n",
    "    return answer\n",
    "\n",
    "\n",
    "def posIntersect(posting1, posting2, k):\n",
    "    \"\"\"\n",
    "    Function to intersect two postings in k proximity. This is\n",
    "    the implementation of the positional intersect algorithm.  \n",
    "    \"\"\"\n",
    "    # List containing documents where match is found.\n",
    "    # List is tuple (but List implementations) with\n",
    "    # values (docID, posOfWTerm1, posOfWTerm2)\n",
    "    answer = []\n",
    "\n",
    "    # Indices for enumerating document IDs\n",
    "    i = j = 0\n",
    "\n",
    "    # List of documents for each term\n",
    "    post1_ids = getDocuments(posting1)\n",
    "    post2_ids = getDocuments(posting2)\n",
    "\n",
    "    # docID for each document at current index\n",
    "    docID1 = post1_ids[i]\n",
    "    docID2 = post2_ids[j]\n",
    "\n",
    "    while docID1 != None and docID2 != None:\n",
    "        if int(docID1) == int(docID2):\n",
    "            l = []\n",
    "\n",
    "            # Positions of terms in current documents\n",
    "            docPos1 = getPositions(posting1)[i]\n",
    "            docPos2 = getPositions(posting2)[j]        \n",
    "           \n",
    "            # Indices for positions of terms\n",
    "            ii = jj = 0\n",
    "\n",
    "            # Point to...\n",
    "            while ii != len(docPos1):\n",
    "                while jj != len(docPos2):\n",
    "                    if abs( docPos1[ii] - docPos2[jj]) <= k:\n",
    "                        l.append(docPos2[jj])\n",
    "                    elif docPos2[jj] > docPos1[ii]:\n",
    "                        break\n",
    "                    jj += 1\n",
    "\n",
    "                while l != [] and abs(l[0] - docPos1[ii]) > k:\n",
    "                    del l[0]\n",
    "                \n",
    "                for ps in l:\n",
    "                    answer.append( [docID1, docPos1[ii], ps] )\n",
    "                ii += 1\n",
    "\n",
    "            docID1, i = nextPointer(docID1, i, post1_ids)\n",
    "            docID2, j = nextPointer(docID2, j, post2_ids)\n",
    "\n",
    "        elif int(docID1) < int(docID2):\n",
    "            docID1, i = nextPointer(docID1, i, post1_ids)\n",
    "        else:\n",
    "            docID2, j = nextPointer(docID2, j, post2_ids)\n",
    "    return answer\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------ #\n",
    "# (4) TF-IDF Functions\n",
    "# ------------------------------------------------------------------------------------------ #\n",
    "\n",
    "def idf(df, N):\n",
    "    \"\"\"\n",
    "    Calculates and returns the inverse-document frequency\n",
    "    for a given document frequency (df) and size of documents\n",
    "    in a collection (N).\n",
    "    \"\"\"\n",
    "    return math.log10(N/df)\n",
    "\n",
    "\n",
    "def tf(term,docID,invertedIndex):\n",
    "    \"\"\"\n",
    "    Calculates and returns the term frequency in the collection\n",
    "    for a given term.\n",
    "    \"\"\"\n",
    "    if docID not in list(invertedIndex[term][1].keys()):\n",
    "        return 0\n",
    "    else:\n",
    "        return len(invertedIndex[term][1][docID])\n",
    "\n",
    "\n",
    "def get_df(term, invertedIndex):\n",
    "    \"\"\"\n",
    "    Returns the document frequency of a term as a list.\n",
    "    \"\"\"\n",
    "    return len(invertedIndex[term][1])\n",
    "\n",
    "\n",
    "def tf_idf_normal(term, docID, invertedIndex, documents):\n",
    "    \"\"\"\n",
    "    Calculates TF-IDF (term frequency - inverse document frequency)\n",
    "    and applies normalization priort to return the score. \n",
    "    \"\"\"\n",
    "    collectionSize = len(documents)\n",
    "    df = get_df(term, invertedIndex)\n",
    "    idf_val = idf(df, collectionSize)\n",
    "    tf_val = tf(term, docID, invertedIndex)\n",
    "    if tf_val == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        tf_idf = (1 + math.log10(tf_val) ) * idf_val\n",
    "        return tf_idf\n",
    "\n",
    "\n",
    "def score(terms, docID, invertedIndex, documents):\n",
    "    \"\"\"\n",
    "    Returns the ranking score after aggregating the normalized\n",
    "    tf-idf for each search term.\n",
    "    \"\"\"\n",
    "    s = 0\n",
    "    for term in terms:\n",
    "        s += tf_idf_normal(term, docID, invertedIndex, documents)\n",
    "    return ( docID, round(s,4) )\n",
    "\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------------------------------ #\n",
    "# (5) Search Functions\n",
    "# ------------------------------------------------------------------------------------------ #\n",
    "\n",
    "def phraseSearch(queryString, k=1):\n",
    "    \"\"\"\n",
    "    Function to search for phrase queries. Unless the k is specified,\n",
    "    the default proximity is 1. Queries must begin with double quotes (\"\").\n",
    "    \"\"\"\n",
    "    phraseQueries = re.finditer(r'\"(\\w|\\d|\\s{0,})+\"', queryString)\n",
    "    invertedIndexKeys = list(invertedIndex.keys())\n",
    "    result = []\n",
    "\n",
    "    for pq in phraseQueries:\n",
    "        phraseToken = pq.group().replace(\"\\\"\", \"\")\n",
    "        # print( phraseToken )\n",
    "        tokens = re.split(r'\\s+', phraseToken)\n",
    "        preprocessed_tl = []\n",
    "        for i, t in enumerate(tokens):\n",
    "            tl = t.casefold()\n",
    "            tl = stem(tl)\n",
    "            preprocessed_tl.append(tl)\n",
    "\n",
    "            if i > 0:\n",
    "                if preprocessed_tl[i - 1] in invertedIndexKeys and preprocessed_tl[i] in invertedIndexKeys:\n",
    "                    result += posIntersect(\n",
    "                            invertedIndex[ preprocessed_tl[i-1] ], \n",
    "                            invertedIndex[ preprocessed_tl[i] ],\n",
    "                            k\n",
    "                        )\n",
    "    return result\n",
    "\n",
    "\n",
    "def booleanSearch(queryString):\n",
    "    \"\"\"\n",
    "    Function to search for terms containing the logical\n",
    "    operators 'AND', 'OR', and 'NOT' using the set operations\n",
    "    intersection, union, and complement, respectively.\n",
    "    \"\"\"\n",
    "    queryTokens = re.split(\"\\s+\", queryString)\n",
    "    querySet = set({})\n",
    "    searchTerms = []\n",
    "\n",
    "    for i, t in enumerate(queryTokens):\n",
    "        if t[0] == '\"':\n",
    "\n",
    "            # Remove '+' for intermediate processing\n",
    "            term = t.replace(\"+\",\" \")\n",
    "            matches = phraseSearch(term)\n",
    "            docs = set({})\n",
    "            for m in matches:\n",
    "                docs.add(m[0])\n",
    "\n",
    "            searchTerms.append(term)\n",
    "            uSet = docs\n",
    "\n",
    "            if i>=0 and queryTokens[i-1] == 'OR':\n",
    "                querySet = querySet.union( uSet )\n",
    "\n",
    "            elif i>=0 and queryTokens[i-1] == 'AND':\n",
    "\n",
    "                querySet = querySet.intersection( uSet )\n",
    "            elif i>=0 and queryTokens[i-1] == 'NOT':\n",
    "                querySet = querySet - uSet\n",
    "\n",
    "            else:\n",
    "                querySet = querySet.union( uSet )\n",
    "        \n",
    "        else:\n",
    "            if t != 'AND' and t != 'OR' and t!= 'NOT' and t != '' and t != ' ':\n",
    "                # Processed term\n",
    "                pTerm = stem(t.strip().casefold()) \n",
    "                uSet = set(invertedIndex[pTerm][1].keys())\n",
    "                searchTerms.append(pTerm)\n",
    "\n",
    "                if i>=0 and queryTokens[i-1] == 'OR':\n",
    "                    querySet = querySet.union( uSet )\n",
    "\n",
    "                elif i>=0 and queryTokens[i-1] == 'AND':\n",
    "                    querySet = querySet.intersection( uSet )\n",
    "\n",
    "                elif i>=0 and queryTokens[i-1] == 'NOT':\n",
    "                    querySet = querySet - uSet\n",
    "\n",
    "                else:\n",
    "                    querySet = querySet.union( uSet )\n",
    "\n",
    "    return searchTerms, querySet\n",
    "\n",
    "\n",
    "def positionalSearch(queryString):\n",
    "    \"\"\"\n",
    "    This function will search for terms using the positional\n",
    "    intersect function (posIntersect) for queries of the form \n",
    "    #k(term1, term2)\n",
    "    \"\"\"\n",
    "    positionSearch = re.search('#[0-9]+', queryString).group()\n",
    "\n",
    "    if bool(positionSearch):\n",
    "        maxDistance = int(positionSearch[1:])\n",
    "        positionalTerms = re.split('(#[0-9]+)', queryString)\n",
    "\n",
    "        queryTokens = re.split('(#[0-9]+|\\(|\\)|\\,)', queryString)\n",
    "        positionalTerms = []\n",
    "        for t_i, t in enumerate(queryTokens):\n",
    "            \n",
    "            if t_i > 0:\n",
    "                if t != '' and t != ',' and t != '(' and t != ')' and ('#' not in t):\n",
    "                    t = t.replace(' ', '')\n",
    "                    t = stem(t.strip().casefold())\n",
    "                    positionalTerms.append(t)\n",
    "\n",
    "    t1 = invertedIndex[ positionalTerms[0] ]\n",
    "    t2 = invertedIndex[ positionalTerms[1] ]\n",
    "    return posIntersect(t1, t2, maxDistance)\n",
    "\n",
    "\n",
    "def processQuery(queryString):\n",
    "    \"\"\"\n",
    "    Function to process a query containing phrases and logical \n",
    "    operators for the boolean retrieval.\n",
    "    \"\"\"\n",
    "    phraseQueries = re.finditer(r'\"(\\w|\\d|\\s{0,})+\"', queryString)\n",
    "    searchTokens = []\n",
    "    for q in phraseQueries: \n",
    "        s,e = q.span()\n",
    "\n",
    "        qlist = list(queryString)\n",
    "        for c in range(s,e):\n",
    "            if qlist[c] == ' ':\n",
    "                qlist[c] = '+'\n",
    "            queryString = \"\".join(qlist)\n",
    "    return queryString\n",
    "\n",
    "\n",
    "def rankedRetrieval(queryString, invertedIndex, documents):\n",
    "    \"\"\"\n",
    "    Function to process a query containing phrases and logical \n",
    "    operators for phrase search.\n",
    "    \"\"\"\n",
    "    results = phraseSearch(queryString)\n",
    "    matchingDocs = []\n",
    "    for r in results:\n",
    "        matchingDocs.append(r[0])\n",
    "\n",
    "    searchTerms = queryString.split(\" \")\n",
    "\n",
    "    results = []\n",
    "    for docID in list(matchingDocs):\n",
    "        results.append( score(searchTerms, docID, invertedIndex,documents ) )\n",
    "\n",
    "    results = sorted(results, key = lambda i: i[1], reverse=True)\n",
    "    return results\n",
    "\n",
    "\n",
    "def rankedRetrieval_freeform(queryString, invertedIndex, documents):\n",
    "    \"\"\"\n",
    "    Use boolean search to search for free form queries. \n",
    "    This function will return the normalized tf-idf for all\n",
    "    matches in order of score (descending).\n",
    "    \"\"\"\n",
    "    searchTerms, matchingDocs = booleanSearch(queryString)\n",
    "\n",
    "    results = []\n",
    "    for docID in list(matchingDocs):\n",
    "        results.append( score(searchTerms, docID, invertedIndex,documents ) )\n",
    "\n",
    "    results = sorted(results, key = lambda i: i[1], reverse=True)\n",
    "    return results\n",
    "\n",
    "\n",
    "def booleanRetrieval(queryString):\n",
    "    \"\"\"\n",
    "    Checks query to determine whether to use positional search\n",
    "    or boolean search based. Will return return values of from\n",
    "    either positionalSearch() or booleanSearch() depending on \n",
    "    the type of string argument.\n",
    "    \"\"\"\n",
    "    positionSearch = re.search('#[0-9]+', queryString)\n",
    "    if bool(positionSearch):\n",
    "        return positionalSearch(queryString)\n",
    "    else:\n",
    "        queryString = processQuery(queryString)\n",
    "        return booleanSearch(queryString)\n",
    "\n",
    "\n",
    "\n",
    "print(invertedIndex)\n",
    "\n",
    "exit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
